{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "assert tf.executing_eagerly()\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "import librosa"
      ],
      "metadata": {
        "id": "JSRlxBoHIYoA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert audio signal to numpy array\n",
        "def convert_audio_to_nd(audio_file):\n",
        "\n",
        "    y, sr = librosa.load(audio_file,sr = 16000)\n",
        "    print(\"Audio data type:\", type(y))\n",
        "    print(\"Audio data shape:\", y.shape)\n",
        "\n",
        "    \n",
        "    audio_data = np.array(y)\n",
        "    print(\"New audio data type:\", type(audio_data))\n",
        "    print(\"New audio data shape:\", audio_data.shape)\n",
        "\n",
        "\n",
        "    return audio_data,sr"
      ],
      "metadata": {
        "id": "5jTZyFgtte1e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert audio signal numpy array to embeddings\n",
        "def get_audio_embeddings(audio_signal,sr):\n",
        "    # Load the module and run inference.\n",
        "    module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill/2')\n",
        "    \n",
        "    # Reshape the input ndarray to have shape (num_samples,).\n",
        "    audio_signal = np.squeeze(audio_signal)\n",
        "    \n",
        "    # Resample the audio signal to 16kHz, if necessary.\n",
        "    # resampler = tf.signal.resample\n",
        "    # audio_signal = resampler(audio_signal, tf.constant([len(audio_signal) * 16000 // len(audio_signal)], dtype=tf.int32))\n",
        "    \n",
        "    #audio_signal = librosa.resample(audio_signal, sr, 16000)\n",
        "\n",
        "    # Normalize the audio signal to have values between -1 and 1.\n",
        "    audio_signal = np.asarray(audio_signal, dtype=np.float32)\n",
        "    assert audio_signal.ndim == 1\n",
        "    assert np.abs(audio_signal).max() <= 1.\n",
        "    audio_signal = np.clip(audio_signal, -1., 1.)\n",
        "    \n",
        "    # Generate the embeddings using the loaded module.\n",
        "    emb_dict = module(samples=audio_signal, sample_rate=16000)\n",
        "    emb = emb_dict['embedding']\n",
        "    emb_layer19 = emb_dict['layer19']\n",
        "    \n",
        "    # Return the embeddings as numpy ndarrays.\n",
        "    return emb.numpy(), emb_layer19.numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "cdTcLW1FoYNN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate FAD from test & reference embeddings\n",
        "def calc_fad(ref_emb,test_emb):\n",
        "\n",
        "    euclidean_sq = np.sum((ref_emb.mean(axis=0) - test_emb.mean(axis=0))**2)\n",
        "\n",
        "  \n",
        "    ref_var = np.var(ref_emb, axis=0)\n",
        "    test_var = np.var(test_emb, axis=0)\n",
        "    euclidean_var = np.sum(ref_var + test_var - 2*np.sqrt(ref_var*test_var))\n",
        "    euclidean_var /= ref_emb.shape[1]\n",
        "\n",
        "    # Compute the FrÃ©chet Audio Distance using the Euclidean distance.\n",
        "    fad = euclidean_sq + euclidean_var\n",
        "    fad = np.sqrt(fad)\n",
        "    return fad\n"
      ],
      "metadata": {
        "id": "1vI30f421wdk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get reference & test audio, convert to embeddings and calculate FAD\n",
        "test_audio,sr1 = convert_audio_to_nd(\"/content/sample-0.mp3\") # modify this to take generated audio\n",
        "ref_audio,sr2 = convert_audio_to_nd(\"/content/real.mp3\") # modify this to take corrosponsing music caps audio\n",
        "\n",
        "test_emb,test_emb_19 = get_audio_embeddings(test_audio,sr1)\n",
        "ref_emb,ref_emb_19 = get_audio_embeddings(ref_audio,sr2)\n",
        "\n",
        "fad = calc_fad(test_emb,ref_emb)\n",
        "print(fad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1M3MMxALj6s",
        "outputId": "b7e31929-2c70-4e32-abe1-b428ad1f244a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio data type: <class 'numpy.ndarray'>\n",
            "Audio data shape: (160683,)\n",
            "New audio data type: <class 'numpy.ndarray'>\n",
            "New audio data shape: (160683,)\n",
            "Audio data type: <class 'numpy.ndarray'>\n",
            "Audio data shape: (95109,)\n",
            "New audio data type: <class 'numpy.ndarray'>\n",
            "New audio data shape: (95109,)\n",
            "0.5838273918180972\n"
          ]
        }
      ]
    }
  ]
}