{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txujENPmw_hW"
      },
      "source": [
        "#Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAehkQc7s6R2",
        "outputId": "3f055974-4d36-4d18-9731-10b0dcd8820e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HngJbRKrl2f6",
        "outputId": "79b2844c-0b8a-430e-8c1c-59396071cfc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "v1.0  v2_0.zip\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/gdrive/MyDrive/IDL_Project/data/releases\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql8PXeeytRDh"
      },
      "outputs": [],
      "source": [
        "release_path = \"/content/gdrive/MyDrive/IDL_Project/data/releases/v2_0.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXKyOlMW8nyf"
      },
      "outputs": [],
      "source": [
        "!cp -r \"{release_path}\" ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRBOKjF1mpkn"
      },
      "outputs": [],
      "source": [
        "!unzip -q \"v2_0.zip\" -d /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCIDUEgLtR_h",
        "outputId": "81b00d93-42cd-4da8-f80d-81ed3d43e287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00jcAFB7_pg_03d001.flac\n",
            "00jcAFB7_pg_03d002.flac\n",
            "00jcAFB7_pg_03d003.flac\n",
            "00jcAFB7_pg_03d004.flac\n",
            "00jcAFB7_pg_03d005.flac\n",
            "00jcAFB7_pg_03d006.flac\n",
            "00jcAFB7_pg_03d007.flac\n",
            "00jcAFB7_pg_03d008.flac\n",
            "00jcAFB7_pg_03d009.flac\n",
            "00jcAFB7_pg_03d010.flac\n"
          ]
        }
      ],
      "source": [
        "!ls -1 \"v2.0/\" | head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nghLvbkAsQwd",
        "outputId": "93371902-28ad-40f1-c399-4b4d830c1406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25G\tv2.0/\n"
          ]
        }
      ],
      "source": [
        "!du -h v2.0/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqdm3SEqmVGQ",
        "outputId": "ccd2dd72-91ca-49be-f035-c4ceb1915c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.0/708.0 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.7/272.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install audiolm-pytorch -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36uDXDNesF7a",
        "outputId": "7c34c6e2-7e00-4dd5-aaec-ea9f3c722052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.7/201.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install laion-clap -q #RESTART after this(?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n337KoD2om3L",
        "outputId": "6ccb53da-e2ae-432f-f58c-4231900d0a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon May  1 04:24:19 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbNksjKecRjh"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "JQFjzWqzb4I_",
        "outputId": "bb016fdf-a579-499d-88d3-66eed35ba3e1"
      },
      "outputs": [
        {
          "ename": "SystemExit",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.exit(1) #RESTART after this!!!!!!!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6zkgFFzkqEc"
      },
      "source": [
        "#Yet more pre-processing (NO-EXECUTE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaR_kXG-ko8M"
      },
      "outputs": [],
      "source": [
        "!find v1.0/data/ -size +10M -delete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdSyOEL0meOJ",
        "outputId": "04607577-89ff-45d6-ed49-e4bfdece9ebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base\n",
            "  libsox3\n",
            "Suggested packages:\n",
            "  libsox-fmt-all\n",
            "The following NEW packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base\n",
            "  libsox3 sox\n",
            "0 upgraded, 6 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 513 kB of archives.\n",
            "After this operation, 1,564 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [225 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [10.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [31.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 sox amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [102 kB]\n",
            "Fetched 513 kB in 2s (335 kB/s)\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../2-libsox3_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../3-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../4-libsox-fmt-base_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../5-sox_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up sox (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install sox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUcvQX45syZR",
        "outputId": "3dd0882d-6423-4074-f976-a9209e888d9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  bc\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 86.3 kB of archives.\n",
            "After this operation, 231 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 bc amd64 1.07.1-2build1 [86.3 kB]\n",
            "Fetched 86.3 kB in 1s (80.6 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package bc.\n",
            "(Reading database ... 122586 files and directories currently installed.)\n",
            "Preparing to unpack .../bc_1.07.1-2build1_amd64.deb ...\n",
            "Unpacking bc (1.07.1-2build1) ...\n",
            "Setting up bc (1.07.1-2build1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install bc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fddT73ZEuE5N"
      },
      "outputs": [],
      "source": [
        "!mkdir -p test/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acwHVMb-uDJC"
      },
      "outputs": [],
      "source": [
        "!cp /content/v1.0/data/test/1eu9y3ZPahI.wav test/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqGF-ATBl5a_",
        "outputId": "c10fc5cd-a63d-40a7-db71-c8837a32f01a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Set the input WAV file path\n",
        "export INPUT_WAV_FOLDER=\"/content/v1.0/data\"\n",
        "\n",
        "# Set the output directory for FLAC files\n",
        "export OUTPUT_FLAC_DIR=\"/content/v2.0\"\n",
        "mkdir -p \"$OUTPUT_FLAC_DIR\"\n",
        "\n",
        "# Set the maximum length of each split file in seconds\n",
        "MAX_LENGTH=30\n",
        "\n",
        "# Loop through all WAV files in the input folder\n",
        "for file in \"$INPUT_WAV_FOLDER\"/*/*.wav; do\n",
        "  # Get the file name without extension\n",
        "  filename=$(basename -- \"$file\")\n",
        "  filename=\"${filename%.*}\"\n",
        "\n",
        "  # Convert the WAV file to FLAC format\n",
        "  ffmpeg -i \"$file\" -compression_level 12 \"$OUTPUT_FLAC_DIR/output.flac\" > /dev/null\n",
        "  rm \"$file\"\n",
        "  # Split the FLAC file into multiple files of up to 30 seconds each\n",
        "  sox \"$OUTPUT_FLAC_DIR/output.flac\" \"$OUTPUT_FLAC_DIR/${filename}_%03d.flac\" trim 0 $MAX_LENGTH : newfile : restart\n",
        "  rm \"$OUTPUT_FLAC_DIR/output.flac\"\n",
        "  echo \"Conversion and splitting of $file complete.\"\n",
        "done\n",
        "\n",
        "echo \"Processing complete for all files in $INPUT_WAV_FOLDER.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO6JruQIrNBK"
      },
      "outputs": [],
      "source": [
        "%%bash \n",
        "OUTPUT_FLAC_DIR=\"/content/v2.0\"\n",
        "MAX_LENGTH=30\n",
        "# Loop through each split file and check its duration\n",
        "for f in \"$OUTPUT_FLAC_DIR/\"*; do\n",
        "  # Get the duration of the current file in seconds\n",
        "  duration=$(soxi -D \"$f\")\n",
        "\n",
        "  # If the duration is less or more than the maximum length, delete the file\n",
        "  if (( $(bc <<< \"$duration < $MAX_LENGTH\") )) || (( $(bc <<< \"$duration > $MAX_LENGTH\") )); then\n",
        "    rm \"$f\"\n",
        "  fi\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFbUOV1PmEhI"
      },
      "outputs": [],
      "source": [
        "!zip -rq v2_1.zip \"v2.0/\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FR0nJQp7FQ6"
      },
      "outputs": [],
      "source": [
        "!mkdir -p \"/content/gdrive/MyDrive/IDL_Project/data/releases/v2.1\"\n",
        "!cp -R \"/content/v2_1.zip\" \"/content/gdrive/MyDrive/IDL_Project/data/releases\" \n",
        "!cp -R \"/content/v2.0/\" \"/content/gdrive/MyDrive/IDL_Project/data/releases/v2.1\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjjRTPyZcnZb"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "jh-j5yPMcnsw"
      },
      "outputs": [],
      "source": [
        "config_semantic = dict(\n",
        "        model=\"semantic\",\n",
        "        embedding_dim = 1024,\n",
        "        depth = 6,\n",
        "        heads = 8,\n",
        "        batch_size=8,\n",
        "        data_max_length_seconds = 30, #30\n",
        "        lr=5e-4, #1e-3\n",
        "        warmup_steps=1000,\n",
        "        wd=0.,\n",
        "        attn_dropout=0.0,\n",
        "        ff_dropout=0.0,\n",
        "        grad_accum_every=1,\n",
        "        max_grad_norm=0.5,\n",
        "        scheduler=\"attention_scheduler\",\n",
        "    )\n",
        "\n",
        "config_coarse = dict(\n",
        "        model=\"coarse\",\n",
        "        embedding_dim = 1024, #1024\n",
        "        depth = 12, #6\n",
        "        heads = 16, #8\n",
        "        batch_size=1,\n",
        "        data_max_length_seconds = 7,\n",
        "        lr=1e-4,\n",
        "        warmup_steps=1500,\n",
        "        wd=0.,\n",
        "        attn_dropout=0.0,\n",
        "        ff_dropout=0.0,\n",
        "        grad_accum_every=1,\n",
        "        max_grad_norm=0.5, #0.5 2.0\n",
        "        scheduler=\"attention_scheduler\"\n",
        "    )\n",
        "\n",
        "config_fine = dict(\n",
        "        model=\"fine\",\n",
        "        batch_size=4,\n",
        "        data_max_length_seconds = 3,\n",
        "        lr=1e-3, #1e-3\n",
        "        wd=0.,\n",
        "        attn_dropout=0.0,\n",
        "        ff_dropout=0.0,\n",
        "        grad_accum_every=1,\n",
        "        max_grad_norm=0.5,\n",
        "        scheduler=\"reducelr\",\n",
        "    )\n",
        "\n",
        "config = config_semantic #SELECT ME!!!\n",
        "SAVE_MODEL_EVERY = 400\n",
        "VAL_EVERY_STEPS = 20\n",
        "STEPS = 20000 # 300 fro testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MhmVMu-Idzoc"
      },
      "outputs": [],
      "source": [
        "load_ckpt = False\n",
        "train = True\n",
        "DATA_PATH=\"/content/v2.0\"\n",
        "RESULTS_ROOT_PATH=\"/content/models\" #\"/content/gdrive/MyDrive/IDL_Project/models\"\n",
        "semantic_load_path = RESULTS_ROOT_PATH+'/results_semantic/semantic.transformer.50.pt' \n",
        "coarse_load_path = RESULTS_ROOT_PATH+'/results_coarse/coarse.transformer.300.pt'\n",
        "fine_load_path = RESULTS_ROOT_PATH+'/results_fine/fine.transformer.300.pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBaqCz5jb_l_"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jQpcleFTK0NJ",
        "outputId": "80681717-24b3-4768-c18f-332286272a72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'020239'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import datetime\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "minutes_today = (now.hour * 60) + now.minute\n",
        "days = now.day  \n",
        "hours = minutes_today // 60\n",
        "minutes = minutes_today % 60\n",
        "\n",
        "# Format the output string as \"day-hour-minute\"\n",
        "suffix = \"{:02d}{:02d}{:02d}\".format(days, hours, minutes)\n",
        "suffix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u1VuDAUcABX",
        "outputId": "a2e1d118-dcba-4745-a352-a2d9ab4a551a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mols\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "import wandb as wandb_org\n",
        "using_wandb= True\n",
        "if using_wandb:\n",
        "  wandb = wandb_org\n",
        "  wandb.login(key=\"d599fc284dc6e3f8363808c9da20e06180ec7462\")\n",
        "  # run = wandb.init(\n",
        "  #             name=f\"{config['model']}_{suffix}\",\n",
        "  #             resume=False,\n",
        "  #             project=\"project\",\n",
        "  #             entity=\"cmu-idl\",\n",
        "  #             config=config,\n",
        "  #         )\n",
        "  # RUN_ID=run.id\n",
        "  # print(RUN_ID)\n",
        "else:\n",
        "  wandb = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SglfcvOvmS_Z"
      },
      "source": [
        "#CLAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEFSss2wqCtL"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yU8Jawod3kN3"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BNVJt9bj08XT"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import joblib\n",
        "\n",
        "REPO_ID = \"lukewys/laion_clap\"\n",
        "FILENAME = \"music_audioset_epoch_15_esc_90.14.pt\"\n",
        "\n",
        "clap_ckp_path=hf_hub_download(repo_id=REPO_ID, filename=FILENAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cv0KTESo4Fn",
        "outputId": "3900760f-c89c-4ce2-b36a-be1d7094709a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load the specified checkpoint /root/.cache/huggingface/hub/models--lukewys--laion_clap/snapshots/b3708341862f581175dba5c356a4ebf74a9b6651/music_audioset_epoch_15_esc_90.14.pt from users.\n",
            "Load Checkpoint...\n",
            "logit_scale_a \t Loaded\n",
            "logit_scale_t \t Loaded\n",
            "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
            "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
            "audio_branch.logmel_extractor.melW \t Loaded\n",
            "audio_branch.bn0.weight \t Loaded\n",
            "audio_branch.bn0.bias \t Loaded\n",
            "audio_branch.patch_embed.proj.weight \t Loaded\n",
            "audio_branch.patch_embed.proj.bias \t Loaded\n",
            "audio_branch.patch_embed.norm.weight \t Loaded\n",
            "audio_branch.patch_embed.norm.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.6.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.7.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.8.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.9.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.10.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.norm2.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.2.blocks.11.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
            "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
            "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
            "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
            "audio_branch.norm.weight \t Loaded\n",
            "audio_branch.norm.bias \t Loaded\n",
            "audio_branch.tscam_conv.weight \t Loaded\n",
            "audio_branch.tscam_conv.bias \t Loaded\n",
            "audio_branch.head.weight \t Loaded\n",
            "audio_branch.head.bias \t Loaded\n",
            "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
            "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
            "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
            "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
            "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
            "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
            "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
            "text_branch.pooler.dense.weight \t Loaded\n",
            "text_branch.pooler.dense.bias \t Loaded\n",
            "text_transform.sequential.0.weight \t Loaded\n",
            "text_transform.sequential.0.bias \t Loaded\n",
            "text_transform.sequential.3.weight \t Loaded\n",
            "text_transform.sequential.3.bias \t Loaded\n",
            "text_projection.0.weight \t Loaded\n",
            "text_projection.0.bias \t Loaded\n",
            "text_projection.2.weight \t Loaded\n",
            "text_projection.2.bias \t Loaded\n",
            "audio_transform.sequential.0.weight \t Loaded\n",
            "audio_transform.sequential.0.bias \t Loaded\n",
            "audio_transform.sequential.3.weight \t Loaded\n",
            "audio_transform.sequential.3.bias \t Loaded\n",
            "audio_projection.0.weight \t Loaded\n",
            "audio_projection.0.bias \t Loaded\n",
            "audio_projection.2.weight \t Loaded\n",
            "audio_projection.2.bias \t Loaded\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import laion_clap\n",
        "\n",
        "# quantization\n",
        "def int16_to_float32(x):\n",
        "    return (x / 32767.0).astype(np.float32)\n",
        "\n",
        "\n",
        "def float32_to_int16(x):\n",
        "    x = np.clip(x, a_min=-1., a_max=1.)\n",
        "    return (x * 32767.).astype(np.int16)\n",
        "\n",
        "clap_model = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base')\n",
        "clap_model.load_ckpt(clap_ckp_path) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiEBo6goqFRE"
      },
      "source": [
        "### Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wJJrpfgUqNhE"
      },
      "outputs": [],
      "source": [
        "# Directly get audio embeddings from audio files\n",
        "run_this=False\n",
        "if run_this:\n",
        "  audio_file = [\n",
        "      DATA_PATH+'/train/-y3Abv2ovkU.wav',\n",
        "      DATA_PATH+'/train/BNyoM9gvGMs.wav'\n",
        "  ]\n",
        "  audio_embed = clap_model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=False)\n",
        "  print(audio_embed[:,-20:])\n",
        "  print(audio_embed.shape)\n",
        "\n",
        "  # Get audio embeddings from audio data\n",
        "  audio_data, _ = librosa.load(DATA_PATH+'/train/-y3Abv2ovkU.wav', sr=48000) # sample rate should be 48000\n",
        "  audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n",
        "  audio_embed = clap_model.get_audio_embedding_from_data(x = audio_data, use_tensor=False)\n",
        "  print(audio_embed[:,-20:])\n",
        "  print(audio_embed.shape)\n",
        "\n",
        "  # Directly get audio embeddings from audio files, but return torch tensor\n",
        "  audio_embed = clap_model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=True)\n",
        "  print(audio_embed[:,-20:])\n",
        "  print(audio_embed.shape)\n",
        "\n",
        "  # Get audio embeddings from audio data\n",
        "  audio_data, _ = librosa.load(DATA_PATH+'/train/-y3Abv2ovkU.wav', sr=48000) # sample rate should be 48000\n",
        "  audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n",
        "  audio_data = torch.from_numpy(int16_to_float32(float32_to_int16(audio_data))).float() # quantize before send it in to the model\n",
        "  audio_embed = clap_model.get_audio_embedding_from_data(x = audio_data, use_tensor=True)\n",
        "  print(audio_embed[:,-20:])\n",
        "  print(audio_embed.shape)\n",
        "\n",
        "  # Get text embedings from texts:\n",
        "  text_data = [\"I love the contrastive learning\", \"I love the pretrain model\"] \n",
        "  text_embed = clap_model.get_text_embedding(text_data)\n",
        "  print(text_embed)\n",
        "  print(text_embed.shape)\n",
        "\n",
        "  # Get text embedings from texts, but return torch tensor:\n",
        "  text_data = [\"I love the contrastive learning\", \"I love the pretrain model\"] \n",
        "  text_embed = clap_model.get_text_embedding(text_data, use_tensor=True)\n",
        "  print(text_embed)\n",
        "  print(text_embed.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZizSIRjnwDlI",
        "outputId": "418739e4-f3ef-459b-916b-c23f2c7fbe3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "wavs = torch.randn(2, 1024)\n",
        "texts = torch.randint(0, 20000, (2, 256))\n",
        "clap_model.get_audio_embedding_from_data(x = wavs, use_tensor=True).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhK_3FSVqF2t"
      },
      "source": [
        "### Quantizer for AudioML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ihv8jANAqN-J"
      },
      "outputs": [],
      "source": [
        "# Based on MuLaNEmbedQuantizer implementation on https://github.com/lucidrains/musiclm-pytorch\n",
        "from audiolm_pytorch.utils import AudioConditionerBase\n",
        "from beartype.typing import List, Optional, Tuple\n",
        "from beartype import beartype\n",
        "from einops import rearrange, repeat, reduce, pack, unpack\n",
        "from einops.layers.torch import Rearrange\n",
        "from vector_quantize_pytorch import ResidualVQ\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "class CLAPEmbedQuantizer(AudioConditionerBase):\n",
        "    @beartype\n",
        "    def __init__(\n",
        "        self,\n",
        "        clap: laion_clap.CLAP_Module,\n",
        "        conditioning_dims: Tuple[int, ...],\n",
        "        rq_num_quantizers = 8,\n",
        "        rq_ema_decay = 0.9,\n",
        "        codebook_size = 1024,\n",
        "        namespaces: Tuple[str, ...] = ('semantic', 'coarse', 'fine'),\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.clap = clap\n",
        "\n",
        "        assert len(namespaces) > 0\n",
        "        self.namespaces = namespaces\n",
        "        self.conditioning_dims = conditioning_dims\n",
        "\n",
        "        assert len(conditioning_dims) == len(namespaces), 'number of conditioning dimensions must be equal to number of namespaces'\n",
        "\n",
        "        dim = clap.model.audio_projection[-1].out_features\n",
        "\n",
        "        self.rq = ResidualVQ(\n",
        "            dim = dim,\n",
        "            num_quantizers = rq_num_quantizers,\n",
        "            codebook_size = codebook_size,\n",
        "            decay = rq_ema_decay,\n",
        "            commitment_weight = 0,    # only use EMA to update codebooks\n",
        "            kmeans_init = True,\n",
        "            threshold_ema_dead_code = 2,\n",
        "            quantize_dropout = False  # no quantize dropout\n",
        "        )\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_codebooks = rq_num_quantizers\n",
        "\n",
        "        self.cond_embeddings = nn.ParameterDict({})\n",
        "\n",
        "        for namespace, conditioning_dim in zip(namespaces, conditioning_dims):\n",
        "            cond_embeddings = nn.Parameter(torch.randn(rq_num_quantizers, codebook_size, conditioning_dim))\n",
        "            nn.init.normal_(cond_embeddings, std = 0.02)\n",
        "\n",
        "            self.cond_embeddings[namespace] = cond_embeddings\n",
        "\n",
        "        self.set_default_namespace(namespaces[0])\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.cond_embeddings.parameters()\n",
        "\n",
        "    def set_default_namespace(self, namespace):\n",
        "        self._default_namespace = namespace\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        wavs = None,\n",
        "        texts = None,\n",
        "        namespace = None\n",
        "    ):\n",
        "        assert exists(wavs) ^ exists(texts)\n",
        "\n",
        "        namespace = default(namespace, self._default_namespace)\n",
        "        assert namespace in self.namespaces, f'namespace {namespace} not found'\n",
        "        cond_embeddings = self.cond_embeddings[namespace]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.clap.eval()\n",
        "\n",
        "            # sound and language live in joint embedding space because of contrastive learning\n",
        "\n",
        "            if exists(wavs):\n",
        "                latents = self.clap.get_audio_embedding_from_data(x = wavs, use_tensor=True) \n",
        "            elif exists(texts):\n",
        "                latents = self.clap.get_text_embedding(texts, use_tensor=True) \n",
        "\n",
        "        _, indices, _ = self.rq(latents)\n",
        "\n",
        "        batch, num_codebooks, dim = indices.shape[0], self.num_codebooks, cond_embeddings.shape[-1]\n",
        "\n",
        "        cond_embeddings = repeat(cond_embeddings, 'q c d -> b q c d', b = batch)\n",
        "        indices = repeat(indices, 'b q -> b q 1 d', q = num_codebooks, d = dim)\n",
        "\n",
        "        cond_embeddings = cond_embeddings.gather(2, indices)\n",
        "        return rearrange(cond_embeddings, 'b q 1 d -> b q d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ExxNRIaavxk4"
      },
      "outputs": [],
      "source": [
        "# setup the quantizer with the namespaced conditioning embeddings, \n",
        "# unique per quantizer as well as namespace (per transformer)\n",
        "quantizer = CLAPEmbedQuantizer(\n",
        "    clap = clap_model,                         \n",
        "    conditioning_dims = (1024, 1024, 1024), # say all three transformers have model dimensions of 1024\n",
        "    namespaces = ('semantic', 'coarse', 'fine')\n",
        ").cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2HUYIETzxSib"
      },
      "outputs": [],
      "source": [
        "# now say you want the conditioning embeddings for semantic transformer\n",
        "# wavs = torch.randn(2, 1024).cuda()\n",
        "# conds = quantizer(wavs = wavs, namespace = 'coarse')\n",
        "# print(conds.shape)\n",
        "# conds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q1fIZMgIFGNJ"
      },
      "outputs": [],
      "source": [
        "# text_embbeddings = quantizer(texts=[\"I love the contrastive learning\", \"\"])[0:1]\n",
        "# text_embbeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYEwiQq93j11"
      },
      "source": [
        "#AudioLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBxNK5cKW--_"
      },
      "source": [
        "### Imports & paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OrNeKngVVM0L"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import math\n",
        "import wave\n",
        "import struct\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from audiolm_pytorch import SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchaudio\n",
        "import gc\n",
        "\n",
        "# define all dataset paths, checkpoints, etc\n",
        "dataset_folder = DATA_PATH\n",
        "soundstream_ckpt = RESULTS_ROOT_PATH+\"/results_soundstream/soundstream.8.pt\" # this can change depending on number of steps\n",
        "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
        "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N3q3VXndzJll"
      },
      "outputs": [],
      "source": [
        "def noop(*args, **kwargs):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYcI0aXEwuxR"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now that we have a dataset, we can train AudioLM.\n",
        "\n",
        "**Note**: do NOT type \"y\" to overwrite previous experiments/ checkpoints when running through the cells here unless you're ready to the entire results folder! Otherwise you will end up erasing things (e.g. you train SoundStream first, and if you choose \"overwrite\" then you lose the SoundStream checkpoint when you then train SemanticTransformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7GiyBcBWiZV"
      },
      "source": [
        "### SoundStream / encodec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Oz4Mll6lVPbt"
      },
      "outputs": [],
      "source": [
        "using_encodec=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wSVRf1Z7V8dT"
      },
      "outputs": [],
      "source": [
        "# from audiolm_pytorch import AudioLMSoundStream, MusicLMSoundStream\n",
        "# MusicLMSoundStream()\n",
        "from audiolm_pytorch import EncodecWrapper\n",
        "soundstream = EncodecWrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nGU0OZiOwPEO"
      },
      "outputs": [],
      "source": [
        "if not using_encodec:\n",
        "  soundstream = SoundStream(\n",
        "      codebook_size = 1024,\n",
        "      rq_num_quantizers = 8,\n",
        "  )\n",
        "\n",
        "  trainer = SoundStreamTrainer( \n",
        "      soundstream,\n",
        "      folder = dataset_folder,\n",
        "      batch_size = 4,\n",
        "      grad_accum_every = 8,         # effective batch size of 32\n",
        "      data_max_length = 320 * 32,\n",
        "      save_results_every = 2,\n",
        "      save_model_every = 4,\n",
        "      num_train_steps = 9,\n",
        "      results_folder = RESULTS_ROOT_PATH+'/results_soundstream',\n",
        "      force_clear_prev_results = True,\n",
        "  ).cuda()\n",
        "  # NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
        "  # adjusting save_*_every variables for the same reason\n",
        "\n",
        "  trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKAFNMrqpMBC"
      },
      "source": [
        "### wav2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bu8Ve0-pO9J",
        "outputId": "fe859a9a-a126-4bbd-f84c-16399f66305d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# hubert checkpoints can be downloaded at\n",
        "# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
        "if not os.path.isdir(\"hubert\"):\n",
        "  os.makedirs(\"hubert\")\n",
        "if not os.path.isfile(hubert_ckpt):\n",
        "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
        "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
        "if not os.path.isfile(hubert_quantizer):\n",
        "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
        "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n",
        "\n",
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = f'./{hubert_ckpt}',\n",
        "    kmeans_path = f'./{hubert_quantizer}'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOCVHaqWtC3D",
        "outputId": "e1a1a55d-5ad9-44dd-a699-3b7437d45c01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "wav2vec.target_sample_hz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom classes"
      ],
      "metadata": {
        "id": "hsLvQ0-6T8Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Taken from https://kikaben.com/transformers-training-details/\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class AttentionPScheduler(_LRScheduler):\n",
        "    def __init__(self, \n",
        "                 optimizer: Optimizer,\n",
        "                 dim_embed: int,\n",
        "                 warmup_steps: int,\n",
        "                 last_epoch: int=-1,\n",
        "                 verbose: bool=False) -> None:\n",
        "\n",
        "        self.dim_embed = dim_embed\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.num_param_groups = len(optimizer.param_groups)\n",
        "\n",
        "        super().__init__(optimizer, last_epoch, verbose)\n",
        "        \n",
        "    def get_lr(self) -> float:\n",
        "        lr = calc_lr(self._step_count, self.dim_embed, self.warmup_steps)\n",
        "        return [lr] * self.num_param_groups\n",
        "\n",
        "\n",
        "def calc_lr(step, dim_embed, warmup_steps):\n",
        "    return dim_embed**(-0.5) * min(step**(-0.5), step * warmup_steps**(-1.5))"
      ],
      "metadata": {
        "id": "cWi1_fMhT_L2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhKGvSDSC-Mr"
      },
      "source": [
        "### Common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_S6pCA-F5KsZ"
      },
      "outputs": [],
      "source": [
        "import traceback\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "giauL5TlTD2f"
      },
      "outputs": [],
      "source": [
        "logger = wandb.log if wandb is not None else noop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KS--wGvPEJfj"
      },
      "outputs": [],
      "source": [
        "def free_gpu_memory(model):\n",
        "  for p in model.parameters():\n",
        "      if p.grad is not None:\n",
        "          del p.grad  # free some memory\n",
        "  torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "l3MB2Jzn5HfM"
      },
      "outputs": [],
      "source": [
        "def custom_train(trainer, config, scheduler=None, from_step=None):\n",
        "  logger= noop\n",
        "  if wandb is not None:\n",
        "    logger = wandb.log \n",
        "    wandb.config.update(config)\n",
        "\n",
        "  if from_step is not None:\n",
        "    trainer.steps = from_step\n",
        "  \n",
        "  num_raise_oom=0\n",
        "\n",
        "  while trainer.steps < trainer.num_train_steps:\n",
        "      torch.cuda.empty_cache()\n",
        "      try:\n",
        "          logs = trainer.train_step()\n",
        "\n",
        "          if scheduler is not None:\n",
        "            if isinstance(scheduler, ReduceLROnPlateau):\n",
        "              scheduler.step(logs[\"loss\"])\n",
        "            else:\n",
        "              scheduler.step(trainer.steps)\n",
        "            logs[\"lr\"] = trainer.optim.param_groups[0][\"lr\"]\n",
        "          logger(logs)\n",
        "          num_raise_oom = 0\n",
        "      except RuntimeError as e:\n",
        "\n",
        "          if 'out of memory' in str(e) and num_raise_oom<7:\n",
        "              print('| WARNING: ran out of memory, retrying batch')\n",
        "              free_gpu_memory(trainer.transformer)\n",
        "              num_raise_oom+=1\n",
        "          elif num_raise_oom>= 7:\n",
        "            print(traceback.format_exc())\n",
        "            print(e)\n",
        "            break\n",
        "          else:\n",
        "              print(traceback.format_exc())\n",
        "              print(e)\n",
        "              \n",
        "\n",
        "  model_path = str(trainer.results_folder / f\"transformer.last.pt\")\n",
        "  trainer.save(model_path)\n",
        "  trainer.print(\"training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4PemCzvVB_1w"
      },
      "outputs": [],
      "source": [
        "def get_accelerate_kwargs(config):\n",
        "  return dict(log_with=\"wandb\") if wandb is not None else None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scheduler factory"
      ],
      "metadata": {
        "id": "HYA8-hXCUQsB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ds3-LSnKAP2U"
      },
      "outputs": [],
      "source": [
        "def get_scheduler(optim, config):\n",
        "  if config[\"scheduler\"] == None:\n",
        "    return None\n",
        "  if config[\"scheduler\"] == \"reducelr\":\n",
        "    if config[\"model\"] == \"semantic\":\n",
        "      return ReduceLROnPlateau(optim, \n",
        "                              factor=0.7, patience=15, \n",
        "                              threshold=0.0001, threshold_mode='rel', \n",
        "                              cooldown=0, min_lr=1e-5, eps=1e-08, \n",
        "                              verbose=True)\n",
        "    elif config[\"model\"] == \"coarse\":\n",
        "      return ReduceLROnPlateau(optim, \n",
        "                              factor=0.7, patience=200, \n",
        "                              threshold=0.0001, threshold_mode='rel', \n",
        "                              cooldown=100, min_lr=1e-5, eps=1e-08, \n",
        "                              verbose=True)\n",
        "    elif config[\"model\"] == \"fine\":\n",
        "      return ReduceLROnPlateau(optim, \n",
        "                              factor=0.7, patience=40, \n",
        "                              threshold=0.0001, threshold_mode='rel', \n",
        "                              cooldown=5, min_lr=1e-5, eps=1e-08, \n",
        "                              verbose=True)\n",
        "  if config[\"scheduler\"] == \"attention_scheduler\":\n",
        "    return AttentionPScheduler(optim,  \n",
        "                                dim_embed=config[\"embedding_dim\"] * config[\"heads\"],\n",
        "                                warmup_steps=config.get(\"warmup_steps\",1),\n",
        "                                verbose=False)\n",
        "  \n",
        "  raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqjN28L4Wc5Q"
      },
      "source": [
        "### SemanticTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "F3nnCLqZyM0o"
      },
      "outputs": [],
      "source": [
        "# data_max_length = data_max_length_seconds * wav2vec.target_sample_hz\n",
        "\n",
        "# custom_ds = SoundDataset(\n",
        "#     dataset_folder,\n",
        "#     max_length = data_max_length,\n",
        "#     target_sample_hz = wav2vec.target_sample_hz,\n",
        "#     seq_len_multiple_of = wav2vec.seq_len_multiple_of\n",
        "# )\n",
        "try:\n",
        "  if train:\n",
        "    free_gpu_memory(semantic_transformer)\n",
        "    del semantic_transformer, semantic_trainer\n",
        "except:\n",
        "  pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b0f82a8ec2b64f66a9d2aa6c3fa69865",
            "d589e64142994af6b4703fa708adecb7",
            "403e52fb75154038addbf23795f4bf83",
            "c45978fb10674c6c864517118fbf8736",
            "159b18f1df504487a89b8002a23b56e8",
            "1e1353106fd24520b827e01567c43e4c",
            "2fb5a72d28c642c891498129429fb235",
            "2567923cfe9f41bba9fbd2a28e7d37cf"
          ]
        },
        "id": "qgd962eSvDzS",
        "outputId": "c8f217b3-29a1-46ab-f137-266991e98a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training with dataset of 3950 samples and validating with randomly splitted 208 samples\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:3ke6jkhf) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0f82a8ec2b64f66a9d2aa6c3fa69865"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▇▆▇▅▅▆▅▅▆▇▅▅▇▆▄▃▃█▅▄▆▅▂▁▂▂▃▁▄▂</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▇▆▇▅▅▆▅▅▆▇▅▅▇▆▄▃▃█▅▄▆▅▂▁▂▂▃▁▄▂</td></tr><tr><td>valid_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.38169</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train_loss</td><td>6.38169</td></tr><tr><td>valid_loss</td><td>6.38168</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">peach-water-6</strong> at: <a href='https://wandb.ai/ols/semantic/runs/3ke6jkhf' target=\"_blank\">https://wandb.ai/ols/semantic/runs/3ke6jkhf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230502_025844-3ke6jkhf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:3ke6jkhf). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230502_030307-6fhwhtiq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ols/semantic/runs/6fhwhtiq' target=\"_blank\">kind-dream-7</a></strong> to <a href='https://wandb.ai/ols/semantic' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ols/semantic' target=\"_blank\">https://wandb.ai/ols/semantic</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ols/semantic/runs/6fhwhtiq' target=\"_blank\">https://wandb.ai/ols/semantic/runs/6fhwhtiq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.AttentionPScheduler object at 0x7f4e15c4d0f0>\n",
            "0: loss: 6.407796382904053\n",
            "0: valid loss 6.40590763092041\n",
            "0: saving model to /content/models/results_semantic\n",
            "1: loss: 6.3984150886535645\n",
            "2: loss: 6.400951385498047\n",
            "3: loss: 6.4098286628723145\n",
            "4: loss: 6.395705699920654\n",
            "5: loss: 6.39543342590332\n",
            "6: loss: 6.4067864418029785\n",
            "7: loss: 6.382561683654785\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "8: loss: 6.376889705657959\n",
            "9: loss: 6.371155261993408\n",
            "10: loss: 6.372185707092285\n",
            "11: loss: 6.353247165679932\n",
            "12: loss: 6.346142292022705\n",
            "13: loss: 6.348211288452148\n",
            "14: loss: 6.343505382537842\n",
            "15: loss: 6.301234245300293\n",
            "16: loss: 6.320677757263184\n",
            "17: loss: 6.286899566650391\n",
            "18: loss: 6.288695812225342\n",
            "19: loss: 6.258553981781006\n",
            "20: loss: 6.243719100952148\n",
            "20: valid loss 6.2432541847229\n",
            "21: loss: 6.229126930236816\n",
            "22: loss: 6.212930679321289\n",
            "23: loss: 6.186420917510986\n",
            "24: loss: 6.1923980712890625\n",
            "25: loss: 6.212419509887695\n",
            "26: loss: 6.113217353820801\n",
            "27: loss: 6.111051082611084\n",
            "28: loss: 6.071352958679199\n",
            "29: loss: 6.078534126281738\n",
            "30: loss: 6.003357887268066\n",
            "31: loss: 5.9884724617004395\n",
            "32: loss: 5.96439266204834\n",
            "33: loss: 5.923735618591309\n",
            "34: loss: 5.946560382843018\n",
            "35: loss: 5.838588714599609\n",
            "36: loss: 5.859656810760498\n",
            "37: loss: 5.866389751434326\n",
            "38: loss: 5.803969383239746\n",
            "39: loss: 5.818819999694824\n",
            "40: loss: 5.691559314727783\n",
            "40: valid loss 5.7418131828308105\n",
            "41: loss: 5.780309200286865\n",
            "42: loss: 5.617382526397705\n",
            "43: loss: 5.726038932800293\n",
            "44: loss: 5.601118564605713\n",
            "45: loss: 5.482565879821777\n",
            "46: loss: 5.671475410461426\n",
            "47: loss: 5.437525272369385\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "48: loss: 5.448891639709473\n",
            "49: loss: 5.6487555503845215\n",
            "50: loss: 5.506927490234375\n",
            "51: loss: 5.300477981567383\n",
            "52: loss: 5.558677673339844\n",
            "53: loss: 5.404686450958252\n",
            "54: loss: 5.506477355957031\n",
            "55: loss: 5.554041385650635\n",
            "56: loss: 5.474678993225098\n",
            "57: loss: 5.344779014587402\n",
            "58: loss: 5.3412628173828125\n",
            "59: loss: 5.33938455581665\n",
            "60: loss: 5.253950119018555\n",
            "60: valid loss 5.363131523132324\n",
            "61: loss: 5.233175754547119\n",
            "62: loss: 5.337016582489014\n",
            "63: loss: 5.116377830505371\n",
            "64: loss: 5.4073920249938965\n",
            "65: loss: 5.302305698394775\n",
            "66: loss: 5.179465293884277\n",
            "67: loss: 5.325121879577637\n",
            "68: loss: 5.4084296226501465\n",
            "69: loss: 5.14041805267334\n",
            "70: loss: 5.16755485534668\n",
            "71: loss: 5.050405979156494\n",
            "72: loss: 5.31564998626709\n",
            "73: loss: 5.031894683837891\n",
            "74: loss: 5.301384925842285\n",
            "75: loss: 5.16133975982666\n",
            "76: loss: 5.182422161102295\n",
            "77: loss: 5.1977152824401855\n",
            "78: loss: 4.965269565582275\n",
            "79: loss: 5.280414581298828\n",
            "80: loss: 5.119248867034912\n",
            "80: valid loss 5.031619071960449\n",
            "81: loss: 5.063929080963135\n",
            "82: loss: 4.997048854827881\n",
            "83: loss: 5.047478199005127\n",
            "84: loss: 5.13170862197876\n",
            "85: loss: 5.063801288604736\n",
            "86: loss: 4.982264518737793\n",
            "87: loss: 5.099342346191406\n",
            "88: loss: 5.037532806396484\n",
            "89: loss: 4.916414260864258\n",
            "90: loss: 4.842164993286133\n",
            "91: loss: 5.050356864929199\n",
            "92: loss: 5.005406379699707\n",
            "93: loss: 4.919682502746582\n",
            "94: loss: 4.958575248718262\n",
            "95: loss: 4.933947563171387\n",
            "96: loss: 4.813350200653076\n",
            "97: loss: 4.814290523529053\n",
            "98: loss: 4.626646518707275\n",
            "99: loss: 4.773284435272217\n",
            "100: loss: 4.752089023590088\n",
            "100: valid loss 4.741609573364258\n",
            "101: loss: 4.797893047332764\n",
            "102: loss: 4.644512176513672\n",
            "103: loss: 4.552005290985107\n",
            "104: loss: 4.767022132873535\n",
            "105: loss: 4.6430745124816895\n",
            "106: loss: 4.61850643157959\n",
            "107: loss: 4.537135601043701\n",
            "108: loss: 4.792855739593506\n",
            "109: loss: 4.616835117340088\n",
            "110: loss: 4.83689546585083\n",
            "111: loss: 4.381420135498047\n",
            "112: loss: 4.5898613929748535\n",
            "113: loss: 4.555317401885986\n",
            "114: loss: 4.665974140167236\n",
            "115: loss: 4.548506736755371\n",
            "116: loss: 4.583437919616699\n",
            "117: loss: 4.544970512390137\n",
            "118: loss: 4.636308670043945\n",
            "119: loss: 4.466184139251709\n",
            "120: loss: 4.360831260681152\n",
            "120: valid loss 4.5616984367370605\n",
            "121: loss: 4.324117183685303\n",
            "122: loss: 4.393500804901123\n",
            "123: loss: 4.435815811157227\n",
            "124: loss: 4.493104457855225\n",
            "125: loss: 4.277628421783447\n",
            "126: loss: 4.290339469909668\n",
            "127: loss: 4.41259241104126\n",
            "128: loss: 4.3772292137146\n",
            "129: loss: 4.253324031829834\n",
            "130: loss: 4.275501728057861\n",
            "131: loss: 4.359826564788818\n",
            "132: loss: 4.490655899047852\n",
            "133: loss: 4.31381368637085\n",
            "134: loss: 4.329563140869141\n",
            "135: loss: 4.300106525421143\n",
            "136: loss: 4.260450839996338\n",
            "137: loss: 4.279183864593506\n",
            "138: loss: 4.212268829345703\n",
            "139: loss: 4.267721652984619\n",
            "140: loss: 4.144763946533203\n",
            "140: valid loss 4.306369304656982\n",
            "141: loss: 4.167838096618652\n",
            "142: loss: 4.1573405265808105\n",
            "143: loss: 4.196822643280029\n",
            "144: loss: 4.174964427947998\n",
            "145: loss: 4.2910356521606445\n",
            "146: loss: 4.228393077850342\n",
            "147: loss: 4.1575212478637695\n",
            "148: loss: 4.104408264160156\n",
            "149: loss: 4.1226725578308105\n",
            "150: loss: 4.170821666717529\n",
            "151: loss: 4.1668171882629395\n",
            "152: loss: 4.1868181228637695\n",
            "153: loss: 4.207265377044678\n",
            "154: loss: 4.097597122192383\n",
            "155: loss: 4.115647315979004\n",
            "156: loss: 4.123803615570068\n",
            "157: loss: 4.040353298187256\n",
            "158: loss: 4.00666618347168\n",
            "159: loss: 4.149011611938477\n",
            "160: loss: 4.127629280090332\n",
            "160: valid loss 4.202968597412109\n",
            "161: loss: 3.981093406677246\n",
            "162: loss: 4.007931709289551\n",
            "163: loss: 4.04551362991333\n",
            "164: loss: 4.014496803283691\n",
            "165: loss: 4.0577616691589355\n",
            "166: loss: 3.9871203899383545\n",
            "167: loss: 3.99123477935791\n",
            "168: loss: 3.9327821731567383\n",
            "169: loss: 3.9819889068603516\n",
            "170: loss: 3.841815233230591\n",
            "171: loss: 3.955725908279419\n",
            "172: loss: 3.8542847633361816\n",
            "173: loss: 4.035892009735107\n",
            "174: loss: 3.981522560119629\n",
            "175: loss: 4.0922675132751465\n",
            "176: loss: 4.098443508148193\n",
            "177: loss: 4.015601634979248\n",
            "178: loss: 3.950122117996216\n",
            "179: loss: 3.855286121368408\n",
            "180: loss: 3.798229694366455\n",
            "180: valid loss 3.987086057662964\n",
            "181: loss: 3.876558542251587\n",
            "182: loss: 4.024234771728516\n",
            "183: loss: 4.026522159576416\n",
            "184: loss: 4.032130718231201\n",
            "185: loss: 3.9822170734405518\n",
            "186: loss: 3.937160015106201\n",
            "187: loss: 3.9031736850738525\n",
            "188: loss: 3.7899327278137207\n",
            "189: loss: 4.007061958312988\n",
            "190: loss: 4.045432090759277\n",
            "191: loss: 3.798283100128174\n",
            "192: loss: 3.8811697959899902\n",
            "193: loss: 3.913585662841797\n",
            "194: loss: 3.899797201156616\n",
            "195: loss: 3.909079074859619\n",
            "196: loss: 3.83379864692688\n",
            "197: loss: 3.9530839920043945\n",
            "198: loss: 3.8720672130584717\n",
            "199: loss: 3.8132123947143555\n",
            "200: loss: 3.7746741771698\n",
            "200: valid loss 3.887911796569824\n",
            "201: loss: 3.823230028152466\n",
            "202: loss: 3.879045009613037\n",
            "203: loss: 3.7227842807769775\n",
            "204: loss: 3.904942512512207\n",
            "205: loss: 3.7731223106384277\n",
            "206: loss: 3.7294960021972656\n",
            "207: loss: 3.8535830974578857\n",
            "208: loss: 3.864833354949951\n",
            "209: loss: 3.738744020462036\n",
            "210: loss: 3.6457221508026123\n",
            "211: loss: 3.9177088737487793\n",
            "212: loss: 3.793302536010742\n",
            "213: loss: 3.6983489990234375\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "214: loss: 3.8253183364868164\n",
            "215: loss: 3.8015642166137695\n",
            "216: loss: 3.9272940158843994\n",
            "217: loss: 3.75284743309021\n",
            "218: loss: 3.7492196559906006\n",
            "219: loss: 3.7247941493988037\n",
            "220: loss: 3.8005523681640625\n",
            "220: valid loss 3.6996445655822754\n",
            "221: loss: 3.7107748985290527\n",
            "222: loss: 3.7537147998809814\n",
            "223: loss: 3.832867383956909\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "224: loss: 3.777322292327881\n",
            "225: loss: 3.765759229660034\n",
            "226: loss: 3.910726547241211\n",
            "227: loss: 3.6373612880706787\n",
            "228: loss: 3.8053956031799316\n",
            "229: loss: 3.7307240962982178\n",
            "230: loss: 3.7223479747772217\n",
            "231: loss: 3.8337466716766357\n",
            "232: loss: 3.7704153060913086\n",
            "233: loss: 3.811018943786621\n",
            "234: loss: 3.748687267303467\n",
            "235: loss: 3.816943407058716\n",
            "236: loss: 3.7945797443389893\n",
            "237: loss: 3.877181053161621\n",
            "238: loss: 3.8663973808288574\n",
            "239: loss: 3.719169855117798\n",
            "240: loss: 3.6413254737854004\n",
            "240: valid loss 3.748112916946411\n",
            "241: loss: 3.75799560546875\n",
            "242: loss: 3.7450308799743652\n",
            "243: loss: 3.762162923812866\n",
            "244: loss: 3.689293384552002\n",
            "245: loss: 3.7298190593719482\n",
            "246: loss: 3.793476104736328\n",
            "247: loss: 3.786982297897339\n",
            "248: loss: 3.8394291400909424\n",
            "249: loss: 3.671372652053833\n",
            "250: loss: 3.690631151199341\n",
            "251: loss: 3.704782485961914\n",
            "252: loss: 3.672271728515625\n",
            "253: loss: 3.655099630355835\n",
            "254: loss: 3.8648154735565186\n",
            "255: loss: 3.711487293243408\n",
            "256: loss: 3.7294352054595947\n",
            "257: loss: 3.6542060375213623\n",
            "258: loss: 3.6586272716522217\n",
            "259: loss: 3.7775981426239014\n",
            "260: loss: 3.682305335998535\n",
            "260: valid loss 3.5141172409057617\n",
            "261: loss: 3.764861583709717\n",
            "262: loss: 3.7773590087890625\n",
            "263: loss: 3.6521170139312744\n",
            "264: loss: 3.784213066101074\n",
            "265: loss: 3.7160627841949463\n",
            "266: loss: 3.755096912384033\n",
            "267: loss: 3.7623655796051025\n",
            "268: loss: 3.730273962020874\n",
            "269: loss: 3.5696604251861572\n",
            "270: loss: 3.685248613357544\n",
            "271: loss: 3.693930149078369\n",
            "272: loss: 3.5713438987731934\n",
            "273: loss: 3.713230609893799\n",
            "274: loss: 3.543431043624878\n",
            "275: loss: 3.6669118404388428\n",
            "276: loss: 3.6979055404663086\n",
            "277: loss: 3.723726749420166\n",
            "278: loss: 3.691549777984619\n",
            "279: loss: 3.5051426887512207\n",
            "280: loss: 3.764094352722168\n",
            "280: valid loss 3.7490627765655518\n",
            "281: loss: 3.742950916290283\n",
            "282: loss: 3.6411383152008057\n",
            "283: loss: 3.631042003631592\n",
            "284: loss: 3.7194600105285645\n",
            "285: loss: 3.6387391090393066\n",
            "286: loss: 3.681694984436035\n",
            "287: loss: 3.7519679069519043\n",
            "288: loss: 3.7039294242858887\n",
            "289: loss: 3.5175957679748535\n",
            "290: loss: 3.6575496196746826\n",
            "291: loss: 3.59142804145813\n",
            "292: loss: 3.5059869289398193\n",
            "293: loss: 3.6500003337860107\n",
            "294: loss: 3.6821377277374268\n",
            "295: loss: 3.6873104572296143\n",
            "296: loss: 3.563319444656372\n",
            "297: loss: 3.6494359970092773\n",
            "298: loss: 3.3816230297088623\n",
            "299: loss: 3.7035350799560547\n",
            "300: loss: 3.5556559562683105\n",
            "300: valid loss 3.673332929611206\n",
            "301: loss: 3.7045278549194336\n",
            "302: loss: 3.692241668701172\n",
            "303: loss: 3.5385873317718506\n",
            "304: loss: 3.608772039413452\n",
            "305: loss: 3.651529312133789\n",
            "306: loss: 3.6223645210266113\n",
            "307: loss: 3.617353916168213\n",
            "308: loss: 3.738065481185913\n",
            "309: loss: 3.611982822418213\n",
            "310: loss: 3.668247938156128\n",
            "311: loss: 3.530306577682495\n",
            "312: loss: 3.4733312129974365\n",
            "313: loss: 3.590372323989868\n",
            "314: loss: 3.601252794265747\n",
            "315: loss: 3.803565740585327\n",
            "316: loss: 3.6925790309906006\n",
            "317: loss: 3.5673513412475586\n",
            "318: loss: 3.648324728012085\n",
            "319: loss: 3.6316001415252686\n",
            "320: loss: 3.667802333831787\n",
            "320: valid loss 3.711071729660034\n",
            "321: loss: 3.732954740524292\n",
            "322: loss: 3.653351068496704\n",
            "323: loss: 3.5432651042938232\n",
            "324: loss: 3.5565719604492188\n",
            "325: loss: 3.7248072624206543\n",
            "326: loss: 3.6659350395202637\n",
            "327: loss: 3.615445137023926\n",
            "328: loss: 3.466254234313965\n",
            "329: loss: 3.5185141563415527\n",
            "330: loss: 3.5836687088012695\n",
            "331: loss: 3.622563600540161\n",
            "332: loss: 3.5496551990509033\n",
            "333: loss: 3.6574912071228027\n",
            "334: loss: 3.640761613845825\n",
            "335: loss: 3.635742664337158\n",
            "336: loss: 3.603720188140869\n",
            "337: loss: 3.492429733276367\n",
            "338: loss: 3.690479040145874\n",
            "339: loss: 3.5487043857574463\n",
            "340: loss: 3.547130823135376\n",
            "340: valid loss 3.6764979362487793\n",
            "341: loss: 3.603407144546509\n",
            "342: loss: 3.630581855773926\n",
            "343: loss: 3.6841073036193848\n",
            "344: loss: 3.666233539581299\n",
            "345: loss: 3.5255541801452637\n",
            "346: loss: 3.739140748977661\n",
            "347: loss: 3.5980238914489746\n",
            "348: loss: 3.633422374725342\n",
            "349: loss: 3.5268383026123047\n",
            "350: loss: 3.68306827545166\n",
            "351: loss: 3.486156702041626\n",
            "352: loss: 3.66204833984375\n",
            "353: loss: 3.480757713317871\n",
            "354: loss: 3.7486705780029297\n",
            "355: loss: 3.5738532543182373\n",
            "356: loss: 3.6511240005493164\n",
            "357: loss: 3.340820550918579\n",
            "358: loss: 3.5765573978424072\n",
            "359: loss: 3.577195882797241\n",
            "360: loss: 3.605719566345215\n",
            "360: valid loss 3.5508313179016113\n",
            "361: loss: 3.4697771072387695\n",
            "362: loss: 3.6357860565185547\n",
            "363: loss: 3.43304443359375\n",
            "364: loss: 3.653912305831909\n",
            "365: loss: 3.5061609745025635\n",
            "366: loss: 3.6508240699768066\n",
            "367: loss: 3.589878797531128\n",
            "368: loss: 3.6549816131591797\n",
            "369: loss: 3.7142159938812256\n",
            "370: loss: 3.5762386322021484\n",
            "371: loss: 3.6237080097198486\n",
            "372: loss: 3.6066246032714844\n",
            "373: loss: 3.5694093704223633\n",
            "374: loss: 3.5820910930633545\n",
            "375: loss: 3.7022969722747803\n",
            "376: loss: 3.4173636436462402\n",
            "377: loss: 3.570512533187866\n",
            "378: loss: 3.5582969188690186\n",
            "379: loss: 3.6670138835906982\n",
            "380: loss: 3.5261857509613037\n",
            "380: valid loss 3.5881426334381104\n",
            "381: loss: 3.5359299182891846\n",
            "382: loss: 3.5949599742889404\n",
            "383: loss: 3.5510432720184326\n",
            "384: loss: 3.61037015914917\n",
            "385: loss: 3.3814754486083984\n",
            "386: loss: 3.6070077419281006\n",
            "387: loss: 3.4839210510253906\n",
            "388: loss: 3.5974721908569336\n",
            "389: loss: 3.7063510417938232\n",
            "390: loss: 3.598393678665161\n",
            "391: loss: 3.5583276748657227\n",
            "392: loss: 3.4262940883636475\n",
            "393: loss: 3.603264331817627\n",
            "394: loss: 3.603062868118286\n",
            "395: loss: 3.643982172012329\n",
            "396: loss: 3.4762775897979736\n",
            "397: loss: 3.53631591796875\n",
            "398: loss: 3.640805244445801\n",
            "399: loss: 3.512482166290283\n",
            "400: loss: 3.5011422634124756\n",
            "400: valid loss 3.5669236183166504\n",
            "400: saving model to /content/models/results_semantic\n",
            "401: loss: 3.4498708248138428\n",
            "402: loss: 3.5513241291046143\n",
            "403: loss: 3.6628828048706055\n",
            "404: loss: 3.462656259536743\n",
            "405: loss: 3.600980281829834\n",
            "406: loss: 3.4687750339508057\n",
            "407: loss: 3.529026985168457\n",
            "408: loss: 3.428926706314087\n",
            "409: loss: 3.636749744415283\n",
            "410: loss: 3.521852970123291\n",
            "411: loss: 3.486785888671875\n",
            "412: loss: 3.492934226989746\n",
            "413: loss: 3.639570474624634\n",
            "414: loss: 3.6035714149475098\n",
            "415: loss: 3.4517335891723633\n",
            "416: loss: 3.6580371856689453\n",
            "417: loss: 3.467517137527466\n",
            "418: loss: 3.6057820320129395\n",
            "419: loss: 3.4625139236450195\n",
            "420: loss: 3.6027257442474365\n",
            "420: valid loss 3.577157497406006\n",
            "421: loss: 3.480149984359741\n",
            "422: loss: 3.604800224304199\n",
            "423: loss: 3.5379879474639893\n",
            "424: loss: 3.5162878036499023\n",
            "425: loss: 3.6168758869171143\n",
            "426: loss: 3.501864433288574\n",
            "427: loss: 3.6025099754333496\n",
            "428: loss: 3.5493290424346924\n",
            "429: loss: 3.5997066497802734\n",
            "430: loss: 3.627121925354004\n",
            "431: loss: 3.627953290939331\n",
            "432: loss: 3.577141046524048\n",
            "433: loss: 3.4280331134796143\n",
            "434: loss: 3.6019983291625977\n",
            "435: loss: 3.5195014476776123\n",
            "436: loss: 3.5755271911621094\n",
            "437: loss: 3.5188705921173096\n",
            "438: loss: 3.7245371341705322\n",
            "439: loss: 3.540712356567383\n",
            "440: loss: 3.665144205093384\n",
            "440: valid loss 3.4454307556152344\n",
            "441: loss: 3.4212160110473633\n",
            "442: loss: 3.635347366333008\n",
            "443: loss: 3.5361106395721436\n",
            "444: loss: 3.634219169616699\n",
            "445: loss: 3.587130069732666\n",
            "446: loss: 3.44793963432312\n",
            "447: loss: 3.465965986251831\n",
            "448: loss: 3.4660959243774414\n",
            "449: loss: 3.5801968574523926\n",
            "450: loss: 3.509996175765991\n",
            "451: loss: 3.4880733489990234\n",
            "452: loss: 3.4236249923706055\n",
            "453: loss: 3.5054931640625\n",
            "454: loss: 3.525895118713379\n",
            "455: loss: 3.548612117767334\n",
            "456: loss: 3.6078031063079834\n",
            "457: loss: 3.4829585552215576\n",
            "458: loss: 3.658687114715576\n",
            "459: loss: 3.5032498836517334\n",
            "460: loss: 3.491809129714966\n",
            "460: valid loss 3.510808229446411\n",
            "461: loss: 3.4663333892822266\n",
            "462: loss: 3.5668649673461914\n",
            "463: loss: 3.4575157165527344\n",
            "464: loss: 3.3624353408813477\n",
            "465: loss: 3.5446603298187256\n",
            "466: loss: 3.54510760307312\n",
            "467: loss: 3.64180326461792\n",
            "468: loss: 3.4624183177948\n",
            "469: loss: 3.490147829055786\n",
            "470: loss: 3.2833025455474854\n",
            "471: loss: 3.6098721027374268\n",
            "472: loss: 3.68051815032959\n",
            "473: loss: 3.4796526432037354\n",
            "474: loss: 3.561448097229004\n",
            "475: loss: 3.5316460132598877\n",
            "476: loss: 3.5356247425079346\n",
            "477: loss: 3.437800884246826\n",
            "478: loss: 3.4849178791046143\n",
            "479: loss: 3.3360037803649902\n",
            "480: loss: 3.618530035018921\n",
            "480: valid loss 3.4216840267181396\n",
            "481: loss: 3.5077908039093018\n",
            "482: loss: 3.548750400543213\n",
            "483: loss: 3.481485366821289\n",
            "484: loss: 3.4451982975006104\n",
            "485: loss: 3.4116218090057373\n",
            "486: loss: 3.5131256580352783\n",
            "487: loss: 3.634366035461426\n",
            "488: loss: 3.586992025375366\n",
            "489: loss: 3.405348777770996\n",
            "490: loss: 3.436361074447632\n",
            "491: loss: 3.3646655082702637\n",
            "492: loss: 3.5947625637054443\n",
            "493: loss: 3.3089888095855713\n",
            "494: loss: 3.4610636234283447\n",
            "495: loss: 3.395141124725342\n",
            "496: loss: 3.5153141021728516\n",
            "497: loss: 3.3555877208709717\n",
            "498: loss: 3.464291572570801\n",
            "499: loss: 3.3710145950317383\n",
            "500: loss: 3.5474109649658203\n",
            "500: valid loss 3.457834005355835\n",
            "501: loss: 3.455568552017212\n",
            "502: loss: 3.5077879428863525\n",
            "503: loss: 3.41796612739563\n",
            "504: loss: 3.290328025817871\n",
            "505: loss: 3.3923280239105225\n",
            "506: loss: 3.403465509414673\n",
            "507: loss: 3.440687656402588\n",
            "508: loss: 3.4099698066711426\n",
            "509: loss: 3.5963664054870605\n",
            "510: loss: 3.3440332412719727\n",
            "511: loss: 3.4260001182556152\n",
            "512: loss: 3.313265562057495\n",
            "513: loss: 3.4968318939208984\n",
            "514: loss: 3.3805103302001953\n",
            "515: loss: 3.4619555473327637\n",
            "516: loss: 3.3729467391967773\n",
            "517: loss: 3.39064884185791\n",
            "518: loss: 3.4666101932525635\n",
            "519: loss: 3.5397937297821045\n",
            "520: loss: 3.3326573371887207\n",
            "520: valid loss 3.4287095069885254\n",
            "521: loss: 3.448509931564331\n",
            "522: loss: 3.34965181350708\n",
            "523: loss: 3.2849385738372803\n",
            "524: loss: 3.451899290084839\n",
            "525: loss: 3.408684253692627\n",
            "526: loss: 3.502561092376709\n",
            "527: loss: 3.31247615814209\n",
            "528: loss: 3.453437089920044\n",
            "529: loss: 3.4927945137023926\n",
            "530: loss: 3.4059455394744873\n",
            "531: loss: 3.405672073364258\n",
            "532: loss: 3.225362539291382\n",
            "533: loss: 3.480971336364746\n",
            "534: loss: 3.2480287551879883\n",
            "535: loss: 3.57672119140625\n",
            "536: loss: 3.348886489868164\n",
            "537: loss: 3.3378231525421143\n",
            "538: loss: 3.5187573432922363\n",
            "539: loss: 3.3675947189331055\n",
            "540: loss: 3.4531993865966797\n",
            "540: valid loss 3.6165502071380615\n",
            "541: loss: 3.500066041946411\n",
            "542: loss: 3.5883989334106445\n",
            "543: loss: 3.4462263584136963\n",
            "544: loss: 3.478393793106079\n",
            "545: loss: 3.411928653717041\n",
            "546: loss: 3.441786527633667\n",
            "547: loss: 3.407855987548828\n",
            "548: loss: 3.3544418811798096\n",
            "549: loss: 3.453967809677124\n",
            "550: loss: 3.4600772857666016\n",
            "551: loss: 3.404541254043579\n",
            "552: loss: 3.4876489639282227\n",
            "553: loss: 3.2609052658081055\n",
            "554: loss: 3.238746404647827\n",
            "555: loss: 3.131389856338501\n",
            "556: loss: 3.5313546657562256\n",
            "557: loss: 3.3895225524902344\n",
            "558: loss: 3.431151866912842\n",
            "559: loss: 3.4361588954925537\n",
            "560: loss: 3.476846218109131\n",
            "560: valid loss 3.470036268234253\n",
            "561: loss: 3.4808177947998047\n",
            "562: loss: 3.4220943450927734\n",
            "563: loss: 3.3932321071624756\n",
            "564: loss: 3.4809370040893555\n",
            "565: loss: 3.343590021133423\n",
            "566: loss: 3.346965789794922\n",
            "567: loss: 3.4511959552764893\n",
            "568: loss: 3.4536194801330566\n",
            "569: loss: 3.3718063831329346\n",
            "570: loss: 3.267021894454956\n",
            "571: loss: 3.4418208599090576\n",
            "572: loss: 3.489356756210327\n",
            "573: loss: 3.4039933681488037\n",
            "574: loss: 3.547828197479248\n",
            "575: loss: 3.3422012329101562\n",
            "576: loss: 3.398282051086426\n",
            "577: loss: 3.5125842094421387\n",
            "578: loss: 3.493877649307251\n",
            "579: loss: 3.3911068439483643\n",
            "580: loss: 3.4469456672668457\n",
            "580: valid loss 3.60526442527771\n",
            "581: loss: 3.535128593444824\n",
            "582: loss: 3.4520819187164307\n",
            "583: loss: 3.5161755084991455\n",
            "584: loss: 3.4075443744659424\n",
            "585: loss: 3.331127166748047\n",
            "586: loss: 3.525900363922119\n",
            "587: loss: 3.371994972229004\n",
            "588: loss: 3.3553547859191895\n",
            "589: loss: 3.4060308933258057\n",
            "590: loss: 3.4596481323242188\n",
            "591: loss: 3.4398751258850098\n",
            "592: loss: 3.404226303100586\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "593: loss: 3.4321377277374268\n",
            "594: loss: 3.3015995025634766\n",
            "595: loss: 3.354830265045166\n",
            "596: loss: 3.511476516723633\n",
            "597: loss: 3.4107470512390137\n",
            "598: loss: 3.4863088130950928\n",
            "599: loss: 3.406616449356079\n",
            "600: loss: 3.4792306423187256\n",
            "600: valid loss 3.5897839069366455\n",
            "601: loss: 3.362220525741577\n",
            "602: loss: 3.43839168548584\n",
            "603: loss: 3.5083277225494385\n",
            "604: loss: 3.462411403656006\n",
            "605: loss: 3.2981925010681152\n",
            "606: loss: 3.516376256942749\n",
            "607: loss: 3.602867603302002\n",
            "608: loss: 3.472341775894165\n",
            "609: loss: 3.3967106342315674\n",
            "610: loss: 3.463681697845459\n",
            "611: loss: 3.416719913482666\n",
            "612: loss: 3.3236823081970215\n",
            "613: loss: 3.487814426422119\n",
            "614: loss: 3.390307664871216\n",
            "615: loss: 3.5131185054779053\n",
            "616: loss: 3.3020215034484863\n",
            "617: loss: 3.5025851726531982\n",
            "618: loss: 3.391087532043457\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "619: loss: 3.3330252170562744\n",
            "620: loss: 3.5002739429473877\n",
            "620: valid loss 3.4892404079437256\n",
            "621: loss: 3.33413028717041\n",
            "622: loss: 3.368105888366699\n",
            "623: loss: 3.556685209274292\n",
            "624: loss: 3.4887752532958984\n",
            "625: loss: 3.4166791439056396\n",
            "626: loss: 3.4530906677246094\n",
            "627: loss: 3.337225914001465\n",
            "628: loss: 3.4745028018951416\n",
            "629: loss: 3.559025764465332\n",
            "630: loss: 3.5538856983184814\n",
            "631: loss: 3.510291814804077\n",
            "632: loss: 3.354422092437744\n",
            "633: loss: 3.4684927463531494\n",
            "634: loss: 3.2994349002838135\n",
            "635: loss: 3.422313690185547\n",
            "636: loss: 3.4829189777374268\n",
            "637: loss: 3.4716954231262207\n",
            "638: loss: 3.4935436248779297\n",
            "639: loss: 3.4458506107330322\n",
            "640: loss: 3.4757981300354004\n",
            "640: valid loss 3.509695291519165\n",
            "641: loss: 3.344252824783325\n",
            "642: loss: 3.400831460952759\n",
            "643: loss: 3.5584652423858643\n",
            "644: loss: 3.5538268089294434\n",
            "645: loss: 3.486110210418701\n",
            "646: loss: 3.585094451904297\n",
            "647: loss: 3.3805124759674072\n",
            "648: loss: 3.399630546569824\n",
            "649: loss: 3.5810747146606445\n",
            "650: loss: 3.4390459060668945\n",
            "651: loss: 3.3439767360687256\n",
            "652: loss: 3.5776352882385254\n",
            "653: loss: 3.4829928874969482\n",
            "654: loss: 3.4555392265319824\n",
            "655: loss: 3.452507495880127\n",
            "656: loss: 3.4330079555511475\n",
            "657: loss: 3.41900897026062\n",
            "658: loss: 3.504642963409424\n",
            "659: loss: 3.5199496746063232\n",
            "660: loss: 3.379277229309082\n",
            "660: valid loss 3.426063299179077\n",
            "661: loss: 3.520089626312256\n",
            "662: loss: 3.4671645164489746\n",
            "663: loss: 3.437751531600952\n",
            "664: loss: 3.4072048664093018\n",
            "665: loss: 3.330899238586426\n",
            "666: loss: 3.541985273361206\n",
            "667: loss: 3.4849672317504883\n",
            "668: loss: 3.4743435382843018\n",
            "669: loss: 3.4511590003967285\n",
            "670: loss: 3.4027481079101562\n",
            "671: loss: 3.5272505283355713\n",
            "672: loss: 3.4535105228424072\n",
            "673: loss: 3.289689540863037\n",
            "674: loss: 3.4892184734344482\n",
            "675: loss: 3.5116395950317383\n",
            "676: loss: 3.425600051879883\n",
            "677: loss: 3.4947509765625\n",
            "678: loss: 3.5774059295654297\n",
            "679: loss: 3.35770845413208\n",
            "680: loss: 3.6030426025390625\n",
            "680: valid loss 3.4096219539642334\n",
            "681: loss: 3.477564573287964\n",
            "682: loss: 3.525752067565918\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "683: loss: 3.535304307937622\n",
            "684: loss: 3.425553560256958\n",
            "685: loss: 3.4296858310699463\n",
            "686: loss: 3.35929536819458\n",
            "687: loss: 3.4907288551330566\n",
            "688: loss: 3.355231523513794\n",
            "689: loss: 3.471773386001587\n",
            "690: loss: 3.426642656326294\n",
            "691: loss: 3.5056211948394775\n",
            "692: loss: 3.4680564403533936\n",
            "693: loss: 3.4050190448760986\n",
            "694: loss: 3.385986328125\n",
            "695: loss: 3.345454216003418\n",
            "696: loss: 3.503256320953369\n",
            "697: loss: 3.3354620933532715\n",
            "698: loss: 3.4181196689605713\n",
            "699: loss: 3.4055893421173096\n",
            "700: loss: 3.391427516937256\n",
            "700: valid loss 3.380185842514038\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "701: loss: 3.416290760040283\n",
            "702: loss: 3.560906410217285\n",
            "703: loss: 3.4715518951416016\n",
            "704: loss: 3.4182212352752686\n",
            "705: loss: 3.3421263694763184\n",
            "706: loss: 3.3713650703430176\n",
            "707: loss: 3.3692057132720947\n",
            "708: loss: 3.421858072280884\n",
            "709: loss: 3.346461057662964\n",
            "710: loss: 3.4056923389434814\n",
            "711: loss: 3.328408718109131\n",
            "712: loss: 3.5829508304595947\n",
            "713: loss: 3.35548996925354\n",
            "714: loss: 3.459766149520874\n",
            "715: loss: 3.5861434936523438\n",
            "716: loss: 3.485955238342285\n",
            "717: loss: 3.5360729694366455\n",
            "718: loss: 3.428278684616089\n",
            "719: loss: 3.3152027130126953\n",
            "720: loss: 3.5284135341644287\n",
            "720: valid loss 3.570251703262329\n",
            "721: loss: 3.3287148475646973\n",
            "722: loss: 3.3826699256896973\n",
            "723: loss: 3.524214506149292\n",
            "724: loss: 3.5340874195098877\n",
            "725: loss: 3.417217254638672\n",
            "726: loss: 3.544278383255005\n",
            "727: loss: 3.547065258026123\n",
            "728: loss: 3.4445250034332275\n",
            "729: loss: 3.4174556732177734\n",
            "730: loss: 3.532604455947876\n",
            "731: loss: 3.4153196811676025\n",
            "732: loss: 3.464810848236084\n",
            "733: loss: 3.5781612396240234\n",
            "734: loss: 3.25321626663208\n",
            "735: loss: 3.3199462890625\n",
            "736: loss: 3.316784620285034\n",
            "737: loss: 3.4950923919677734\n",
            "738: loss: 3.2835628986358643\n",
            "739: loss: 3.4123783111572266\n",
            "740: loss: 3.41227126121521\n",
            "740: valid loss 3.466376781463623\n",
            "741: loss: 3.4027915000915527\n",
            "742: loss: 3.3981473445892334\n",
            "743: loss: 3.445441961288452\n",
            "744: loss: 3.385741949081421\n",
            "745: loss: 3.453540563583374\n",
            "746: loss: 3.485154867172241\n",
            "747: loss: 3.479299306869507\n",
            "748: loss: 3.3248050212860107\n",
            "749: loss: 3.3432137966156006\n",
            "750: loss: 3.363532066345215\n",
            "751: loss: 3.324392795562744\n",
            "752: loss: 3.4127564430236816\n",
            "753: loss: 3.443573474884033\n",
            "754: loss: 3.4073550701141357\n",
            "755: loss: 3.457101345062256\n",
            "756: loss: 3.4636738300323486\n",
            "757: loss: 3.4270591735839844\n",
            "758: loss: 3.3760039806365967\n",
            "759: loss: 3.283587694168091\n",
            "760: loss: 3.436938524246216\n",
            "760: valid loss 3.3935060501098633\n",
            "761: loss: 3.5605721473693848\n",
            "762: loss: 3.3546762466430664\n",
            "763: loss: 3.3767354488372803\n",
            "764: loss: 3.377880334854126\n",
            "765: loss: 3.3634591102600098\n",
            "766: loss: 3.3369247913360596\n",
            "767: loss: 3.49295711517334\n",
            "768: loss: 3.355022430419922\n",
            "769: loss: 3.3241207599639893\n",
            "770: loss: 3.3233752250671387\n",
            "771: loss: 3.432615280151367\n",
            "772: loss: 3.237870454788208\n",
            "773: loss: 3.5024375915527344\n",
            "774: loss: 3.51613712310791\n",
            "775: loss: 3.4181530475616455\n",
            "776: loss: 3.484290599822998\n",
            "777: loss: 3.4484806060791016\n",
            "778: loss: 3.4908180236816406\n",
            "779: loss: 3.438401699066162\n",
            "780: loss: 3.3223636150360107\n",
            "780: valid loss 3.5080347061157227\n",
            "781: loss: 3.5289056301116943\n",
            "782: loss: 3.487368106842041\n",
            "783: loss: 3.2395403385162354\n",
            "784: loss: 3.4120373725891113\n",
            "785: loss: 3.5114331245422363\n",
            "786: loss: 3.588637590408325\n",
            "787: loss: 3.358065128326416\n",
            "788: loss: 3.5075299739837646\n",
            "789: loss: 3.4030346870422363\n",
            "790: loss: 3.477958917617798\n",
            "791: loss: 3.3707942962646484\n",
            "792: loss: 3.492820978164673\n",
            "793: loss: 3.2942163944244385\n",
            "794: loss: 3.5372257232666016\n",
            "795: loss: 3.482630968093872\n",
            "796: loss: 3.4147868156433105\n",
            "797: loss: 3.4406180381774902\n",
            "798: loss: 3.466783285140991\n",
            "799: loss: 3.3477606773376465\n",
            "800: loss: 3.234174966812134\n",
            "800: valid loss 3.438162088394165\n",
            "800: saving model to /content/models/results_semantic\n",
            "801: loss: 3.4578990936279297\n",
            "802: loss: 3.498213768005371\n",
            "803: loss: 3.3244879245758057\n",
            "804: loss: 3.529385566711426\n",
            "805: loss: 3.4505984783172607\n",
            "806: loss: 3.348842144012451\n",
            "807: loss: 3.343282699584961\n",
            "808: loss: 3.377154588699341\n",
            "809: loss: 3.316295862197876\n",
            "810: loss: 3.4488351345062256\n",
            "811: loss: 3.418790578842163\n",
            "812: loss: 3.388049602508545\n",
            "813: loss: 3.2368109226226807\n",
            "814: loss: 3.37307071685791\n",
            "815: loss: 3.477391004562378\n",
            "816: loss: 3.3995819091796875\n",
            "817: loss: 3.401157855987549\n",
            "818: loss: 3.4595963954925537\n",
            "819: loss: 3.462996482849121\n",
            "820: loss: 3.412710189819336\n",
            "820: valid loss 3.373309850692749\n",
            "821: loss: 3.379099130630493\n",
            "822: loss: 3.41890549659729\n",
            "823: loss: 3.5383453369140625\n",
            "824: loss: 3.2953760623931885\n",
            "825: loss: 3.2848658561706543\n",
            "826: loss: 3.439589738845825\n",
            "827: loss: 3.478717565536499\n",
            "828: loss: 3.3140127658843994\n",
            "829: loss: 3.353943347930908\n",
            "830: loss: 3.427011728286743\n",
            "831: loss: 3.4130403995513916\n",
            "832: loss: 3.4974331855773926\n",
            "833: loss: 3.3866002559661865\n",
            "834: loss: 3.4858388900756836\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "835: loss: 3.200439929962158\n",
            "836: loss: 3.5126516819000244\n",
            "837: loss: 3.3292269706726074\n",
            "838: loss: 3.405998468399048\n",
            "839: loss: 3.3766977787017822\n",
            "840: loss: 3.3268299102783203\n",
            "840: valid loss 3.4363973140716553\n",
            "841: loss: 3.390505313873291\n",
            "842: loss: 3.3906731605529785\n",
            "843: loss: 3.532432794570923\n",
            "844: loss: 3.3148069381713867\n",
            "845: loss: 3.4635868072509766\n",
            "846: loss: 3.3569772243499756\n",
            "847: loss: 3.5365140438079834\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "848: loss: 3.4513120651245117\n",
            "849: loss: 3.4038655757904053\n",
            "850: loss: 3.4306280612945557\n",
            "851: loss: 3.4900002479553223\n",
            "852: loss: 3.291954517364502\n",
            "853: loss: 3.4210686683654785\n",
            "854: loss: 3.335428476333618\n",
            "855: loss: 3.3188836574554443\n",
            "856: loss: 3.4576590061187744\n",
            "857: loss: 3.362668514251709\n",
            "858: loss: 3.5182223320007324\n",
            "859: loss: 3.4086754322052\n",
            "860: loss: 3.3388848304748535\n",
            "860: valid loss 3.386904001235962\n",
            "861: loss: 3.3789114952087402\n",
            "862: loss: 3.445474147796631\n",
            "863: loss: 3.545903205871582\n",
            "864: loss: 3.415806770324707\n",
            "865: loss: 3.4011576175689697\n",
            "866: loss: 3.4161875247955322\n",
            "867: loss: 3.3558826446533203\n",
            "868: loss: 3.4477250576019287\n",
            "869: loss: 3.2962958812713623\n",
            "870: loss: 3.312476873397827\n",
            "871: loss: 3.458921432495117\n",
            "872: loss: 3.520996570587158\n",
            "873: loss: 3.3708016872406006\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "874: loss: 3.479844093322754\n",
            "875: loss: 3.362009286880493\n",
            "876: loss: 3.3865442276000977\n",
            "877: loss: 3.392350673675537\n",
            "878: loss: 3.426039218902588\n",
            "879: loss: 3.219165563583374\n",
            "880: loss: 3.400123357772827\n",
            "880: valid loss 3.4970908164978027\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "881: loss: 3.418607711791992\n",
            "882: loss: 3.445131301879883\n",
            "883: loss: 3.4367475509643555\n",
            "884: loss: 3.43947696685791\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "885: loss: 3.5110275745391846\n",
            "886: loss: 3.389652967453003\n",
            "887: loss: 3.4001965522766113\n",
            "888: loss: 3.255833864212036\n",
            "889: loss: 3.329010486602783\n",
            "890: loss: 3.528047561645508\n",
            "891: loss: 3.4888718128204346\n",
            "892: loss: 3.364039659500122\n",
            "893: loss: 3.3496196269989014\n",
            "894: loss: 3.4886109828948975\n",
            "895: loss: 3.3315353393554688\n",
            "896: loss: 3.560847759246826\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "897: loss: 3.2624948024749756\n",
            "898: loss: 3.2957746982574463\n",
            "899: loss: 3.352339506149292\n",
            "900: loss: 3.3088796138763428\n",
            "900: valid loss 3.406433582305908\n",
            "901: loss: 3.4296514987945557\n",
            "902: loss: 3.449796676635742\n",
            "903: loss: 3.376486301422119\n",
            "904: loss: 3.4190475940704346\n",
            "905: loss: 3.419208288192749\n",
            "906: loss: 3.3279848098754883\n",
            "907: loss: 3.455220937728882\n",
            "908: loss: 3.2699575424194336\n",
            "909: loss: 3.2789998054504395\n",
            "910: loss: 3.4827795028686523\n",
            "911: loss: 3.514631509780884\n",
            "912: loss: 3.4139065742492676\n",
            "913: loss: 3.511535882949829\n",
            "914: loss: 3.4718520641326904\n",
            "915: loss: 3.4187586307525635\n",
            "916: loss: 3.220318555831909\n",
            "917: loss: 3.32491135597229\n",
            "918: loss: 3.498218536376953\n",
            "919: loss: 3.487060546875\n",
            "920: loss: 3.3505771160125732\n",
            "920: valid loss 3.4415342807769775\n",
            "921: loss: 3.4225714206695557\n",
            "922: loss: 3.44087553024292\n",
            "923: loss: 3.4051058292388916\n",
            "924: loss: 3.3123202323913574\n",
            "925: loss: 3.4603137969970703\n",
            "926: loss: 3.4152238368988037\n",
            "927: loss: 3.372466564178467\n",
            "928: loss: 3.3693957328796387\n",
            "929: loss: 3.492046594619751\n",
            "930: loss: 3.4785921573638916\n",
            "931: loss: 3.2095677852630615\n",
            "932: loss: 3.3349151611328125\n",
            "933: loss: 3.52670955657959\n",
            "934: loss: 3.4841344356536865\n",
            "935: loss: 3.383124589920044\n",
            "936: loss: 3.3507378101348877\n",
            "937: loss: 3.285567283630371\n",
            "938: loss: 3.1912808418273926\n",
            "939: loss: 3.4796454906463623\n",
            "940: loss: 3.4820637702941895\n",
            "940: valid loss 3.3090128898620605\n",
            "941: loss: 3.3079020977020264\n",
            "942: loss: 3.3693642616271973\n",
            "943: loss: 3.4924662113189697\n",
            "944: loss: 3.346372127532959\n",
            "945: loss: 3.3042662143707275\n",
            "946: loss: 3.3459389209747314\n",
            "947: loss: 3.545550584793091\n",
            "948: loss: 3.29730224609375\n",
            "949: loss: 3.387608051300049\n",
            "950: loss: 3.5425562858581543\n",
            "951: loss: 3.3423051834106445\n",
            "952: loss: 3.346832752227783\n",
            "953: loss: 3.3952383995056152\n",
            "954: loss: 3.415963888168335\n",
            "955: loss: 3.3132574558258057\n",
            "956: loss: 3.311053991317749\n",
            "957: loss: 3.374568223953247\n",
            "958: loss: 3.3989474773406982\n",
            "959: loss: 3.3957014083862305\n",
            "960: loss: 3.3280465602874756\n",
            "960: valid loss 3.6360673904418945\n",
            "961: loss: 3.226233959197998\n",
            "962: loss: 3.244922637939453\n",
            "963: loss: 3.4396002292633057\n",
            "964: loss: 3.407628059387207\n",
            "965: loss: 3.407621145248413\n",
            "966: loss: 3.4761886596679688\n",
            "967: loss: 3.3464512825012207\n",
            "968: loss: 3.372022867202759\n",
            "969: loss: 3.4077160358428955\n",
            "970: loss: 3.243628978729248\n",
            "971: loss: 3.4017391204833984\n",
            "972: loss: 3.4851973056793213\n",
            "973: loss: 3.3893961906433105\n",
            "974: loss: 3.3076095581054688\n",
            "975: loss: 3.1976757049560547\n",
            "976: loss: 3.3458075523376465\n",
            "977: loss: 3.1793293952941895\n",
            "978: loss: 3.31063175201416\n",
            "979: loss: 3.3685200214385986\n",
            "980: loss: 3.2953379154205322\n",
            "980: valid loss 3.448955535888672\n",
            "981: loss: 3.1218936443328857\n",
            "982: loss: 3.260889768600464\n",
            "983: loss: 3.29636549949646\n",
            "984: loss: 3.27782940864563\n",
            "985: loss: 3.401829481124878\n",
            "986: loss: 3.287677049636841\n",
            "987: loss: 3.336176872253418\n",
            "988: loss: 3.181398630142212\n",
            "989: loss: 3.2917885780334473\n",
            "990: loss: 3.144335985183716\n",
            "991: loss: 3.227327585220337\n",
            "992: loss: 3.3164126873016357\n",
            "993: loss: 3.3144583702087402\n",
            "994: loss: 3.2180674076080322\n",
            "995: loss: 3.1505191326141357\n",
            "996: loss: 3.283031702041626\n",
            "997: loss: 3.2383925914764404\n",
            "998: loss: 3.352142095565796\n",
            "999: loss: 3.193187713623047\n",
            "1000: loss: 3.345964193344116\n",
            "1000: valid loss 3.354349374771118\n",
            "1001: loss: 3.3033013343811035\n",
            "1002: loss: 3.1572906970977783\n",
            "1003: loss: 3.180738687515259\n",
            "1004: loss: 3.1783833503723145\n",
            "1005: loss: 3.249274730682373\n",
            "1006: loss: 3.2455649375915527\n",
            "1007: loss: 3.3661837577819824\n",
            "1008: loss: 3.169299364089966\n",
            "1009: loss: 3.3225059509277344\n",
            "1010: loss: 3.2483930587768555\n",
            "1011: loss: 3.263843297958374\n",
            "1012: loss: 3.1305322647094727\n",
            "1013: loss: 3.498645067214966\n",
            "1014: loss: 3.386422872543335\n",
            "1015: loss: 3.362844705581665\n",
            "1016: loss: 3.186091184616089\n",
            "1017: loss: 3.2396233081817627\n",
            "1018: loss: 3.072995185852051\n",
            "1019: loss: 3.16323184967041\n",
            "1020: loss: 3.2146596908569336\n",
            "1020: valid loss 3.4029757976531982\n",
            "1021: loss: 3.274181365966797\n",
            "1022: loss: 3.3924150466918945\n",
            "1023: loss: 3.2434210777282715\n",
            "1024: loss: 3.3938333988189697\n",
            "1025: loss: 3.1534013748168945\n",
            "1026: loss: 3.2739436626434326\n",
            "1027: loss: 3.2394845485687256\n",
            "1028: loss: 3.351501941680908\n",
            "1029: loss: 3.3097798824310303\n",
            "1030: loss: 3.3549020290374756\n",
            "1031: loss: 3.191909074783325\n",
            "1032: loss: 3.1747443675994873\n",
            "1033: loss: 3.310969114303589\n",
            "1034: loss: 3.3540220260620117\n",
            "1035: loss: 3.1342852115631104\n",
            "1036: loss: 3.204118251800537\n",
            "1037: loss: 3.185429334640503\n",
            "1038: loss: 3.0153450965881348\n",
            "1039: loss: 3.1885387897491455\n",
            "1040: loss: 3.2932136058807373\n",
            "1040: valid loss 3.394437789916992\n",
            "1041: loss: 3.180281162261963\n",
            "1042: loss: 3.355381727218628\n",
            "1043: loss: 3.1798250675201416\n",
            "1044: loss: 3.167743444442749\n",
            "1045: loss: 3.3413679599761963\n",
            "1046: loss: 3.1427276134490967\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "1047: loss: 3.219831705093384\n",
            "1048: loss: 3.359661102294922\n",
            "1049: loss: 3.227048397064209\n",
            "1050: loss: 3.26735782623291\n",
            "1051: loss: 3.3623805046081543\n",
            "1052: loss: 3.256396532058716\n",
            "1053: loss: 3.245309352874756\n",
            "1054: loss: 3.145179271697998\n",
            "1055: loss: 2.935062885284424\n",
            "1056: loss: 3.1922435760498047\n",
            "1057: loss: 3.082563638687134\n",
            "1058: loss: 3.3998255729675293\n",
            "1059: loss: 3.141324520111084\n",
            "1060: loss: 3.3454983234405518\n",
            "1060: valid loss 3.3600168228149414\n",
            "1061: loss: 3.2607994079589844\n",
            "1062: loss: 3.3285269737243652\n",
            "1063: loss: 3.339191436767578\n",
            "1064: loss: 3.386725664138794\n",
            "1065: loss: 3.3200771808624268\n",
            "1066: loss: 3.2791378498077393\n",
            "1067: loss: 3.2749979496002197\n",
            "1068: loss: 3.3559927940368652\n",
            "1069: loss: 3.214684009552002\n",
            "1070: loss: 3.344905376434326\n",
            "1071: loss: 3.300454616546631\n",
            "1072: loss: 3.3416433334350586\n",
            "1073: loss: 3.34108567237854\n",
            "1074: loss: 3.246716022491455\n",
            "1075: loss: 3.214547872543335\n",
            "1076: loss: 3.389145612716675\n",
            "1077: loss: 3.271716356277466\n",
            "1078: loss: 3.352724075317383\n",
            "1079: loss: 3.2681612968444824\n",
            "1080: loss: 3.297266960144043\n",
            "1080: valid loss 3.555396795272827\n",
            "1081: loss: 3.283770799636841\n",
            "1082: loss: 3.510969400405884\n",
            "1083: loss: 3.259937047958374\n",
            "1084: loss: 3.2735984325408936\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "1085: loss: 3.279891014099121\n",
            "1086: loss: 3.0706913471221924\n",
            "1087: loss: 3.1475350856781006\n",
            "1088: loss: 3.4166693687438965\n",
            "1089: loss: 3.358090877532959\n",
            "1090: loss: 3.1744489669799805\n",
            "1091: loss: 3.2956666946411133\n",
            "1092: loss: 3.2346510887145996\n",
            "1093: loss: 3.236733913421631\n",
            "1094: loss: 3.21921968460083\n",
            "1095: loss: 3.313843250274658\n",
            "1096: loss: 3.03008770942688\n",
            "1097: loss: 3.183563709259033\n",
            "1098: loss: 3.366960048675537\n",
            "1099: loss: 3.136518955230713\n",
            "1100: loss: 3.1105430126190186\n",
            "1100: valid loss 3.446629285812378\n",
            "1101: loss: 3.0932085514068604\n",
            "1102: loss: 3.258612871170044\n",
            "1103: loss: 3.2131593227386475\n",
            "1104: loss: 3.308772563934326\n",
            "1105: loss: 3.195892095565796\n",
            "1106: loss: 3.125176191329956\n",
            "1107: loss: 3.220454454421997\n",
            "1108: loss: 3.31451416015625\n",
            "1109: loss: 3.3054208755493164\n",
            "1110: loss: 3.429535388946533\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "1111: loss: 3.2018775939941406\n",
            "1112: loss: 3.409097909927368\n",
            "1113: loss: 3.2346713542938232\n",
            "1114: loss: 3.2665793895721436\n",
            "1115: loss: 3.2539196014404297\n",
            "1116: loss: 3.1916799545288086\n",
            "1117: loss: 3.3643274307250977\n",
            "1118: loss: 3.2752280235290527\n",
            "1119: loss: 3.349395513534546\n",
            "1120: loss: 3.279200553894043\n",
            "1120: valid loss 3.4396090507507324\n",
            "1121: loss: 3.211198568344116\n",
            "1122: loss: 3.1183018684387207\n",
            "1123: loss: 3.447707414627075\n",
            "1124: loss: 3.3676059246063232\n",
            "1125: loss: 3.32694935798645\n",
            "| WARNING: ran out of memory, retrying batch\n",
            "1126: loss: 3.3425817489624023\n",
            "1127: loss: 3.149742364883423\n",
            "1128: loss: 3.2499616146087646\n",
            "1129: loss: 3.447782278060913\n",
            "1130: loss: 3.353330373764038\n",
            "1131: loss: 3.2741026878356934\n",
            "1132: loss: 3.1458897590637207\n",
            "1133: loss: 3.164644479751587\n",
            "1134: loss: 3.4000558853149414\n",
            "1135: loss: 3.2365269660949707\n",
            "1136: loss: 3.246826648712158\n",
            "1137: loss: 3.2520151138305664\n",
            "1138: loss: 3.406654119491577\n",
            "1139: loss: 3.347817897796631\n",
            "1140: loss: 3.254291534423828\n",
            "1140: valid loss 3.285515308380127\n",
            "1141: loss: 3.2022016048431396\n",
            "1142: loss: 3.3985228538513184\n",
            "1143: loss: 3.214254856109619\n",
            "1144: loss: 3.2667124271392822\n",
            "1145: loss: 3.1976640224456787\n",
            "1146: loss: 3.079416275024414\n",
            "1147: loss: 3.369370937347412\n",
            "1148: loss: 3.1988861560821533\n",
            "1149: loss: 3.386622190475464\n",
            "1150: loss: 3.1970274448394775\n",
            "1151: loss: 3.3453128337860107\n",
            "1152: loss: 3.214111566543579\n",
            "1153: loss: 3.1912970542907715\n",
            "1154: loss: 3.009913682937622\n",
            "1155: loss: 3.195093870162964\n",
            "1156: loss: 3.277101755142212\n",
            "1157: loss: 3.2272231578826904\n",
            "1158: loss: 3.4273979663848877\n",
            "1159: loss: 3.2074642181396484\n",
            "1160: loss: 3.148336887359619\n",
            "1160: valid loss 3.422443151473999\n",
            "1161: loss: 3.326871871948242\n",
            "1162: loss: 3.0916543006896973\n",
            "1163: loss: 3.3188202381134033\n",
            "1164: loss: 3.3404641151428223\n",
            "1165: loss: 3.290036916732788\n",
            "1166: loss: 3.243893623352051\n",
            "1167: loss: 3.5196409225463867\n",
            "1168: loss: 3.4261422157287598\n",
            "1169: loss: 3.3156609535217285\n",
            "1170: loss: 3.224900245666504\n",
            "1171: loss: 3.467212438583374\n",
            "1172: loss: 3.1847567558288574\n",
            "1173: loss: 3.282794952392578\n",
            "1174: loss: 3.2602741718292236\n",
            "1175: loss: 3.2698161602020264\n",
            "1176: loss: 3.276047945022583\n",
            "1177: loss: 3.2622549533843994\n",
            "1178: loss: 3.2403597831726074\n",
            "1179: loss: 3.2399098873138428\n",
            "1180: loss: 3.309602737426758\n",
            "1180: valid loss 3.509925365447998\n",
            "1181: loss: 3.2808985710144043\n",
            "1182: loss: 3.228332281112671\n",
            "1183: loss: 3.4675066471099854\n",
            "1184: loss: 3.1249101161956787\n",
            "1185: loss: 3.4243903160095215\n",
            "1186: loss: 3.317448139190674\n",
            "1187: loss: 3.2800676822662354\n",
            "1188: loss: 3.335752010345459\n",
            "1189: loss: 3.4436910152435303\n",
            "1190: loss: 3.352538585662842\n",
            "1191: loss: 3.2751624584198\n",
            "1192: loss: 3.224337100982666\n",
            "1193: loss: 3.2153537273406982\n",
            "1194: loss: 3.3572044372558594\n",
            "1195: loss: 3.1678073406219482\n",
            "1196: loss: 3.4050400257110596\n",
            "1197: loss: 3.375617504119873\n",
            "1198: loss: 3.3733818531036377\n",
            "1199: loss: 3.3524765968322754\n",
            "1200: loss: 3.2638285160064697\n",
            "1200: valid loss 3.4251232147216797\n",
            "1200: saving model to /content/models/results_semantic\n",
            "1201: loss: 3.375016927719116\n",
            "1202: loss: 3.4218640327453613\n",
            "1203: loss: 3.194897413253784\n",
            "1204: loss: 3.227163553237915\n",
            "1205: loss: 3.1448419094085693\n",
            "1206: loss: 3.1967077255249023\n",
            "1207: loss: 3.405941963195801\n",
            "1208: loss: 3.4107275009155273\n",
            "1209: loss: 3.325582981109619\n",
            "1210: loss: 3.2326862812042236\n",
            "1211: loss: 3.314706563949585\n",
            "1212: loss: 3.448868751525879\n",
            "1213: loss: 3.387662410736084\n",
            "1214: loss: 2.967597484588623\n",
            "1215: loss: 3.189337968826294\n",
            "1216: loss: 3.237717628479004\n",
            "1217: loss: 3.2825443744659424\n",
            "1218: loss: 3.2572765350341797\n",
            "1219: loss: 3.2897307872772217\n",
            "1220: loss: 3.3242714405059814\n",
            "1220: valid loss 3.3623507022857666\n",
            "1221: loss: 3.3152823448181152\n",
            "1222: loss: 3.377887725830078\n",
            "1223: loss: 3.198321580886841\n",
            "1224: loss: 3.4417643547058105\n",
            "1225: loss: 3.4083006381988525\n",
            "1226: loss: 3.359964370727539\n",
            "1227: loss: 3.2766523361206055\n",
            "1228: loss: 3.3085713386535645\n",
            "1229: loss: 3.355480432510376\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "semantic_transformer = SemanticTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    dim = config[\"embedding_dim\"],\n",
        "    depth = config[\"depth\"],\n",
        "    heads = config[\"heads\"],\n",
        "    audio_text_condition = True,\n",
        "    attn_dropout = config[\"attn_dropout\"],\n",
        "    ff_dropout = config[\"ff_dropout\"],\n",
        ").cuda()\n",
        "\n",
        "\n",
        "semantic_trainer = SemanticTransformerTrainer(\n",
        "    transformer = semantic_transformer,\n",
        "    wav2vec = wav2vec,\n",
        "    audio_conditioner = quantizer,\n",
        "    folder = dataset_folder,\n",
        "    lr=config[\"lr\"],\n",
        "    batch_size = config[\"batch_size\"],\n",
        "    data_max_length_seconds = config.get(\"data_max_length_seconds\"),\n",
        "    data_max_length = config.get(\"data_max_length\"),\n",
        "    save_results_every = VAL_EVERY_STEPS,\n",
        "    wd = config[\"wd\"],\n",
        "    grad_accum_every = config[\"grad_accum_every\"],\n",
        "    max_grad_norm = config[\"max_grad_norm\"],\n",
        "    save_model_every = SAVE_MODEL_EVERY,\n",
        "    num_train_steps = STEPS,\n",
        "    results_folder = RESULTS_ROOT_PATH+'/results_semantic',\n",
        "    force_clear_prev_results = False,\n",
        "    accelerate_kwargs = get_accelerate_kwargs(config)\n",
        ")\n",
        "\n",
        "scheduler = get_scheduler(semantic_trainer.optim, config)\n",
        "print(scheduler)\n",
        "if load_ckpt:\n",
        "  semantic_trainer.load(semantic_load_path)\n",
        "\n",
        "if train:\n",
        "  custom_train(semantic_trainer,\n",
        "               config,\n",
        "               scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eEvIzhEWwRz"
      },
      "source": [
        "### CoarseTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bx3OGKzFECEa"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  if train:\n",
        "    free_gpu_memory(coarse_transformer)\n",
        "    free_gpu_memory(semantic_transformer)\n",
        "    del semantic_transformer, semantic_trainer\n",
        "except:\n",
        "  pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c4940d9988d548b4b3efd04ade6618aa",
            "76e3632ad2e34456b0dc43f4b8a734b9",
            "da34fff74e3c42969270c70cb38e61e8",
            "0df94dc33d3d49169ed2bccd6cafc5ab",
            "4c8402148c3647de900311e97758e7d2",
            "54bffd551b6f4944b880b76c32fd3564",
            "e7fb474363924f5dabd9dcc3e0f24a89",
            "7f8e31cb14334bb0a58727e131d08dcd"
          ]
        },
        "id": "1LeWmaNHzzY9",
        "outputId": "84e6012e-4b46-48c7-c2f9-90c80323e7b1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training with dataset of 3950 samples and validating with randomly splitted 208 samples\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:q4wfdpnj) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4940d9988d548b4b3efd04ade6618aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███████▇▇</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>6.95266</td></tr><tr><td>lr</td><td>0.00074</td></tr><tr><td>train_loss</td><td>6.95266</td></tr><tr><td>valid_loss</td><td>7.09663</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dry-cloud-34</strong> at: <a href='https://wandb.ai/ols/coarse/runs/q4wfdpnj' target=\"_blank\">https://wandb.ai/ols/coarse/runs/q4wfdpnj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230501_221952-q4wfdpnj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:q4wfdpnj). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230501_231243-uoevyad9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ols/coarse/runs/uoevyad9' target=\"_blank\">rural-voice-36</a></strong> to <a href='https://wandb.ai/ols/coarse' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ols/coarse' target=\"_blank\">https://wandb.ai/ols/coarse</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ols/coarse/runs/uoevyad9' target=\"_blank\">https://wandb.ai/ols/coarse/runs/uoevyad9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "2026: loss: 7.025412082672119\n",
            "2027: loss: 7.1286797523498535\n",
            "2028: loss: 7.358229160308838\n",
            "2029: loss: 7.08974552154541\n",
            "2030: loss: 7.167626857757568\n",
            "2031: loss: 7.006067752838135\n",
            "2032: loss: 7.081326007843018\n",
            "2033: loss: 6.652551651000977\n",
            "2034: loss: 7.348058223724365\n",
            "2035: loss: 7.508452415466309\n",
            "2036: loss: 6.920841693878174\n",
            "2037: loss: 7.191387176513672\n",
            "2038: loss: 7.472121238708496\n",
            "2039: loss: 7.1390204429626465\n",
            "2040: loss: 7.3639068603515625\n",
            "2040: valid loss 7.348334789276123\n",
            "2041: loss: 7.124777317047119\n",
            "2042: loss: 6.7101545333862305\n",
            "2043: loss: 7.239994049072266\n",
            "2044: loss: 7.173505783081055\n",
            "2045: loss: 6.652553558349609\n",
            "2046: loss: 7.300714492797852\n",
            "2047: loss: 7.594173908233643\n",
            "2048: loss: 7.37505578994751\n",
            "2049: loss: 6.970643043518066\n",
            "2050: loss: 7.154393196105957\n",
            "2051: loss: 7.383277893066406\n",
            "2052: loss: 6.772135257720947\n",
            "2053: loss: 6.982758045196533\n",
            "2054: loss: 7.70366096496582\n",
            "2055: loss: 7.111751556396484\n",
            "2056: loss: 7.428465366363525\n",
            "2057: loss: 6.898808479309082\n",
            "2058: loss: 7.132274150848389\n",
            "2059: loss: 7.37148380279541\n",
            "2060: loss: 7.224778175354004\n",
            "2060: valid loss 7.311691761016846\n",
            "2061: loss: 7.108475685119629\n",
            "2062: loss: 7.0953593254089355\n",
            "2063: loss: 6.758395195007324\n",
            "2064: loss: 6.294065952301025\n",
            "2065: loss: 7.994129657745361\n",
            "2066: loss: 7.492636203765869\n",
            "2067: loss: 7.780452728271484\n",
            "2068: loss: 7.540788173675537\n",
            "2069: loss: 7.1025471687316895\n",
            "2070: loss: 7.519072532653809\n",
            "2071: loss: 7.530398845672607\n",
            "2072: loss: 6.896386623382568\n",
            "2073: loss: 7.440889835357666\n",
            "2074: loss: 7.102850437164307\n",
            "2075: loss: 7.061439037322998\n",
            "2076: loss: 7.168560981750488\n",
            "2077: loss: 7.0962748527526855\n",
            "2078: loss: 6.975256443023682\n",
            "2079: loss: 7.207590579986572\n",
            "2080: loss: 7.372282981872559\n",
            "2080: valid loss 6.821249485015869\n",
            "2081: loss: 7.087821006774902\n",
            "2082: loss: 6.9660115242004395\n",
            "2083: loss: 6.828850269317627\n",
            "2084: loss: 7.255117416381836\n",
            "2085: loss: 6.738993167877197\n",
            "2086: loss: 6.877936363220215\n",
            "2087: loss: 7.680142402648926\n",
            "2088: loss: 7.782862663269043\n",
            "2089: loss: 7.334716796875\n",
            "2090: loss: 7.154821395874023\n",
            "2091: loss: 7.513007164001465\n",
            "2092: loss: 7.065500259399414\n",
            "2093: loss: 6.766441345214844\n",
            "2094: loss: 7.196395397186279\n",
            "2095: loss: 3.97446608543396\n",
            "2096: loss: 7.399960994720459\n",
            "2097: loss: 7.835813522338867\n",
            "2098: loss: 7.567265510559082\n",
            "2099: loss: 7.030722141265869\n",
            "2100: loss: 7.4484710693359375\n",
            "2100: valid loss 6.974526405334473\n",
            "2101: loss: 7.041237831115723\n",
            "2102: loss: 7.178620338439941\n",
            "2103: loss: 7.082614421844482\n",
            "2104: loss: 6.907695770263672\n",
            "2105: loss: 6.958643436431885\n",
            "2106: loss: 7.049636363983154\n",
            "2107: loss: 6.411746025085449\n",
            "2108: loss: 6.922027111053467\n",
            "2109: loss: 6.705537796020508\n",
            "2110: loss: 7.191381931304932\n",
            "2111: loss: 7.149688720703125\n",
            "2112: loss: 7.011128902435303\n",
            "2113: loss: 6.621213436126709\n",
            "2114: loss: 7.35984468460083\n",
            "2115: loss: 7.444127559661865\n",
            "2116: loss: 6.837518215179443\n",
            "2117: loss: 7.279808044433594\n",
            "2118: loss: 7.282009124755859\n",
            "2119: loss: 7.136720657348633\n",
            "2120: loss: 6.764147758483887\n",
            "2120: valid loss 7.093630790710449\n",
            "2121: loss: 7.31440544128418\n",
            "2122: loss: 7.253065586090088\n",
            "2123: loss: 7.15787935256958\n",
            "2124: loss: 7.0922417640686035\n",
            "2125: loss: 7.207815170288086\n",
            "2126: loss: 6.902529239654541\n",
            "2127: loss: 7.603346347808838\n",
            "2128: loss: 7.026572227478027\n",
            "2129: loss: 7.02303409576416\n",
            "2130: loss: 7.01964807510376\n",
            "2131: loss: 7.100845813751221\n",
            "2132: loss: 6.9390482902526855\n",
            "2133: loss: 7.136584758758545\n",
            "2134: loss: 7.0795369148254395\n",
            "2135: loss: 6.903080463409424\n",
            "2136: loss: 7.179446220397949\n",
            "2137: loss: 7.262770175933838\n",
            "2138: loss: 7.558584213256836\n",
            "2139: loss: 7.038297176361084\n",
            "2140: loss: 7.176670074462891\n",
            "2140: valid loss 7.123584747314453\n",
            "2141: loss: 6.836413860321045\n",
            "2142: loss: 7.022984027862549\n",
            "2143: loss: 7.357141017913818\n",
            "2144: loss: 7.1590166091918945\n",
            "2145: loss: 6.962752342224121\n",
            "2146: loss: 6.951068878173828\n",
            "2147: loss: 7.00730562210083\n",
            "2148: loss: 7.315966606140137\n",
            "2149: loss: 7.581587314605713\n",
            "2150: loss: 7.07383394241333\n",
            "2151: loss: 6.946834087371826\n",
            "2152: loss: 7.063892364501953\n",
            "2153: loss: 7.212856292724609\n",
            "2154: loss: 7.074767589569092\n",
            "2155: loss: 7.345226764678955\n",
            "2156: loss: 7.081552982330322\n",
            "2157: loss: 6.9788818359375\n",
            "2158: loss: 7.111069679260254\n",
            "2159: loss: 6.968264102935791\n",
            "2160: loss: 6.88848876953125\n",
            "2160: valid loss 7.265787601470947\n",
            "2161: loss: 6.898892879486084\n",
            "2162: loss: 7.149194240570068\n",
            "2163: loss: 7.1337361335754395\n",
            "2164: loss: 7.666824817657471\n",
            "2165: loss: 6.982254505157471\n",
            "2166: loss: 7.372979164123535\n",
            "2167: loss: 6.856997966766357\n",
            "2168: loss: 7.050665855407715\n",
            "2169: loss: 6.9410247802734375\n",
            "2170: loss: 6.836826324462891\n",
            "2171: loss: 7.094679355621338\n",
            "2172: loss: 7.0622735023498535\n",
            "2173: loss: 7.241646766662598\n",
            "2174: loss: 7.138678073883057\n",
            "2175: loss: 7.23314905166626\n",
            "2176: loss: 7.220293998718262\n",
            "2177: loss: 6.890290260314941\n",
            "2178: loss: 7.240083694458008\n",
            "2179: loss: 7.029165744781494\n",
            "2180: loss: 6.8895955085754395\n",
            "2180: valid loss 6.807297229766846\n",
            "2181: loss: 6.9304633140563965\n",
            "2182: loss: 7.169386386871338\n",
            "2183: loss: 7.36694860458374\n",
            "2184: loss: 6.900900363922119\n",
            "2185: loss: 6.918991565704346\n",
            "2186: loss: 6.894188404083252\n",
            "2187: loss: 7.251168727874756\n",
            "2188: loss: 7.159205913543701\n",
            "2189: loss: 7.138217926025391\n",
            "2190: loss: 7.171324729919434\n",
            "2191: loss: 7.312413215637207\n",
            "2192: loss: 6.965840816497803\n",
            "2193: loss: 6.847311496734619\n",
            "2194: loss: 7.222813606262207\n",
            "2195: loss: 7.233316898345947\n",
            "2196: loss: 7.032175064086914\n",
            "2197: loss: 6.826335906982422\n",
            "2198: loss: 7.269734859466553\n",
            "2199: loss: 6.916130542755127\n",
            "2200: loss: 6.967062473297119\n",
            "2200: valid loss 6.6883697509765625\n",
            "2200: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "2201: loss: 6.964622497558594\n",
            "2202: loss: 7.24680757522583\n",
            "2203: loss: 7.569454193115234\n",
            "2204: loss: 7.005590915679932\n",
            "2205: loss: 6.778114318847656\n",
            "2206: loss: 7.242748260498047\n",
            "2207: loss: 7.358397960662842\n",
            "2208: loss: 7.546750545501709\n",
            "2209: loss: 6.922011852264404\n",
            "2210: loss: 6.744714736938477\n",
            "2211: loss: 7.0475358963012695\n",
            "2212: loss: 7.081416606903076\n",
            "2213: loss: 7.003890514373779\n",
            "2214: loss: 6.898484230041504\n",
            "2215: loss: 7.289401054382324\n",
            "2216: loss: 6.941073417663574\n",
            "2217: loss: 7.263992786407471\n",
            "2218: loss: 7.039546012878418\n",
            "2219: loss: 7.026708126068115\n",
            "2220: loss: 7.084997177124023\n",
            "2220: valid loss 6.617550373077393\n",
            "2221: loss: 6.85237979888916\n",
            "2222: loss: 7.07037353515625\n",
            "2223: loss: 7.0573248863220215\n",
            "2224: loss: 6.8417181968688965\n",
            "2225: loss: 7.169980049133301\n",
            "2226: loss: 6.920629978179932\n",
            "2227: loss: 7.212594032287598\n",
            "2228: loss: 7.095689296722412\n",
            "2229: loss: 7.094297885894775\n",
            "2230: loss: 7.031002521514893\n",
            "2231: loss: 7.08621072769165\n",
            "2232: loss: 7.280797481536865\n",
            "2233: loss: 6.99355936050415\n",
            "2234: loss: 6.805760860443115\n",
            "2235: loss: 7.156459331512451\n",
            "2236: loss: 7.038300037384033\n",
            "2237: loss: 6.937510967254639\n",
            "2238: loss: 7.201209545135498\n",
            "2239: loss: 7.144834041595459\n",
            "2240: loss: 7.020275115966797\n",
            "2240: valid loss 7.294677734375\n",
            "2241: loss: 7.38670015335083\n",
            "2242: loss: 7.314918518066406\n",
            "2243: loss: 6.7813310623168945\n",
            "2244: loss: 7.382991790771484\n",
            "2245: loss: 6.911185264587402\n",
            "2246: loss: 6.949476718902588\n",
            "2247: loss: 7.0152082443237305\n",
            "2248: loss: 6.92067289352417\n",
            "2249: loss: 7.07700777053833\n",
            "2250: loss: 6.592740535736084\n",
            "2251: loss: 7.030385494232178\n",
            "2252: loss: 6.697685241699219\n",
            "2253: loss: 6.816143989562988\n",
            "2254: loss: 7.156857013702393\n",
            "2255: loss: 6.920232772827148\n",
            "2256: loss: 7.5327277183532715\n",
            "2257: loss: 7.039144039154053\n",
            "2258: loss: 7.3100714683532715\n",
            "2259: loss: 6.9142303466796875\n",
            "2260: loss: 6.993987560272217\n",
            "2260: valid loss 7.034704208374023\n",
            "2261: loss: 6.94370174407959\n",
            "2262: loss: 6.69105863571167\n",
            "2263: loss: 6.849020481109619\n",
            "2264: loss: 7.44056510925293\n",
            "2265: loss: 7.36611795425415\n",
            "2266: loss: 7.102200984954834\n",
            "2267: loss: 7.398592472076416\n",
            "2268: loss: 7.308776378631592\n",
            "2269: loss: 7.312142848968506\n",
            "2270: loss: 7.365723133087158\n",
            "2271: loss: 7.1111321449279785\n",
            "2272: loss: 7.0269365310668945\n",
            "2273: loss: 7.2318925857543945\n",
            "2274: loss: 7.018157958984375\n",
            "2275: loss: 6.922304630279541\n",
            "2276: loss: 7.1723761558532715\n",
            "2277: loss: 6.747549057006836\n",
            "2278: loss: 7.388251304626465\n",
            "2279: loss: 7.135768413543701\n",
            "2280: loss: 6.850984573364258\n",
            "2280: valid loss 7.082092761993408\n",
            "2281: loss: 7.065869331359863\n",
            "2282: loss: 6.799942493438721\n",
            "2283: loss: 6.742924690246582\n",
            "2284: loss: 7.424604892730713\n",
            "2285: loss: 7.0608344078063965\n",
            "2286: loss: 6.763740062713623\n",
            "2287: loss: 7.093264579772949\n",
            "2288: loss: 7.600922584533691\n",
            "2289: loss: 6.965950012207031\n",
            "2290: loss: 7.755337238311768\n",
            "2291: loss: 6.8433122634887695\n",
            "2292: loss: 7.1164093017578125\n",
            "2293: loss: 7.047553062438965\n",
            "2294: loss: 6.8489909172058105\n",
            "2295: loss: 7.217602252960205\n",
            "2296: loss: 6.888662815093994\n",
            "2297: loss: 6.946532726287842\n",
            "2298: loss: 6.8633904457092285\n",
            "2299: loss: 7.056973934173584\n",
            "2300: loss: 7.12820291519165\n",
            "2300: valid loss 7.187480449676514\n",
            "2301: loss: 7.002068519592285\n",
            "2302: loss: 7.108938217163086\n",
            "2303: loss: 6.990664958953857\n",
            "2304: loss: 7.050310134887695\n",
            "2305: loss: 6.675144195556641\n",
            "2306: loss: 7.001229763031006\n",
            "2307: loss: 6.879167556762695\n",
            "2308: loss: 7.137660980224609\n",
            "2309: loss: 8.252846717834473\n",
            "2310: loss: 7.148697853088379\n",
            "2311: loss: 7.224569797515869\n",
            "2312: loss: 6.685315132141113\n",
            "2313: loss: 6.921659469604492\n",
            "2314: loss: 7.104507923126221\n",
            "2315: loss: 6.535022735595703\n",
            "2316: loss: 7.2040510177612305\n",
            "2317: loss: 7.364995002746582\n",
            "2318: loss: 7.181621074676514\n",
            "2319: loss: 6.833822727203369\n",
            "2320: loss: 7.235400676727295\n",
            "2320: valid loss 7.039181709289551\n",
            "2321: loss: 6.966418743133545\n",
            "2322: loss: 7.059954643249512\n",
            "2323: loss: 6.770473003387451\n",
            "2324: loss: 7.0502238273620605\n",
            "2325: loss: 6.9966559410095215\n",
            "2326: loss: 6.999669075012207\n",
            "2327: loss: 7.234225749969482\n",
            "2328: loss: 7.2141642570495605\n",
            "2329: loss: 7.071231842041016\n",
            "2330: loss: 6.904607772827148\n",
            "2331: loss: 7.2917094230651855\n",
            "2332: loss: 6.93792724609375\n",
            "2333: loss: 7.026794910430908\n",
            "2334: loss: 7.42248010635376\n",
            "2335: loss: 7.231227874755859\n",
            "2336: loss: 7.303012371063232\n",
            "2337: loss: 9.155760765075684\n",
            "2338: loss: 7.077500820159912\n",
            "2339: loss: 6.948503494262695\n",
            "2340: loss: 7.239660739898682\n",
            "2340: valid loss 7.372993469238281\n",
            "2341: loss: 7.138047218322754\n",
            "2342: loss: 6.945704936981201\n",
            "2343: loss: 6.992547512054443\n",
            "2344: loss: 7.054201602935791\n",
            "2345: loss: 7.247043609619141\n",
            "2346: loss: 6.843968868255615\n",
            "2347: loss: 6.905960559844971\n",
            "2348: loss: 7.040565490722656\n",
            "2349: loss: 6.756783962249756\n",
            "2350: loss: 6.766353607177734\n",
            "2351: loss: 6.712676048278809\n",
            "2352: loss: 6.785073757171631\n",
            "2353: loss: 7.02044153213501\n",
            "2354: loss: 7.0554680824279785\n",
            "2355: loss: 7.287607669830322\n",
            "2356: loss: 6.863532066345215\n",
            "2357: loss: 7.072545528411865\n",
            "2358: loss: 6.988379001617432\n",
            "2359: loss: 7.309330940246582\n",
            "2360: loss: 7.232372283935547\n",
            "2360: valid loss 6.987933158874512\n",
            "2361: loss: 6.972440242767334\n",
            "2362: loss: 6.97562837600708\n",
            "2363: loss: 7.082805156707764\n",
            "2364: loss: 6.89784049987793\n",
            "2365: loss: 6.996001720428467\n",
            "2366: loss: 6.737060546875\n",
            "2367: loss: 6.738081455230713\n",
            "2368: loss: 6.771383762359619\n",
            "2369: loss: 6.735476493835449\n",
            "2370: loss: 6.929708957672119\n",
            "2371: loss: 6.84644889831543\n",
            "2372: loss: 7.694482803344727\n",
            "2373: loss: 6.767269611358643\n",
            "2374: loss: 7.254132270812988\n",
            "2375: loss: 7.269401550292969\n",
            "2376: loss: 7.158926963806152\n",
            "2377: loss: 7.237680912017822\n",
            "2378: loss: 6.941055774688721\n",
            "2379: loss: 7.10521936416626\n",
            "2380: loss: 6.745277404785156\n",
            "2380: valid loss 6.780930519104004\n",
            "2381: loss: 7.144776344299316\n",
            "2382: loss: 7.188952445983887\n",
            "2383: loss: 7.015864372253418\n",
            "2384: loss: 6.9402899742126465\n",
            "2385: loss: 6.95468282699585\n",
            "2386: loss: 6.916321754455566\n",
            "2387: loss: 7.205611705780029\n",
            "2388: loss: 6.933890342712402\n",
            "2389: loss: 6.904491901397705\n",
            "2390: loss: 6.967296600341797\n",
            "2391: loss: 7.164523124694824\n",
            "2392: loss: 6.998283863067627\n",
            "2393: loss: 6.99114465713501\n",
            "2394: loss: 7.050501346588135\n",
            "2395: loss: 6.963220596313477\n",
            "2396: loss: 7.164538860321045\n",
            "2397: loss: 7.086553573608398\n",
            "2398: loss: 7.029845714569092\n",
            "2399: loss: 6.914628505706787\n",
            "2400: loss: 7.214034557342529\n",
            "2400: valid loss 7.0452728271484375\n",
            "2400: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "2401: loss: 6.948464870452881\n",
            "2402: loss: 6.939268589019775\n",
            "2403: loss: 6.839206695556641\n",
            "2404: loss: 6.886332988739014\n",
            "2405: loss: 6.9146409034729\n",
            "2406: loss: 6.957949638366699\n",
            "2407: loss: 6.821324348449707\n",
            "2408: loss: 7.211206436157227\n",
            "2409: loss: 7.242955207824707\n",
            "2410: loss: 7.402758598327637\n",
            "2411: loss: 7.130364894866943\n",
            "2412: loss: 6.672887325286865\n",
            "2413: loss: 6.880339622497559\n",
            "2414: loss: 7.057541370391846\n",
            "2415: loss: 7.1023478507995605\n",
            "2416: loss: 7.418525218963623\n",
            "2417: loss: 7.034071922302246\n",
            "2418: loss: 6.885934352874756\n",
            "2419: loss: 6.805465221405029\n",
            "2420: loss: 7.230427265167236\n",
            "2420: valid loss 7.047396183013916\n",
            "2421: loss: 7.048587799072266\n",
            "2422: loss: 6.704745292663574\n",
            "2423: loss: 6.827961444854736\n",
            "2424: loss: 7.133023262023926\n",
            "2425: loss: 7.070274829864502\n",
            "2426: loss: 7.141726970672607\n",
            "2427: loss: 6.772152900695801\n",
            "2428: loss: 6.790166854858398\n",
            "2429: loss: 6.9057230949401855\n",
            "2430: loss: 6.672386169433594\n",
            "2431: loss: 7.204287528991699\n",
            "2432: loss: 7.162225723266602\n",
            "2433: loss: 7.07321834564209\n",
            "2434: loss: 6.794375419616699\n",
            "2435: loss: 7.105923175811768\n",
            "2436: loss: 6.879293918609619\n",
            "2437: loss: 6.6860575675964355\n",
            "2438: loss: 7.051354885101318\n",
            "2439: loss: 7.031140327453613\n",
            "2440: loss: 6.910533428192139\n",
            "2440: valid loss 7.052234172821045\n",
            "2441: loss: 6.586766242980957\n",
            "2442: loss: 6.875814914703369\n",
            "2443: loss: 6.948535919189453\n",
            "2444: loss: 7.357438087463379\n",
            "2445: loss: 6.835066795349121\n",
            "2446: loss: 6.906698226928711\n",
            "2447: loss: 6.587714195251465\n",
            "2448: loss: 7.369306564331055\n",
            "2449: loss: 7.087777137756348\n",
            "2450: loss: 6.9329023361206055\n",
            "2451: loss: 7.060311317443848\n",
            "2452: loss: 6.959497928619385\n",
            "2453: loss: 6.953644752502441\n",
            "2454: loss: 6.974130630493164\n",
            "2455: loss: 6.988899230957031\n",
            "2456: loss: 6.991329669952393\n",
            "2457: loss: 6.975152969360352\n",
            "2458: loss: 7.256718635559082\n",
            "2459: loss: 6.924065113067627\n",
            "2460: loss: 6.827670097351074\n",
            "2460: valid loss 7.031882286071777\n",
            "2461: loss: 7.090674877166748\n",
            "2462: loss: 6.6859540939331055\n",
            "2463: loss: 7.06473970413208\n",
            "2464: loss: 7.133209228515625\n",
            "2465: loss: 7.066090106964111\n",
            "2466: loss: 6.82537317276001\n",
            "2467: loss: 6.672388553619385\n",
            "2468: loss: 6.9853692054748535\n",
            "2469: loss: 6.865819931030273\n",
            "2470: loss: 7.14626407623291\n",
            "2471: loss: 7.051730632781982\n",
            "2472: loss: 7.140480995178223\n",
            "2473: loss: 6.8805999755859375\n",
            "2474: loss: 6.793762683868408\n",
            "2475: loss: 6.913801193237305\n",
            "2476: loss: 6.671104907989502\n",
            "2477: loss: 7.031493663787842\n",
            "2478: loss: 7.084091663360596\n",
            "2479: loss: 7.503532409667969\n",
            "2480: loss: 6.988796710968018\n",
            "2480: valid loss 7.057011604309082\n",
            "2481: loss: 6.979177951812744\n",
            "2482: loss: 6.996798992156982\n",
            "2483: loss: 6.8404059410095215\n",
            "2484: loss: 7.000715255737305\n",
            "2485: loss: 6.625484466552734\n",
            "2486: loss: 6.912053108215332\n",
            "2487: loss: 7.090816020965576\n",
            "2488: loss: 6.584508419036865\n",
            "2489: loss: 6.529642581939697\n",
            "2490: loss: 6.971107482910156\n",
            "2491: loss: 6.9384942054748535\n",
            "2492: loss: 7.129716396331787\n",
            "2493: loss: 7.076630592346191\n",
            "2494: loss: 7.0503411293029785\n",
            "2495: loss: 7.273439407348633\n",
            "2496: loss: 6.919649124145508\n",
            "2497: loss: 6.891122341156006\n",
            "2498: loss: 7.0569748878479\n",
            "2499: loss: 7.067985534667969\n",
            "2500: loss: 6.7636566162109375\n",
            "2500: valid loss 7.236802577972412\n",
            "2501: loss: 6.869477272033691\n",
            "2502: loss: 7.340432167053223\n",
            "2503: loss: 7.131923675537109\n",
            "2504: loss: 7.096870422363281\n",
            "2505: loss: 6.792673110961914\n",
            "2506: loss: 7.024672508239746\n",
            "2507: loss: 7.325874328613281\n",
            "2508: loss: 7.056972503662109\n",
            "2509: loss: 6.899702548980713\n",
            "2510: loss: 6.819666862487793\n",
            "2511: loss: 6.922482013702393\n",
            "2512: loss: 6.77718448638916\n",
            "2513: loss: 6.993544578552246\n",
            "2514: loss: 6.733081817626953\n",
            "2515: loss: 6.837332248687744\n",
            "2516: loss: 6.94675350189209\n",
            "2517: loss: 6.983013153076172\n",
            "2518: loss: 7.189743995666504\n",
            "2519: loss: 6.870096206665039\n",
            "2520: loss: 6.815337181091309\n",
            "2520: valid loss 7.08710241317749\n",
            "2521: loss: 7.286163330078125\n",
            "2522: loss: 7.068023204803467\n",
            "2523: loss: 6.834901809692383\n",
            "2524: loss: 7.041613578796387\n",
            "2525: loss: 7.524304389953613\n",
            "2526: loss: 6.9205098152160645\n",
            "2527: loss: 7.120680809020996\n",
            "2528: loss: 7.088611602783203\n",
            "2529: loss: 6.813675880432129\n",
            "2530: loss: 7.070786476135254\n",
            "2531: loss: 6.998875141143799\n",
            "2532: loss: 6.735366344451904\n",
            "2533: loss: 6.819637298583984\n",
            "2534: loss: 7.144931316375732\n",
            "2535: loss: 7.202223777770996\n",
            "2536: loss: 7.172317028045654\n",
            "2537: loss: 7.0307841300964355\n",
            "2538: loss: 6.795467853546143\n",
            "2539: loss: 7.045797824859619\n",
            "2540: loss: 6.7408833503723145\n",
            "2540: valid loss 6.986758232116699\n",
            "2541: loss: 6.931453227996826\n",
            "2542: loss: 7.0272297859191895\n",
            "2543: loss: 6.781006813049316\n",
            "2544: loss: 7.3472771644592285\n",
            "2545: loss: 6.96917200088501\n",
            "2546: loss: 6.640347957611084\n",
            "2547: loss: 6.871678352355957\n",
            "2548: loss: 6.8183393478393555\n",
            "2549: loss: 7.008354663848877\n",
            "2550: loss: 7.339561462402344\n",
            "2551: loss: 6.806958198547363\n",
            "2552: loss: 6.818183422088623\n",
            "2553: loss: 7.156994819641113\n",
            "2554: loss: 6.996872425079346\n",
            "2555: loss: 7.0977301597595215\n",
            "2556: loss: 6.700038909912109\n",
            "2557: loss: 6.803010940551758\n",
            "2558: loss: 6.724550724029541\n",
            "2559: loss: 7.2666120529174805\n",
            "2560: loss: 6.765550136566162\n",
            "2560: valid loss 6.561344146728516\n",
            "2561: loss: 6.7805376052856445\n",
            "2562: loss: 6.888058662414551\n",
            "2563: loss: 6.671352863311768\n",
            "2564: loss: 6.949940204620361\n",
            "2565: loss: 7.159974575042725\n",
            "2566: loss: 7.203410625457764\n",
            "2567: loss: 7.344513893127441\n",
            "2568: loss: 7.262853145599365\n",
            "2569: loss: 6.86020040512085\n",
            "2570: loss: 6.9310479164123535\n",
            "2571: loss: 6.723709583282471\n",
            "2572: loss: 7.127429962158203\n",
            "2573: loss: 6.972460746765137\n",
            "2574: loss: 6.8752923011779785\n",
            "2575: loss: 6.868281364440918\n",
            "2576: loss: 6.927258014678955\n",
            "2577: loss: 7.011241912841797\n",
            "2578: loss: 6.436482906341553\n",
            "2579: loss: 6.681540012359619\n",
            "2580: loss: 7.086252212524414\n",
            "2580: valid loss 7.249577045440674\n",
            "2581: loss: 7.03744649887085\n",
            "2582: loss: 6.971977710723877\n",
            "2583: loss: 7.442301273345947\n",
            "2584: loss: 7.008981227874756\n",
            "2585: loss: 7.081397533416748\n",
            "2586: loss: 6.499694347381592\n",
            "2587: loss: 7.0781941413879395\n",
            "2588: loss: 6.709628105163574\n",
            "2589: loss: 6.66369104385376\n",
            "2590: loss: 7.204782009124756\n",
            "2591: loss: 6.811614036560059\n",
            "2592: loss: 7.160857677459717\n",
            "2593: loss: 6.452157497406006\n",
            "2594: loss: 6.821038722991943\n",
            "2595: loss: 7.106810569763184\n",
            "2596: loss: 6.6967573165893555\n",
            "2597: loss: 7.8743181228637695\n",
            "2598: loss: 6.90025520324707\n",
            "2599: loss: 7.105181694030762\n",
            "2600: loss: 6.858736991882324\n",
            "2600: valid loss 6.869789123535156\n",
            "2600: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "2601: loss: 6.870296001434326\n",
            "2602: loss: 7.320939064025879\n",
            "2603: loss: 7.021517276763916\n",
            "2604: loss: 6.972092151641846\n",
            "2605: loss: 6.894279956817627\n",
            "2606: loss: 7.012667655944824\n",
            "2607: loss: 6.873435974121094\n",
            "2608: loss: 6.917265892028809\n",
            "2609: loss: 6.711819171905518\n",
            "2610: loss: 6.782453536987305\n",
            "2611: loss: 7.072368144989014\n",
            "2612: loss: 6.499035358428955\n",
            "2613: loss: 7.072205543518066\n",
            "2614: loss: 6.731897354125977\n",
            "2615: loss: 7.033459186553955\n",
            "2616: loss: 6.832122802734375\n",
            "2617: loss: 6.823583126068115\n",
            "2618: loss: 6.757843971252441\n",
            "2619: loss: 7.005830764770508\n",
            "2620: loss: 6.859342575073242\n",
            "2620: valid loss 7.064319610595703\n",
            "2621: loss: 6.748175144195557\n",
            "2622: loss: 7.128222942352295\n",
            "2623: loss: 7.274262428283691\n",
            "2624: loss: 6.847263813018799\n",
            "2625: loss: 7.162442684173584\n",
            "2626: loss: 6.89805793762207\n",
            "2627: loss: 6.991592884063721\n",
            "2628: loss: 6.887851238250732\n",
            "2629: loss: 7.184398174285889\n",
            "2630: loss: 6.876169681549072\n",
            "2631: loss: 6.738587856292725\n",
            "2632: loss: 7.012106895446777\n",
            "2633: loss: 6.891624450683594\n",
            "2634: loss: 7.212523937225342\n",
            "2635: loss: 7.019310474395752\n",
            "2636: loss: 7.003421306610107\n",
            "2637: loss: 7.043527126312256\n",
            "2638: loss: 6.854452610015869\n",
            "2639: loss: 6.757220268249512\n",
            "2640: loss: 6.647330284118652\n",
            "2640: valid loss 6.498993873596191\n",
            "2641: loss: 7.107994079589844\n",
            "2642: loss: 7.027878761291504\n",
            "2643: loss: 6.987062931060791\n",
            "2644: loss: 7.120429039001465\n",
            "2645: loss: 6.697756767272949\n",
            "2646: loss: 7.127047538757324\n",
            "2647: loss: 7.068050384521484\n",
            "2648: loss: 6.9339070320129395\n",
            "2649: loss: 7.164795875549316\n",
            "2650: loss: 6.822207927703857\n",
            "2651: loss: 6.975042343139648\n",
            "2652: loss: 7.028480052947998\n",
            "2653: loss: 6.950002193450928\n",
            "2654: loss: 6.939297676086426\n",
            "2655: loss: 7.016896724700928\n",
            "2656: loss: 7.287168502807617\n",
            "2657: loss: 7.248918533325195\n",
            "2658: loss: 7.247009754180908\n",
            "2659: loss: 6.848354816436768\n",
            "2660: loss: 7.047799587249756\n",
            "2660: valid loss 7.085761547088623\n",
            "2661: loss: 6.731576442718506\n",
            "2662: loss: 6.848513126373291\n",
            "2663: loss: 7.101198673248291\n",
            "2664: loss: 6.909578323364258\n",
            "2665: loss: 6.818087100982666\n",
            "2666: loss: 6.825613021850586\n",
            "2667: loss: 6.874502658843994\n",
            "2668: loss: 7.0860419273376465\n",
            "2669: loss: 6.748814105987549\n",
            "2670: loss: 6.95751428604126\n",
            "2671: loss: 6.9662275314331055\n",
            "2672: loss: 7.1932525634765625\n",
            "2673: loss: 6.848342418670654\n",
            "2674: loss: 6.918463706970215\n",
            "2675: loss: 7.059310436248779\n",
            "2676: loss: 7.359415531158447\n",
            "2677: loss: 6.751922130584717\n",
            "2678: loss: 6.748388767242432\n",
            "2679: loss: 7.044688701629639\n",
            "2680: loss: 6.817085266113281\n",
            "2680: valid loss 6.869256019592285\n",
            "2681: loss: 7.443654537200928\n",
            "2682: loss: 7.041095733642578\n",
            "2683: loss: 6.84115743637085\n",
            "2684: loss: 7.235454559326172\n",
            "2685: loss: 6.856295108795166\n",
            "2686: loss: 7.0060882568359375\n",
            "2687: loss: 7.003859996795654\n",
            "2688: loss: 2.773890972137451\n",
            "2689: loss: 7.086930751800537\n",
            "2690: loss: 7.297595500946045\n",
            "2691: loss: 6.746581077575684\n",
            "2692: loss: 6.850030899047852\n",
            "2693: loss: 6.594274520874023\n",
            "2694: loss: 7.038726329803467\n",
            "2695: loss: 7.057548522949219\n",
            "2696: loss: 7.037869453430176\n",
            "2697: loss: 6.987825393676758\n",
            "2698: loss: 6.641294002532959\n",
            "2699: loss: 6.7883219718933105\n",
            "2700: loss: 7.037195682525635\n",
            "2700: valid loss 6.752519130706787\n",
            "2701: loss: 6.845569610595703\n",
            "2702: loss: 7.247659206390381\n",
            "2703: loss: 6.624677658081055\n",
            "2704: loss: 7.018138408660889\n",
            "2705: loss: 6.658364772796631\n",
            "2706: loss: 7.016729354858398\n",
            "2707: loss: 7.090936660766602\n",
            "2708: loss: 7.096484184265137\n",
            "2709: loss: 1.2636433839797974\n",
            "2710: loss: 6.960966110229492\n",
            "2711: loss: 6.822205543518066\n",
            "2712: loss: 7.128000736236572\n",
            "2713: loss: 7.451784133911133\n",
            "2714: loss: 6.861395359039307\n",
            "2715: loss: 7.12542200088501\n",
            "2716: loss: 6.90336799621582\n",
            "2717: loss: 6.926335334777832\n",
            "2718: loss: 6.837434768676758\n",
            "2719: loss: 6.692930698394775\n",
            "2720: loss: 7.039828300476074\n",
            "2720: valid loss 6.666141986846924\n",
            "2721: loss: 7.019975185394287\n",
            "2722: loss: 6.791430473327637\n",
            "2723: loss: 6.760213851928711\n",
            "2724: loss: 6.877539157867432\n",
            "2725: loss: 6.957015037536621\n",
            "2726: loss: 6.87933874130249\n",
            "2727: loss: 6.593506813049316\n",
            "2728: loss: 6.758965015411377\n",
            "2729: loss: 6.517423152923584\n",
            "2730: loss: 6.961449146270752\n",
            "2731: loss: 7.157474994659424\n",
            "2732: loss: 7.110514163970947\n",
            "2733: loss: 6.600991725921631\n",
            "2734: loss: 6.984131813049316\n",
            "2735: loss: 6.517746448516846\n",
            "2736: loss: 6.908076763153076\n",
            "2737: loss: 7.004483222961426\n",
            "2738: loss: 6.462556838989258\n",
            "2739: loss: 6.681920051574707\n",
            "2740: loss: 7.014462471008301\n",
            "2740: valid loss 7.2512311935424805\n",
            "2741: loss: 7.328913688659668\n",
            "2742: loss: 7.243995666503906\n",
            "2743: loss: 6.936971664428711\n",
            "2744: loss: 6.529050350189209\n",
            "2745: loss: 10.501053810119629\n",
            "2746: loss: 6.822071552276611\n",
            "2747: loss: 6.9284844398498535\n",
            "2748: loss: 6.43182373046875\n",
            "2749: loss: 6.972160339355469\n",
            "2750: loss: 7.3735432624816895\n",
            "2751: loss: 7.0227274894714355\n",
            "2752: loss: 6.971376419067383\n",
            "2753: loss: 7.04677152633667\n",
            "2754: loss: 6.817526340484619\n",
            "2755: loss: 6.826084136962891\n",
            "2756: loss: 7.268743991851807\n",
            "2757: loss: 7.065143585205078\n",
            "2758: loss: 7.090936660766602\n",
            "2759: loss: 6.873674392700195\n",
            "2760: loss: 8.752707481384277\n",
            "2760: valid loss 6.95290470123291\n",
            "2761: loss: 7.487173080444336\n",
            "2762: loss: 7.5561065673828125\n",
            "2763: loss: 7.076192855834961\n",
            "2764: loss: 6.991739749908447\n",
            "2765: loss: 6.8753662109375\n",
            "2766: loss: 7.362861633300781\n",
            "2767: loss: 6.91885232925415\n",
            "2768: loss: 6.943073272705078\n",
            "2769: loss: 6.708901882171631\n",
            "2770: loss: 6.831840515136719\n",
            "2771: loss: 6.886561870574951\n",
            "2772: loss: 7.16335391998291\n",
            "2773: loss: 7.250992774963379\n",
            "2774: loss: 6.69896936416626\n",
            "2775: loss: 6.8770012855529785\n",
            "2776: loss: 7.088592052459717\n",
            "2777: loss: 6.885780334472656\n",
            "2778: loss: 7.009252071380615\n",
            "2779: loss: 6.992446422576904\n",
            "2780: loss: 7.232483386993408\n",
            "2780: valid loss 6.693073749542236\n",
            "2781: loss: 6.926036834716797\n",
            "2782: loss: 6.841622352600098\n",
            "2783: loss: 7.294309139251709\n",
            "2784: loss: 6.629593372344971\n",
            "2785: loss: 7.03069543838501\n",
            "2786: loss: 6.656527996063232\n",
            "2787: loss: 6.979698657989502\n",
            "2788: loss: 7.02748966217041\n",
            "2789: loss: 7.00616979598999\n",
            "2790: loss: 6.578161716461182\n",
            "2791: loss: 6.926109790802002\n",
            "2792: loss: 6.490224838256836\n",
            "2793: loss: 6.893853664398193\n",
            "2794: loss: 6.888839244842529\n",
            "2795: loss: 7.029313087463379\n",
            "2796: loss: 6.810636520385742\n",
            "2797: loss: 7.058663845062256\n",
            "2798: loss: 7.312356948852539\n",
            "2799: loss: 7.153965473175049\n",
            "2800: loss: 6.795280456542969\n",
            "2800: valid loss 6.599360942840576\n",
            "2800: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "2801: loss: 7.096446990966797\n",
            "2802: loss: 7.040726661682129\n",
            "2803: loss: 7.1501898765563965\n",
            "2804: loss: 7.044054985046387\n",
            "2805: loss: 7.384392738342285\n",
            "2806: loss: 6.6088032722473145\n",
            "2807: loss: 6.707919120788574\n",
            "2808: loss: 6.752457141876221\n",
            "2809: loss: 6.959979057312012\n",
            "2810: loss: 7.054012775421143\n",
            "2811: loss: 6.589094638824463\n",
            "2812: loss: 6.740118980407715\n",
            "2813: loss: 7.160963535308838\n",
            "2814: loss: 6.6253981590271\n",
            "2815: loss: 7.071498394012451\n",
            "2816: loss: 7.11090087890625\n",
            "2817: loss: 6.976641654968262\n",
            "2818: loss: 6.891210556030273\n",
            "2819: loss: 6.9255170822143555\n",
            "2820: loss: 7.2136688232421875\n",
            "2820: valid loss 7.15209436416626\n",
            "2821: loss: 7.091914176940918\n",
            "2822: loss: 7.087228298187256\n",
            "2823: loss: 6.947009086608887\n",
            "2824: loss: 6.904349327087402\n",
            "2825: loss: 6.9518890380859375\n",
            "2826: loss: 7.354482650756836\n",
            "2827: loss: 6.775688648223877\n",
            "2828: loss: 6.94413423538208\n",
            "2829: loss: 6.740131855010986\n",
            "2830: loss: 7.152889728546143\n",
            "2831: loss: 6.855427265167236\n",
            "2832: loss: 6.972897529602051\n",
            "2833: loss: 6.9021172523498535\n",
            "2834: loss: 7.031888961791992\n",
            "2835: loss: 6.896450996398926\n",
            "2836: loss: 6.697307586669922\n",
            "2837: loss: 6.805716514587402\n",
            "2838: loss: 7.356667518615723\n",
            "2839: loss: 6.770452499389648\n",
            "2840: loss: 7.166608810424805\n",
            "2840: valid loss 6.7918500900268555\n",
            "2841: loss: 6.640695571899414\n",
            "2842: loss: 6.727832794189453\n",
            "2843: loss: 6.68623161315918\n",
            "2844: loss: 6.831364154815674\n",
            "2845: loss: 6.994196891784668\n",
            "2846: loss: 7.0666351318359375\n",
            "2847: loss: 6.886794090270996\n",
            "2848: loss: 7.250375270843506\n",
            "2849: loss: 7.037571907043457\n",
            "2850: loss: 6.675256729125977\n",
            "2851: loss: 7.085036754608154\n",
            "2852: loss: 6.904323101043701\n",
            "2853: loss: 7.264261245727539\n",
            "2854: loss: 7.498049259185791\n",
            "2855: loss: 6.7664794921875\n",
            "2856: loss: 7.004929542541504\n",
            "2857: loss: 6.8731794357299805\n",
            "2858: loss: 6.991494655609131\n",
            "2859: loss: 7.017510414123535\n",
            "2860: loss: 6.616746425628662\n",
            "2860: valid loss 6.786979675292969\n",
            "2861: loss: 6.977878570556641\n",
            "2862: loss: 6.912968158721924\n",
            "2863: loss: 6.795661926269531\n",
            "2864: loss: 6.927013874053955\n",
            "2865: loss: 6.804453372955322\n",
            "2866: loss: 6.918488502502441\n",
            "2867: loss: 6.928067207336426\n",
            "2868: loss: 6.900384426116943\n",
            "2869: loss: 6.916830062866211\n",
            "2870: loss: 6.770543098449707\n",
            "2871: loss: 7.30808162689209\n",
            "2872: loss: 7.0395588874816895\n",
            "2873: loss: 6.552990436553955\n",
            "2874: loss: 6.529596328735352\n",
            "2875: loss: 6.87684965133667\n",
            "2876: loss: 6.756309509277344\n",
            "2877: loss: 6.317908763885498\n",
            "2878: loss: 6.607009410858154\n",
            "2879: loss: 6.852575778961182\n",
            "2880: loss: 7.158679962158203\n",
            "2880: valid loss 7.3795576095581055\n",
            "2881: loss: 7.187268257141113\n",
            "2882: loss: 7.225285530090332\n",
            "2883: loss: 6.850387096405029\n",
            "2884: loss: 7.357997417449951\n",
            "2885: loss: 6.865934371948242\n",
            "2886: loss: 7.282472610473633\n",
            "2887: loss: 6.999283313751221\n",
            "2888: loss: 7.000654697418213\n",
            "2889: loss: 7.060648441314697\n",
            "2890: loss: 6.698439121246338\n",
            "2891: loss: 7.022377967834473\n",
            "2892: loss: 7.044818878173828\n",
            "2893: loss: 6.955883979797363\n",
            "2894: loss: 7.130411624908447\n",
            "2895: loss: 6.972401142120361\n",
            "2896: loss: 6.9469170570373535\n",
            "2897: loss: 6.866326332092285\n",
            "2898: loss: 6.642683029174805\n",
            "2899: loss: 6.695858955383301\n",
            "2900: loss: 7.200157165527344\n",
            "2900: valid loss 6.661928653717041\n",
            "2901: loss: 7.019862174987793\n",
            "2902: loss: 6.959787845611572\n",
            "2903: loss: 6.613378524780273\n",
            "2904: loss: 6.682891368865967\n",
            "2905: loss: 7.023899555206299\n",
            "2906: loss: 6.792317867279053\n",
            "2907: loss: 7.056459426879883\n",
            "2908: loss: 6.876255512237549\n",
            "2909: loss: 6.688661098480225\n",
            "2910: loss: 7.069823265075684\n",
            "2911: loss: 6.749319553375244\n",
            "2912: loss: 7.088654518127441\n",
            "2913: loss: 6.892277717590332\n",
            "2914: loss: 6.804958343505859\n",
            "2915: loss: 6.8928422927856445\n",
            "2916: loss: 7.275346279144287\n",
            "2917: loss: 6.716665267944336\n",
            "2918: loss: 7.159138202667236\n",
            "2919: loss: 7.006715774536133\n",
            "2920: loss: 7.117577075958252\n",
            "2920: valid loss 6.936316967010498\n",
            "2921: loss: 6.9431843757629395\n",
            "2922: loss: 6.71657133102417\n",
            "2923: loss: 6.957889080047607\n",
            "2924: loss: 6.828390598297119\n",
            "2925: loss: 6.799316883087158\n",
            "2926: loss: 6.993093967437744\n",
            "2927: loss: 6.843609809875488\n",
            "2928: loss: 6.809860706329346\n",
            "2929: loss: 6.829566478729248\n",
            "2930: loss: 6.7445220947265625\n",
            "2931: loss: 6.665474891662598\n",
            "2932: loss: 6.620014190673828\n",
            "2933: loss: 6.702428340911865\n",
            "2934: loss: 6.94944953918457\n",
            "2935: loss: 7.09276008605957\n",
            "2936: loss: 6.813929557800293\n",
            "2937: loss: 6.961146831512451\n",
            "2938: loss: 7.2078118324279785\n",
            "2939: loss: 7.141632080078125\n",
            "2940: loss: 6.777102947235107\n",
            "2940: valid loss 7.255836486816406\n",
            "2941: loss: 7.546684265136719\n",
            "2942: loss: 7.123483657836914\n",
            "2943: loss: 7.330743312835693\n",
            "2944: loss: 6.83188533782959\n",
            "2945: loss: 6.90056848526001\n",
            "2946: loss: 6.76331090927124\n",
            "2947: loss: 6.839375019073486\n",
            "2948: loss: 6.834801197052002\n",
            "2949: loss: 6.72303581237793\n",
            "2950: loss: 7.119942665100098\n",
            "2951: loss: 7.398383617401123\n",
            "2952: loss: 7.266769886016846\n",
            "2953: loss: 6.916342258453369\n",
            "2954: loss: 7.154863357543945\n",
            "2955: loss: 7.035091876983643\n",
            "2956: loss: 6.8733110427856445\n",
            "2957: loss: 6.6444091796875\n",
            "2958: loss: 6.863314628601074\n",
            "2959: loss: 6.6722917556762695\n",
            "2960: loss: 6.966304302215576\n",
            "2960: valid loss 6.74592399597168\n",
            "2961: loss: 6.848681926727295\n",
            "2962: loss: 7.00651741027832\n",
            "2963: loss: 6.850990295410156\n",
            "2964: loss: 7.0550994873046875\n",
            "2965: loss: 6.80186128616333\n",
            "2966: loss: 6.612756729125977\n",
            "2967: loss: 6.669275760650635\n",
            "2968: loss: 7.22733736038208\n",
            "2969: loss: 6.9114532470703125\n",
            "2970: loss: 6.920764923095703\n",
            "2971: loss: 6.826028823852539\n",
            "2972: loss: 6.773590087890625\n",
            "2973: loss: 7.147417068481445\n",
            "2974: loss: 7.086549282073975\n",
            "2975: loss: 6.620181560516357\n",
            "2976: loss: 6.75308895111084\n",
            "2977: loss: 6.943137168884277\n",
            "2978: loss: 6.538896560668945\n",
            "2979: loss: 6.588766574859619\n",
            "2980: loss: 7.010210990905762\n",
            "2980: valid loss 7.029555320739746\n",
            "2981: loss: 6.495565414428711\n",
            "2982: loss: 7.205125331878662\n",
            "2983: loss: 6.740870952606201\n",
            "2984: loss: 7.140781402587891\n",
            "2985: loss: 7.115258693695068\n",
            "2986: loss: 6.589650630950928\n",
            "2987: loss: 6.944298267364502\n",
            "2988: loss: 7.440873622894287\n",
            "2989: loss: 6.917996406555176\n",
            "2990: loss: 6.837088584899902\n",
            "2991: loss: 6.7699151039123535\n",
            "2992: loss: 7.223766326904297\n",
            "2993: loss: 6.734918117523193\n",
            "2994: loss: 6.705212593078613\n",
            "2995: loss: 6.660947322845459\n",
            "2996: loss: 6.701099395751953\n",
            "2997: loss: 7.048623561859131\n",
            "2998: loss: 6.566048622131348\n",
            "2999: loss: 7.109719753265381\n",
            "3000: loss: 7.412261009216309\n",
            "3000: valid loss 7.040092468261719\n",
            "3000: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "3001: loss: 6.9265007972717285\n",
            "3002: loss: 7.002419948577881\n",
            "3003: loss: 7.008610248565674\n",
            "3004: loss: 6.8189826011657715\n",
            "3005: loss: 6.938079833984375\n",
            "3006: loss: 6.844361305236816\n",
            "3007: loss: 7.127506732940674\n",
            "3008: loss: 6.606851100921631\n",
            "3009: loss: 6.731719970703125\n",
            "3010: loss: 6.954965114593506\n",
            "3011: loss: 6.957544326782227\n",
            "3012: loss: 6.896711349487305\n",
            "3013: loss: 6.856451988220215\n",
            "3014: loss: 6.755638122558594\n",
            "3015: loss: 6.87506103515625\n",
            "3016: loss: 6.870611190795898\n",
            "3017: loss: 6.827917575836182\n",
            "3018: loss: 7.154829025268555\n",
            "3019: loss: 6.795933723449707\n",
            "3020: loss: 7.17913293838501\n",
            "3020: valid loss 7.0650787353515625\n",
            "3021: loss: 6.81627893447876\n",
            "3022: loss: 7.107291221618652\n",
            "3023: loss: 6.913065433502197\n",
            "3024: loss: 6.798900127410889\n",
            "3025: loss: 6.906136989593506\n",
            "3026: loss: 6.863031387329102\n",
            "3027: loss: 6.822328090667725\n",
            "3028: loss: 6.760813236236572\n",
            "3029: loss: 6.931127071380615\n",
            "3030: loss: 7.318514823913574\n",
            "3031: loss: 6.941206455230713\n",
            "3032: loss: 6.845198154449463\n",
            "3033: loss: 6.795346736907959\n",
            "3034: loss: 6.83023738861084\n",
            "3035: loss: 6.724698066711426\n",
            "3036: loss: 6.56299352645874\n",
            "3037: loss: 7.053902626037598\n",
            "3038: loss: 6.933883190155029\n",
            "3039: loss: 7.256851673126221\n",
            "3040: loss: 6.755756378173828\n",
            "3040: valid loss 6.873973846435547\n",
            "3041: loss: 6.730029106140137\n",
            "3042: loss: 6.934310436248779\n",
            "3043: loss: 6.7920660972595215\n",
            "3044: loss: 6.778282165527344\n",
            "3045: loss: 6.9614787101745605\n",
            "3046: loss: 7.410009384155273\n",
            "3047: loss: 6.908332824707031\n",
            "3048: loss: 6.853990077972412\n",
            "3049: loss: 7.108555793762207\n",
            "3050: loss: 6.820151329040527\n",
            "3051: loss: 6.7230000495910645\n",
            "3052: loss: 6.897124290466309\n",
            "3053: loss: 6.986757278442383\n",
            "3054: loss: 7.096559047698975\n",
            "3055: loss: 6.873621940612793\n",
            "3056: loss: 7.079409122467041\n",
            "3057: loss: 6.627499580383301\n",
            "3058: loss: 6.793750762939453\n",
            "3059: loss: 6.880193710327148\n",
            "3060: loss: 6.859912872314453\n",
            "3060: valid loss 6.867495059967041\n",
            "3061: loss: 6.558860778808594\n",
            "3062: loss: 6.812811374664307\n",
            "3063: loss: 6.9835944175720215\n",
            "3064: loss: 7.295739650726318\n",
            "3065: loss: 6.9340434074401855\n",
            "3066: loss: 6.5770440101623535\n",
            "3067: loss: 6.851701736450195\n",
            "3068: loss: 7.122646331787109\n",
            "3069: loss: 6.9583845138549805\n",
            "3070: loss: 6.992652893066406\n",
            "3071: loss: 6.800173759460449\n",
            "3072: loss: 7.3273420333862305\n",
            "3073: loss: 7.16220760345459\n",
            "3074: loss: 7.179512977600098\n",
            "3075: loss: 6.740176677703857\n",
            "3076: loss: 6.722536087036133\n",
            "3077: loss: 7.050948619842529\n",
            "3078: loss: 6.911035060882568\n",
            "3079: loss: 6.822483062744141\n",
            "3080: loss: 6.745278835296631\n",
            "3080: valid loss 7.088295936584473\n",
            "3081: loss: 6.979338645935059\n",
            "3082: loss: 7.171082973480225\n",
            "3083: loss: 6.8226165771484375\n",
            "3084: loss: 6.645132064819336\n",
            "3085: loss: 7.153934478759766\n",
            "3086: loss: 6.586189270019531\n",
            "3087: loss: 6.961808681488037\n",
            "3088: loss: 6.749815940856934\n",
            "3089: loss: 6.833738327026367\n",
            "3090: loss: 6.633866310119629\n",
            "3091: loss: 6.7491888999938965\n",
            "3092: loss: 6.842379570007324\n",
            "3093: loss: 6.638737201690674\n",
            "3094: loss: 6.692345142364502\n",
            "3095: loss: 6.860429286956787\n",
            "3096: loss: 7.230724334716797\n",
            "3097: loss: 6.731694221496582\n",
            "3098: loss: 6.691537380218506\n",
            "3099: loss: 6.99998140335083\n",
            "3100: loss: 7.099695682525635\n",
            "3100: valid loss 6.8869171142578125\n",
            "3101: loss: 6.974855422973633\n",
            "3102: loss: 7.017597675323486\n",
            "3103: loss: 7.050366401672363\n",
            "3104: loss: 6.653030872344971\n",
            "3105: loss: 6.720800876617432\n",
            "3106: loss: 6.734351634979248\n",
            "3107: loss: 7.128965377807617\n",
            "3108: loss: 7.489803791046143\n",
            "3109: loss: 7.1021928787231445\n",
            "3110: loss: 7.253655910491943\n",
            "3111: loss: 6.344049453735352\n",
            "3112: loss: 6.995314598083496\n",
            "3113: loss: 6.922544002532959\n",
            "3114: loss: 6.770906448364258\n",
            "3115: loss: 6.884358882904053\n",
            "3116: loss: 7.287931442260742\n",
            "3117: loss: 6.586947917938232\n",
            "3118: loss: 6.831184387207031\n",
            "3119: loss: 6.771702766418457\n",
            "3120: loss: 6.6255693435668945\n",
            "3120: valid loss 7.016613006591797\n",
            "3121: loss: 6.982017993927002\n",
            "3122: loss: 6.620741844177246\n",
            "3123: loss: 6.754100322723389\n",
            "3124: loss: 6.901025295257568\n",
            "3125: loss: 6.69251012802124\n",
            "3126: loss: 6.858803749084473\n",
            "3127: loss: 6.352024078369141\n",
            "3128: loss: 6.5817413330078125\n",
            "3129: loss: 7.35158109664917\n",
            "3130: loss: 6.833188533782959\n",
            "3131: loss: 6.685858249664307\n",
            "3132: loss: 6.967829704284668\n",
            "3133: loss: 7.104788303375244\n",
            "3134: loss: 6.674894332885742\n",
            "3135: loss: 6.601362705230713\n",
            "3136: loss: 6.793740749359131\n",
            "3137: loss: 6.622254848480225\n",
            "3138: loss: 6.7166924476623535\n",
            "3139: loss: 7.341310024261475\n",
            "3140: loss: 6.8029632568359375\n",
            "3140: valid loss 6.91878604888916\n",
            "3141: loss: 6.712738990783691\n",
            "3142: loss: 6.824374675750732\n",
            "3143: loss: 6.948764801025391\n",
            "3144: loss: 6.676263809204102\n",
            "3145: loss: 6.712374210357666\n",
            "3146: loss: 6.966233253479004\n",
            "3147: loss: 6.812739849090576\n",
            "3148: loss: 6.603066444396973\n",
            "3149: loss: 6.883742332458496\n",
            "3150: loss: 6.973212242126465\n",
            "3151: loss: 7.140410900115967\n",
            "3152: loss: 7.029380798339844\n",
            "3153: loss: 6.538559913635254\n",
            "3154: loss: 7.1050286293029785\n",
            "3155: loss: 6.592501640319824\n",
            "3156: loss: 6.707854270935059\n",
            "3157: loss: 6.84749174118042\n",
            "3158: loss: 6.560916900634766\n",
            "3159: loss: 6.8097076416015625\n",
            "3160: loss: 6.939742088317871\n",
            "3160: valid loss 6.952116012573242\n",
            "3161: loss: 6.693363666534424\n",
            "3162: loss: 6.6310553550720215\n",
            "3163: loss: 7.209306716918945\n",
            "3164: loss: 6.934826374053955\n",
            "3165: loss: 7.02737283706665\n",
            "3166: loss: 6.749326705932617\n",
            "3167: loss: 6.736095905303955\n",
            "3168: loss: 6.654526233673096\n",
            "3169: loss: 6.909167289733887\n",
            "3170: loss: 6.537006378173828\n",
            "3171: loss: 7.1341328620910645\n",
            "3172: loss: 6.959709644317627\n",
            "3173: loss: 6.931893825531006\n",
            "3174: loss: 6.990793228149414\n",
            "3175: loss: 6.91973352432251\n",
            "3176: loss: 6.833625316619873\n",
            "3177: loss: 7.94110631942749\n",
            "3178: loss: 7.071298599243164\n",
            "3179: loss: 6.886627674102783\n",
            "3180: loss: 6.814704418182373\n",
            "3180: valid loss 6.798650741577148\n",
            "3181: loss: 7.046809196472168\n",
            "3182: loss: 6.6593780517578125\n",
            "3183: loss: 6.834751129150391\n",
            "3184: loss: 7.0662360191345215\n",
            "3185: loss: 6.866278648376465\n",
            "3186: loss: 6.610264301300049\n",
            "3187: loss: 6.8499555587768555\n",
            "3188: loss: 6.727203845977783\n",
            "3189: loss: 6.778159141540527\n",
            "3190: loss: 6.916241645812988\n",
            "3191: loss: 6.710758209228516\n",
            "3192: loss: 6.931161403656006\n",
            "3193: loss: 6.976738452911377\n",
            "3194: loss: 6.860505104064941\n",
            "3195: loss: 7.141190052032471\n",
            "3196: loss: 6.904658794403076\n",
            "3197: loss: 6.687376499176025\n",
            "3198: loss: 7.019956111907959\n",
            "3199: loss: 6.815591335296631\n",
            "3200: loss: 7.278450965881348\n",
            "3200: valid loss 6.66422176361084\n",
            "3200: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "3201: loss: 6.300782680511475\n",
            "3202: loss: 7.011544227600098\n",
            "3203: loss: 6.818630218505859\n",
            "3204: loss: 6.5211896896362305\n",
            "3205: loss: 6.987782001495361\n",
            "3206: loss: 6.987000465393066\n",
            "3207: loss: 7.228527545928955\n",
            "3208: loss: 7.11422872543335\n",
            "3209: loss: 7.207134246826172\n",
            "3210: loss: 6.879158020019531\n",
            "3211: loss: 7.381363868713379\n",
            "3212: loss: 7.266292095184326\n",
            "3213: loss: 6.895183086395264\n",
            "3214: loss: 6.776606559753418\n",
            "3215: loss: 7.162076473236084\n",
            "3216: loss: 7.050321102142334\n",
            "3217: loss: 6.8039093017578125\n",
            "3218: loss: 6.79934549331665\n",
            "3219: loss: 2.764345645904541\n",
            "3220: loss: 6.760817527770996\n",
            "3220: valid loss 6.758008003234863\n",
            "3221: loss: 6.777572154998779\n",
            "3222: loss: 7.084918975830078\n",
            "3223: loss: 7.170881748199463\n",
            "3224: loss: 6.861342906951904\n",
            "3225: loss: 6.932063579559326\n",
            "3226: loss: 6.637249946594238\n",
            "3227: loss: 6.4194769859313965\n",
            "3228: loss: 6.956219673156738\n",
            "3229: loss: 6.746889114379883\n",
            "3230: loss: 6.787980556488037\n",
            "3231: loss: 6.7311177253723145\n",
            "3232: loss: 6.88096809387207\n",
            "3233: loss: 6.645354270935059\n",
            "3234: loss: 6.750467300415039\n",
            "3235: loss: 7.210260391235352\n",
            "3236: loss: 6.871304512023926\n",
            "3237: loss: 6.541219711303711\n",
            "3238: loss: 7.074501991271973\n",
            "3239: loss: 6.711926460266113\n",
            "3240: loss: 6.9066948890686035\n",
            "3240: valid loss 6.746002197265625\n",
            "3241: loss: 6.711495876312256\n",
            "3242: loss: 6.6256585121154785\n",
            "3243: loss: 6.727001667022705\n",
            "3244: loss: 7.169143199920654\n",
            "3245: loss: 7.029601097106934\n",
            "3246: loss: 6.5514936447143555\n",
            "3247: loss: 6.6875319480896\n",
            "3248: loss: 7.059238910675049\n",
            "3249: loss: 6.878185749053955\n",
            "3250: loss: 6.736071586608887\n",
            "3251: loss: 6.813989162445068\n",
            "3252: loss: 6.607367515563965\n",
            "3253: loss: 6.852313995361328\n",
            "3254: loss: 6.708444595336914\n",
            "3255: loss: 6.727531433105469\n",
            "3256: loss: 6.68693733215332\n",
            "3257: loss: 6.993368625640869\n",
            "3258: loss: 7.0846662521362305\n",
            "3259: loss: 6.883288860321045\n",
            "3260: loss: 6.81581974029541\n",
            "3260: valid loss 6.695249557495117\n",
            "3261: loss: 6.739327907562256\n",
            "3262: loss: 6.9599432945251465\n",
            "3263: loss: 6.831963539123535\n",
            "3264: loss: 6.892215728759766\n",
            "3265: loss: 6.936018466949463\n",
            "3266: loss: 6.534592151641846\n",
            "3267: loss: 7.04435920715332\n",
            "3268: loss: 6.9513444900512695\n",
            "3269: loss: 6.957525253295898\n",
            "3270: loss: 6.678948879241943\n",
            "3271: loss: 6.904255390167236\n",
            "3272: loss: 7.125621318817139\n",
            "3273: loss: 7.0902299880981445\n",
            "3274: loss: 6.974331378936768\n",
            "3275: loss: 6.900710105895996\n",
            "3276: loss: 6.6312761306762695\n",
            "3277: loss: 7.181074142456055\n",
            "3278: loss: 7.034790515899658\n",
            "3279: loss: 6.739445209503174\n",
            "3280: loss: 6.916956424713135\n",
            "3280: valid loss 6.669641494750977\n",
            "3281: loss: 6.690521717071533\n",
            "3282: loss: 6.884557723999023\n",
            "3283: loss: 6.86936616897583\n",
            "3284: loss: 6.752346992492676\n",
            "3285: loss: 7.20090389251709\n",
            "3286: loss: 6.9088239669799805\n",
            "3287: loss: 6.524612903594971\n",
            "3288: loss: 6.555915832519531\n",
            "3289: loss: 6.643649578094482\n",
            "3290: loss: 7.140638828277588\n",
            "3291: loss: 7.071578502655029\n",
            "3292: loss: 6.743642330169678\n",
            "3293: loss: 6.611406326293945\n",
            "3294: loss: 7.31015682220459\n",
            "3295: loss: 7.140720367431641\n",
            "3296: loss: 6.803001880645752\n",
            "3297: loss: 8.538654327392578\n",
            "3298: loss: 6.900915622711182\n",
            "3299: loss: 7.019229412078857\n",
            "3300: loss: 6.8744659423828125\n",
            "3300: valid loss 6.958425521850586\n",
            "3301: loss: 6.678974628448486\n",
            "3302: loss: 6.540655136108398\n",
            "3303: loss: 6.844374179840088\n",
            "3304: loss: 7.033663272857666\n",
            "3305: loss: 6.941492557525635\n",
            "3306: loss: 6.668309688568115\n",
            "3307: loss: 7.090767860412598\n",
            "3308: loss: 6.8737311363220215\n",
            "3309: loss: 7.036394119262695\n",
            "3310: loss: 6.514815330505371\n",
            "3311: loss: 6.910452842712402\n",
            "3312: loss: 6.861406326293945\n",
            "3313: loss: 6.72160005569458\n",
            "3314: loss: 7.126089572906494\n",
            "3315: loss: 7.248971462249756\n",
            "3316: loss: 7.099278926849365\n",
            "3317: loss: 6.931096076965332\n",
            "3318: loss: 7.002130031585693\n",
            "3319: loss: 6.7444233894348145\n",
            "3320: loss: 7.016007900238037\n",
            "3320: valid loss 6.941811561584473\n",
            "3321: loss: 6.469933032989502\n",
            "3322: loss: 7.00331974029541\n",
            "3323: loss: 7.103729248046875\n",
            "3324: loss: 6.830598831176758\n",
            "3325: loss: 6.63087797164917\n",
            "3326: loss: 6.566744327545166\n",
            "3327: loss: 6.918842315673828\n",
            "3328: loss: 6.740255355834961\n",
            "3329: loss: 6.460577964782715\n",
            "3330: loss: 6.8635687828063965\n",
            "3331: loss: 6.5468645095825195\n",
            "3332: loss: 6.856463432312012\n",
            "3333: loss: 6.992884159088135\n",
            "3334: loss: 7.016476154327393\n",
            "3335: loss: 7.076086044311523\n",
            "3336: loss: 6.559779167175293\n",
            "3337: loss: 6.752718925476074\n",
            "3338: loss: 6.899805068969727\n",
            "3339: loss: 6.581618785858154\n",
            "3340: loss: 6.745820045471191\n",
            "3340: valid loss 6.781299591064453\n",
            "3341: loss: 6.776467800140381\n",
            "3342: loss: 6.89335298538208\n",
            "3343: loss: 6.79451847076416\n",
            "3344: loss: 6.92385721206665\n",
            "3345: loss: 7.164945602416992\n",
            "3346: loss: 6.803472995758057\n",
            "3347: loss: 6.6425323486328125\n",
            "3348: loss: 6.980991840362549\n",
            "3349: loss: 6.9198431968688965\n",
            "3350: loss: 6.6282877922058105\n",
            "3351: loss: 6.971471309661865\n",
            "3352: loss: 6.894258975982666\n",
            "3353: loss: 6.8354926109313965\n",
            "3354: loss: 6.978261947631836\n",
            "3355: loss: 6.875863075256348\n",
            "3356: loss: 6.753767967224121\n",
            "3357: loss: 6.672517776489258\n",
            "3358: loss: 6.804824352264404\n",
            "3359: loss: 7.073612689971924\n",
            "3360: loss: 6.831593990325928\n",
            "3360: valid loss 6.8422136306762695\n",
            "3361: loss: 7.337198257446289\n",
            "3362: loss: 6.986896514892578\n",
            "3363: loss: 6.706855297088623\n",
            "3364: loss: 6.615389823913574\n",
            "3365: loss: 6.626006126403809\n",
            "3366: loss: 6.815066337585449\n",
            "3367: loss: 6.877361297607422\n",
            "3368: loss: 7.098038196563721\n",
            "3369: loss: 6.655516147613525\n",
            "3370: loss: 6.728490352630615\n",
            "3371: loss: 7.111758232116699\n",
            "3372: loss: 6.673303604125977\n",
            "3373: loss: 6.965367794036865\n",
            "3374: loss: 6.8806586265563965\n",
            "3375: loss: 6.842693328857422\n",
            "3376: loss: 6.756495475769043\n",
            "3377: loss: 6.682698726654053\n",
            "3378: loss: 6.6886186599731445\n",
            "3379: loss: 7.0942864418029785\n",
            "3380: loss: 7.041887283325195\n",
            "3380: valid loss 6.731491565704346\n",
            "3381: loss: 6.731947898864746\n",
            "3382: loss: 6.5793256759643555\n",
            "3383: loss: 6.797088146209717\n",
            "3384: loss: 6.592828750610352\n",
            "3385: loss: 6.7270073890686035\n",
            "3386: loss: 6.656093120574951\n",
            "3387: loss: 7.047305583953857\n",
            "3388: loss: 6.699230670928955\n",
            "3389: loss: 7.121070384979248\n",
            "3390: loss: 6.695214748382568\n",
            "3391: loss: 6.8652729988098145\n",
            "3392: loss: 7.076076507568359\n",
            "3393: loss: 7.1081976890563965\n",
            "3394: loss: 6.699126243591309\n",
            "3395: loss: 7.067415237426758\n",
            "3396: loss: 6.5813798904418945\n",
            "3397: loss: 7.029261589050293\n",
            "3398: loss: 6.895429611206055\n",
            "3399: loss: 6.909254550933838\n",
            "3400: loss: 6.810675621032715\n",
            "3400: valid loss 6.659040927886963\n",
            "3400: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "3401: loss: 6.550548076629639\n",
            "3402: loss: 7.050243854522705\n",
            "3403: loss: 7.699653625488281\n",
            "3404: loss: 7.118420600891113\n",
            "3405: loss: 7.1995415687561035\n",
            "3406: loss: 7.095376968383789\n",
            "3407: loss: 7.0145039558410645\n",
            "3408: loss: 6.966573715209961\n",
            "3409: loss: 6.964658737182617\n",
            "3410: loss: 6.885627746582031\n",
            "3411: loss: 7.190402507781982\n",
            "3412: loss: 6.916515827178955\n",
            "3413: loss: 6.750034809112549\n",
            "3414: loss: 7.1221747398376465\n",
            "3415: loss: 6.668233871459961\n",
            "3416: loss: 6.988034248352051\n",
            "3417: loss: 6.92151403427124\n",
            "3418: loss: 7.0406575202941895\n",
            "3419: loss: 7.1493401527404785\n",
            "3420: loss: 6.9020915031433105\n",
            "3420: valid loss 7.019450664520264\n",
            "3421: loss: 7.012207508087158\n",
            "3422: loss: 7.86647367477417\n",
            "3423: loss: 7.2353692054748535\n",
            "3424: loss: 7.0223708152771\n",
            "3425: loss: 6.979260444641113\n",
            "3426: loss: 6.942209243774414\n",
            "3427: loss: 6.726061820983887\n",
            "3428: loss: 6.930246829986572\n",
            "3429: loss: 6.843542575836182\n",
            "3430: loss: 6.428144454956055\n",
            "3431: loss: 6.900386810302734\n",
            "3432: loss: 6.960310935974121\n",
            "3433: loss: 7.166093826293945\n",
            "3434: loss: 6.706246376037598\n",
            "3435: loss: 6.609065532684326\n",
            "3436: loss: 6.8822526931762695\n",
            "3437: loss: 6.830719470977783\n",
            "3438: loss: 6.785372734069824\n",
            "3439: loss: 6.568333148956299\n",
            "3440: loss: 7.041567802429199\n",
            "3440: valid loss 6.800939559936523\n",
            "3441: loss: 6.7010064125061035\n",
            "3442: loss: 6.7773613929748535\n",
            "3443: loss: 6.958946228027344\n",
            "3444: loss: 6.878623008728027\n",
            "3445: loss: 6.771114349365234\n",
            "3446: loss: 7.183139801025391\n",
            "3447: loss: 6.926346778869629\n",
            "3448: loss: 6.752758979797363\n",
            "3449: loss: 6.879302501678467\n",
            "3450: loss: 7.047420024871826\n",
            "3451: loss: 7.014341831207275\n",
            "3452: loss: 6.6039652824401855\n",
            "3453: loss: 6.87054967880249\n",
            "3454: loss: 7.119861125946045\n",
            "3455: loss: 6.844795227050781\n",
            "3456: loss: 6.737934589385986\n",
            "3457: loss: 7.081786155700684\n",
            "3458: loss: 6.6396260261535645\n",
            "3459: loss: 7.019427299499512\n",
            "3460: loss: 6.792590618133545\n",
            "3460: valid loss 6.619205474853516\n",
            "3461: loss: 6.668365478515625\n",
            "3462: loss: 6.9987874031066895\n",
            "3463: loss: 6.971210956573486\n",
            "3464: loss: 6.926718711853027\n",
            "3465: loss: 6.660069942474365\n",
            "3466: loss: 6.59576940536499\n",
            "3467: loss: 6.658660411834717\n",
            "3468: loss: 6.7480058670043945\n",
            "3469: loss: 7.206140995025635\n",
            "3470: loss: 6.650885581970215\n",
            "3471: loss: 6.959521293640137\n",
            "3472: loss: 6.802155017852783\n",
            "3473: loss: 6.753121852874756\n",
            "3474: loss: 7.067439079284668\n",
            "3475: loss: 7.070754528045654\n",
            "3476: loss: 7.252954959869385\n",
            "3477: loss: 6.688573837280273\n",
            "3478: loss: 6.924853324890137\n",
            "3479: loss: 6.624857425689697\n",
            "3480: loss: 6.7851972579956055\n",
            "3480: valid loss 6.805902004241943\n",
            "3481: loss: 6.869349479675293\n",
            "3482: loss: 6.952159881591797\n",
            "3483: loss: 6.8103156089782715\n",
            "3484: loss: 6.687709808349609\n",
            "3485: loss: 6.688146591186523\n",
            "3486: loss: 6.554089546203613\n",
            "3487: loss: 7.120013236999512\n",
            "3488: loss: 6.863743782043457\n",
            "3489: loss: 6.965798854827881\n",
            "3490: loss: 6.558554649353027\n",
            "3491: loss: 6.724269866943359\n",
            "3492: loss: 6.848589897155762\n",
            "3493: loss: 6.904114723205566\n",
            "3494: loss: 6.362618446350098\n",
            "3495: loss: 6.932243347167969\n",
            "3496: loss: 6.639941692352295\n",
            "3497: loss: 6.839263916015625\n",
            "3498: loss: 6.884617328643799\n",
            "3499: loss: 6.9577131271362305\n",
            "3500: loss: 7.219970703125\n",
            "3500: valid loss 6.942190647125244\n",
            "3501: loss: 7.057079315185547\n",
            "3502: loss: 6.824827671051025\n",
            "3503: loss: 6.779960632324219\n",
            "3504: loss: 6.962591648101807\n",
            "3505: loss: 7.409639358520508\n",
            "3506: loss: 6.856156826019287\n",
            "3507: loss: 7.002264022827148\n",
            "3508: loss: 6.669606685638428\n",
            "3509: loss: 6.8742899894714355\n",
            "3510: loss: 6.712887763977051\n",
            "3511: loss: 6.755169868469238\n",
            "3512: loss: 7.141404151916504\n",
            "3513: loss: 6.839649200439453\n",
            "3514: loss: 6.874910831451416\n",
            "3515: loss: 6.570775985717773\n",
            "3516: loss: 6.896300792694092\n",
            "3517: loss: 6.6437835693359375\n",
            "3518: loss: 6.776889801025391\n",
            "3519: loss: 6.674998760223389\n",
            "3520: loss: 6.669222831726074\n",
            "3520: valid loss 6.772164821624756\n",
            "3521: loss: 6.613552093505859\n",
            "3522: loss: 6.837070941925049\n",
            "3523: loss: 6.864358425140381\n",
            "3524: loss: 0.7136677503585815\n",
            "3525: loss: 6.93966817855835\n",
            "3526: loss: 6.748002052307129\n",
            "3527: loss: 7.028972625732422\n",
            "3528: loss: 6.542003154754639\n",
            "3529: loss: 6.9615960121154785\n",
            "3530: loss: 6.98970890045166\n",
            "3531: loss: 7.049401760101318\n",
            "3532: loss: 6.469432353973389\n",
            "3533: loss: 6.8267130851745605\n",
            "3534: loss: 6.92565393447876\n",
            "3535: loss: 7.023419380187988\n",
            "3536: loss: 6.43009090423584\n",
            "3537: loss: 6.67222785949707\n",
            "3538: loss: 6.97044038772583\n",
            "3539: loss: 6.543115139007568\n",
            "3540: loss: 6.9302873611450195\n",
            "3540: valid loss 6.4502949714660645\n",
            "3541: loss: 6.960812091827393\n",
            "3542: loss: 6.82895565032959\n",
            "3543: loss: 6.89333963394165\n",
            "3544: loss: 6.857137680053711\n",
            "3545: loss: 6.867599964141846\n",
            "3546: loss: 6.861077308654785\n",
            "3547: loss: 7.11752986907959\n",
            "3548: loss: 7.08806037902832\n",
            "3549: loss: 6.8343987464904785\n",
            "3550: loss: 6.99433708190918\n",
            "3551: loss: 6.788232803344727\n",
            "3552: loss: 6.844854354858398\n",
            "3553: loss: 6.765927791595459\n",
            "3554: loss: 6.866037368774414\n",
            "3555: loss: 6.839967250823975\n",
            "3556: loss: 6.883181571960449\n",
            "3557: loss: 6.949390888214111\n",
            "3558: loss: 6.7932209968566895\n",
            "3559: loss: 6.941885471343994\n",
            "3560: loss: 6.982376575469971\n",
            "3560: valid loss 7.606403350830078\n",
            "3561: loss: 6.904423236846924\n",
            "3562: loss: 6.981374740600586\n",
            "3563: loss: 6.866926670074463\n",
            "3564: loss: 6.944004058837891\n",
            "3565: loss: 6.896787643432617\n",
            "3566: loss: 6.995029449462891\n",
            "3567: loss: 6.908766746520996\n",
            "3568: loss: 6.951718330383301\n",
            "3569: loss: 6.947025299072266\n",
            "3570: loss: 6.812117576599121\n",
            "3571: loss: 6.761769771575928\n",
            "3572: loss: 6.581665992736816\n",
            "3573: loss: 6.802504062652588\n",
            "3574: loss: 7.196084976196289\n",
            "3575: loss: 7.12700891494751\n",
            "3576: loss: 6.813262462615967\n",
            "3577: loss: 6.725346088409424\n",
            "3578: loss: 7.014543056488037\n",
            "3579: loss: 6.8326416015625\n",
            "3580: loss: 6.851205348968506\n",
            "3580: valid loss 6.526518821716309\n",
            "3581: loss: 6.570323944091797\n",
            "3582: loss: 6.786507606506348\n",
            "3583: loss: 6.774738311767578\n",
            "3584: loss: 6.768901348114014\n",
            "3585: loss: 6.872415542602539\n",
            "3586: loss: 6.729392051696777\n",
            "3587: loss: 6.787788391113281\n",
            "3588: loss: 6.969067096710205\n",
            "3589: loss: 6.877513885498047\n",
            "3590: loss: 7.007004261016846\n",
            "3591: loss: 6.806135654449463\n",
            "3592: loss: 6.728372573852539\n",
            "3593: loss: 7.056025981903076\n",
            "3594: loss: 7.188625335693359\n",
            "3595: loss: 6.943972587585449\n",
            "3596: loss: 6.7943115234375\n",
            "3597: loss: 6.91746187210083\n",
            "3598: loss: 6.821348190307617\n",
            "3599: loss: 6.713310241699219\n",
            "3600: loss: 6.732969760894775\n",
            "3600: valid loss 6.873867511749268\n",
            "3600: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "3601: loss: 6.786019325256348\n",
            "3602: loss: 6.493107318878174\n",
            "3603: loss: 6.853298664093018\n",
            "3604: loss: 6.692593574523926\n",
            "3605: loss: 7.003671646118164\n",
            "3606: loss: 6.990089416503906\n",
            "3607: loss: 6.695631980895996\n",
            "3608: loss: 6.619935512542725\n",
            "3609: loss: 6.775467872619629\n",
            "3610: loss: 7.700520992279053\n",
            "3611: loss: 6.988570690155029\n",
            "3612: loss: 6.522441864013672\n",
            "3613: loss: 6.939223289489746\n",
            "3614: loss: 7.013919353485107\n",
            "3615: loss: 6.815309524536133\n",
            "3616: loss: 7.102339267730713\n",
            "3617: loss: 6.663781642913818\n",
            "3618: loss: 6.603233814239502\n",
            "3619: loss: 6.87147855758667\n",
            "3620: loss: 6.434642314910889\n",
            "3620: valid loss 6.939147472381592\n",
            "3621: loss: 6.760066509246826\n",
            "3622: loss: 6.888621807098389\n",
            "3623: loss: 6.8142900466918945\n",
            "3624: loss: 6.958067417144775\n",
            "3625: loss: 6.495940208435059\n",
            "3626: loss: 6.677145957946777\n",
            "3627: loss: 6.573543548583984\n",
            "3628: loss: 6.7313337326049805\n",
            "3629: loss: 6.971247673034668\n",
            "3630: loss: 7.269749164581299\n",
            "3631: loss: 7.137924671173096\n",
            "3632: loss: 6.7779693603515625\n",
            "3633: loss: 6.741313934326172\n",
            "3634: loss: 6.92973518371582\n",
            "3635: loss: 6.862466812133789\n",
            "3636: loss: 6.267483711242676\n",
            "3637: loss: 6.893864154815674\n",
            "3638: loss: 6.906798839569092\n",
            "3639: loss: 6.798458576202393\n",
            "3640: loss: 7.063178062438965\n",
            "3640: valid loss 7.219221115112305\n",
            "3641: loss: 6.872768878936768\n",
            "3642: loss: 6.958065509796143\n",
            "3643: loss: 6.702091217041016\n",
            "3644: loss: 6.883484363555908\n",
            "3645: loss: 6.877058506011963\n",
            "3646: loss: 6.831458568572998\n",
            "3647: loss: 6.936885833740234\n",
            "3648: loss: 6.562739372253418\n",
            "3649: loss: 7.042562007904053\n",
            "3650: loss: 6.893657207489014\n",
            "3651: loss: 6.8346333503723145\n",
            "3652: loss: 6.5756940841674805\n",
            "3653: loss: 7.232143402099609\n",
            "3654: loss: 7.010225772857666\n",
            "3655: loss: 6.894835948944092\n",
            "3656: loss: 7.3948469161987305\n",
            "3657: loss: 6.707247257232666\n",
            "3658: loss: 7.0029706954956055\n",
            "3659: loss: 6.913046836853027\n",
            "3660: loss: 6.987582683563232\n",
            "3660: valid loss 6.927644729614258\n",
            "3661: loss: 6.660091876983643\n",
            "3662: loss: 6.710484504699707\n",
            "3663: loss: 6.597434997558594\n",
            "3664: loss: 7.062054634094238\n",
            "3665: loss: 6.974706649780273\n",
            "3666: loss: 6.822028636932373\n",
            "3667: loss: 6.903660297393799\n",
            "3668: loss: 6.809799671173096\n",
            "3669: loss: 6.67185115814209\n",
            "3670: loss: 7.065277576446533\n",
            "3671: loss: 6.724218845367432\n",
            "3672: loss: 7.0059027671813965\n",
            "3673: loss: 6.847095489501953\n",
            "3674: loss: 6.473431587219238\n",
            "3675: loss: 6.789427757263184\n",
            "3676: loss: 6.892172813415527\n",
            "3677: loss: 6.676174640655518\n",
            "3678: loss: 6.907870769500732\n",
            "3679: loss: 6.665738105773926\n",
            "3680: loss: 6.903678894042969\n",
            "3680: valid loss 7.32168436050415\n",
            "3681: loss: 6.465070724487305\n",
            "3682: loss: 6.519699573516846\n",
            "3683: loss: 6.826681613922119\n",
            "3684: loss: 6.770136833190918\n",
            "3685: loss: 6.4035468101501465\n",
            "3686: loss: 7.003676891326904\n",
            "3687: loss: 6.5836687088012695\n",
            "3688: loss: 6.984537124633789\n",
            "3689: loss: 6.843321800231934\n",
            "3690: loss: 6.826209545135498\n",
            "3691: loss: 7.087398052215576\n",
            "3692: loss: 6.847565650939941\n",
            "3693: loss: 7.070719242095947\n",
            "3694: loss: 6.579336643218994\n",
            "3695: loss: 6.9179816246032715\n",
            "3696: loss: 6.857306957244873\n",
            "3697: loss: 6.8973388671875\n",
            "3698: loss: 6.60997200012207\n",
            "3699: loss: 6.341677665710449\n",
            "3700: loss: 6.898693561553955\n",
            "3700: valid loss 6.850488185882568\n",
            "3701: loss: 6.439091682434082\n",
            "3702: loss: 6.973325252532959\n",
            "3703: loss: 6.701089382171631\n",
            "3704: loss: 6.645075798034668\n",
            "3705: loss: 6.462393760681152\n",
            "3706: loss: 7.034200191497803\n",
            "3707: loss: 6.476778030395508\n",
            "3708: loss: 7.164479732513428\n",
            "3709: loss: 6.7283244132995605\n",
            "3710: loss: 6.783865928649902\n",
            "3711: loss: 6.4721293449401855\n",
            "3712: loss: 6.9835309982299805\n",
            "3713: loss: 7.198343276977539\n",
            "3714: loss: 6.703347682952881\n",
            "3715: loss: 6.612513065338135\n",
            "3716: loss: 7.0724568367004395\n",
            "3717: loss: 7.256707191467285\n",
            "3718: loss: 6.616788387298584\n",
            "3719: loss: 7.255250453948975\n",
            "3720: loss: 6.81400203704834\n",
            "3720: valid loss 6.4563374519348145\n",
            "3721: loss: 6.992776393890381\n",
            "3722: loss: 6.983222484588623\n",
            "3723: loss: 6.783474445343018\n",
            "3724: loss: 6.9289751052856445\n",
            "3725: loss: 6.522925853729248\n",
            "3726: loss: 6.87722110748291\n",
            "3727: loss: 6.957162857055664\n",
            "3728: loss: 6.96691370010376\n",
            "3729: loss: 7.002103328704834\n",
            "3730: loss: 6.931573390960693\n",
            "3731: loss: 6.785976409912109\n",
            "3732: loss: 6.8300371170043945\n",
            "3733: loss: 6.870197772979736\n",
            "3734: loss: 6.694924354553223\n",
            "3735: loss: 6.637866973876953\n",
            "3736: loss: 6.369155406951904\n",
            "3737: loss: 6.696502208709717\n",
            "3738: loss: 7.21246862411499\n",
            "3739: loss: 6.664261817932129\n",
            "3740: loss: 6.561187744140625\n",
            "3740: valid loss 6.669853210449219\n",
            "3741: loss: 6.908308982849121\n",
            "3742: loss: 6.759449005126953\n",
            "3743: loss: 6.646740436553955\n",
            "3744: loss: 6.664594650268555\n",
            "3745: loss: 6.386185646057129\n",
            "3746: loss: 6.577542304992676\n",
            "3747: loss: 6.99824333190918\n",
            "3748: loss: 6.913406848907471\n",
            "3749: loss: 6.870728969573975\n",
            "3750: loss: 7.157848834991455\n",
            "3751: loss: 6.809559345245361\n",
            "3752: loss: 7.087810039520264\n",
            "3753: loss: 6.672333717346191\n",
            "3754: loss: 6.359350681304932\n",
            "3755: loss: 6.697230815887451\n",
            "3756: loss: 6.782395839691162\n",
            "3757: loss: 6.611413478851318\n",
            "3758: loss: 6.716221332550049\n",
            "3759: loss: 6.518199443817139\n",
            "3760: loss: 6.791555881500244\n",
            "3760: valid loss 6.9798903465271\n",
            "3761: loss: 6.736133098602295\n",
            "3762: loss: 7.00313663482666\n",
            "3763: loss: 6.884007930755615\n",
            "3764: loss: 7.8035101890563965\n",
            "3765: loss: 6.409146785736084\n",
            "3766: loss: 7.066229820251465\n",
            "3767: loss: 7.044540882110596\n",
            "3768: loss: 7.294318675994873\n",
            "3769: loss: 7.085448741912842\n",
            "3770: loss: 6.681971549987793\n",
            "3771: loss: 6.872386932373047\n",
            "3772: loss: 6.5633134841918945\n",
            "3773: loss: 6.930472373962402\n",
            "3774: loss: 6.966628551483154\n",
            "3775: loss: 6.749302864074707\n",
            "3776: loss: 6.61437463760376\n",
            "3777: loss: 6.863510608673096\n",
            "3778: loss: 6.80061674118042\n",
            "3779: loss: 6.64689302444458\n",
            "3780: loss: 6.690266132354736\n",
            "3780: valid loss 7.1129150390625\n",
            "3781: loss: 6.848324298858643\n",
            "3782: loss: 6.511141777038574\n",
            "3783: loss: 6.387475967407227\n",
            "3784: loss: 6.828395366668701\n",
            "3785: loss: 7.160432815551758\n",
            "3786: loss: 7.241753101348877\n",
            "3787: loss: 6.663925647735596\n",
            "3788: loss: 6.711893558502197\n",
            "3789: loss: 6.6277899742126465\n",
            "3790: loss: 6.771013259887695\n",
            "3791: loss: 6.955242156982422\n",
            "3792: loss: 6.762226581573486\n",
            "3793: loss: 6.490977764129639\n",
            "3794: loss: 7.012955188751221\n",
            "3795: loss: 6.973330020904541\n",
            "3796: loss: 6.661435127258301\n",
            "3797: loss: 7.066165924072266\n",
            "3798: loss: 6.746444225311279\n",
            "3799: loss: 7.0777106285095215\n",
            "3800: loss: 6.693022727966309\n",
            "3800: valid loss 6.857293605804443\n",
            "3800: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "3801: loss: 6.7559614181518555\n",
            "3802: loss: 6.669832706451416\n",
            "3803: loss: 6.8877387046813965\n",
            "3804: loss: 6.871039390563965\n",
            "3805: loss: 6.590847492218018\n",
            "3806: loss: 6.609914779663086\n",
            "3807: loss: 7.091033935546875\n",
            "3808: loss: 6.8299126625061035\n",
            "3809: loss: 6.735861778259277\n",
            "3810: loss: 6.993875503540039\n",
            "3811: loss: 6.952427387237549\n",
            "3812: loss: 7.11423921585083\n",
            "3813: loss: 6.890411376953125\n",
            "3814: loss: 6.933243274688721\n",
            "3815: loss: 6.479862213134766\n",
            "3816: loss: 6.939484119415283\n",
            "3817: loss: 6.510227203369141\n",
            "3818: loss: 6.8727946281433105\n",
            "3819: loss: 6.834978103637695\n",
            "3820: loss: 6.543801784515381\n",
            "3820: valid loss 6.244210243225098\n",
            "3821: loss: 6.793920993804932\n",
            "3822: loss: 6.912817478179932\n",
            "3823: loss: 7.363306522369385\n",
            "3824: loss: 6.761336326599121\n",
            "3825: loss: 6.828275680541992\n",
            "3826: loss: 6.895918369293213\n",
            "3827: loss: 6.857894420623779\n",
            "3828: loss: 6.50893497467041\n",
            "3829: loss: 6.610386371612549\n",
            "3830: loss: 6.888354301452637\n",
            "3831: loss: 6.762269973754883\n",
            "3832: loss: 6.950026988983154\n",
            "3833: loss: 6.871150493621826\n",
            "3834: loss: 6.9866533279418945\n",
            "3835: loss: 6.7719926834106445\n",
            "3836: loss: 6.512749671936035\n",
            "3837: loss: 7.081644535064697\n",
            "3838: loss: 6.638881683349609\n",
            "3839: loss: 6.698645114898682\n",
            "3840: loss: 6.674139022827148\n",
            "3840: valid loss 6.916624069213867\n",
            "3841: loss: 6.846920490264893\n",
            "3842: loss: 6.608445644378662\n",
            "3843: loss: 7.33925199508667\n",
            "3844: loss: 6.696134567260742\n",
            "3845: loss: 6.760827541351318\n",
            "3846: loss: 6.463140487670898\n",
            "3847: loss: 6.68295955657959\n",
            "3848: loss: 6.586738586425781\n",
            "3849: loss: 7.10179328918457\n",
            "3850: loss: 6.899785995483398\n",
            "3851: loss: 6.669083595275879\n",
            "3852: loss: 6.705390930175781\n",
            "3853: loss: 6.7952165603637695\n",
            "3854: loss: 7.134857654571533\n",
            "3855: loss: 6.526983261108398\n",
            "3856: loss: 7.865643501281738\n",
            "3857: loss: 6.929588794708252\n",
            "3858: loss: 6.552259922027588\n",
            "3859: loss: 6.7881855964660645\n",
            "3860: loss: 6.987731456756592\n",
            "3860: valid loss 7.133394718170166\n",
            "3861: loss: 6.7652974128723145\n",
            "3862: loss: 6.68849515914917\n",
            "3863: loss: 6.727607250213623\n",
            "3864: loss: 6.9505934715271\n",
            "3865: loss: 6.873044490814209\n",
            "3866: loss: 6.798216819763184\n",
            "3867: loss: 6.979052543640137\n",
            "3868: loss: 6.292038440704346\n",
            "3869: loss: 6.640128135681152\n",
            "3870: loss: 6.686189651489258\n",
            "3871: loss: 6.435311317443848\n",
            "3872: loss: 7.061588287353516\n",
            "3873: loss: 6.770962238311768\n",
            "3874: loss: 6.458497524261475\n",
            "3875: loss: 7.015938758850098\n",
            "3876: loss: 6.596347332000732\n",
            "3877: loss: 7.039183616638184\n",
            "3878: loss: 6.400358200073242\n",
            "3879: loss: 6.9309282302856445\n",
            "3880: loss: 6.932323455810547\n",
            "3880: valid loss 6.810542583465576\n",
            "3881: loss: 6.61211633682251\n",
            "3882: loss: 7.148838996887207\n",
            "3883: loss: 6.985809326171875\n",
            "3884: loss: 6.733487606048584\n",
            "3885: loss: 7.048829078674316\n",
            "3886: loss: 6.600221157073975\n",
            "3887: loss: 6.845465660095215\n",
            "3888: loss: 6.980158805847168\n",
            "3889: loss: 6.651571273803711\n",
            "3890: loss: 6.835193157196045\n",
            "3891: loss: 6.719582557678223\n",
            "3892: loss: 7.213459491729736\n",
            "3893: loss: 6.943317413330078\n",
            "3894: loss: 6.449028015136719\n",
            "3895: loss: 6.766607761383057\n",
            "3896: loss: 7.023721218109131\n",
            "3897: loss: 6.7402191162109375\n",
            "3898: loss: 6.8192644119262695\n",
            "3899: loss: 6.527376174926758\n",
            "3900: loss: 6.885534763336182\n",
            "3900: valid loss 6.933595657348633\n",
            "3901: loss: 6.8217926025390625\n",
            "3902: loss: 6.839725494384766\n",
            "3903: loss: 6.94096040725708\n",
            "3904: loss: 6.738523960113525\n",
            "3905: loss: 6.661300182342529\n",
            "3906: loss: 7.046050071716309\n",
            "3907: loss: 6.764063835144043\n",
            "3908: loss: 6.832350254058838\n",
            "3909: loss: 6.544952392578125\n",
            "3910: loss: 6.729576587677002\n",
            "3911: loss: 6.55293607711792\n",
            "3912: loss: 6.742599010467529\n",
            "3913: loss: 6.918968200683594\n",
            "3914: loss: 6.24691104888916\n",
            "3915: loss: 6.604116439819336\n",
            "3916: loss: 6.791858673095703\n",
            "3917: loss: 7.152529239654541\n",
            "3918: loss: 7.063185214996338\n",
            "3919: loss: 6.943571090698242\n",
            "3920: loss: 6.8090291023254395\n",
            "3920: valid loss 6.700092315673828\n",
            "3921: loss: 6.5547003746032715\n",
            "3922: loss: 7.032777786254883\n",
            "3923: loss: 6.6671342849731445\n",
            "3924: loss: 6.633415222167969\n",
            "3925: loss: 6.843611240386963\n",
            "3926: loss: 6.802923202514648\n",
            "3927: loss: 6.763142108917236\n",
            "3928: loss: 7.188723087310791\n",
            "3929: loss: 6.938925743103027\n",
            "3930: loss: 6.718059539794922\n",
            "3931: loss: 6.836276054382324\n",
            "3932: loss: 6.7712321281433105\n",
            "3933: loss: 6.780633926391602\n",
            "3934: loss: 6.750599384307861\n",
            "3935: loss: 6.992675304412842\n",
            "3936: loss: 6.8692450523376465\n",
            "3937: loss: 6.576374053955078\n",
            "3938: loss: 6.533377170562744\n",
            "3939: loss: 6.421157360076904\n",
            "3940: loss: 7.369889259338379\n",
            "3940: valid loss 6.985527038574219\n",
            "3941: loss: 6.662639617919922\n",
            "3942: loss: 7.330874919891357\n",
            "3943: loss: 6.697005748748779\n",
            "3944: loss: 7.019796371459961\n",
            "3945: loss: 7.201603412628174\n",
            "3946: loss: 6.823760986328125\n",
            "3947: loss: 6.69021463394165\n",
            "3948: loss: 6.890523910522461\n",
            "3949: loss: 6.709816932678223\n",
            "3950: loss: 7.24623441696167\n",
            "3951: loss: 6.886747360229492\n",
            "3952: loss: 6.633387088775635\n",
            "3953: loss: 7.076775550842285\n",
            "3954: loss: 6.798855781555176\n",
            "3955: loss: 6.948423385620117\n",
            "3956: loss: 6.384684085845947\n",
            "3957: loss: 6.662026405334473\n",
            "3958: loss: 6.892422199249268\n",
            "3959: loss: 6.520028591156006\n",
            "3960: loss: 6.7684783935546875\n",
            "3960: valid loss 6.918529987335205\n",
            "3961: loss: 7.198202610015869\n",
            "3962: loss: 6.56417179107666\n",
            "3963: loss: 6.400568962097168\n",
            "3964: loss: 6.531070232391357\n",
            "3965: loss: 6.914183616638184\n",
            "3966: loss: 6.733086109161377\n",
            "3967: loss: 6.999781608581543\n",
            "3968: loss: 6.768917560577393\n",
            "3969: loss: 6.747673034667969\n",
            "3970: loss: 6.79135274887085\n",
            "3971: loss: 6.692177772521973\n",
            "3972: loss: 6.617688179016113\n",
            "3973: loss: 6.770971775054932\n",
            "3974: loss: 6.569397926330566\n",
            "3975: loss: 6.684769153594971\n",
            "3976: loss: 6.501668453216553\n",
            "3977: loss: 6.769301414489746\n",
            "3978: loss: 6.624087810516357\n",
            "3979: loss: 6.867280960083008\n",
            "3980: loss: 6.9092912673950195\n",
            "3980: valid loss 7.033478260040283\n",
            "3981: loss: 6.916266441345215\n",
            "3982: loss: 6.8632683753967285\n",
            "3983: loss: 6.715909957885742\n",
            "3984: loss: 6.9360737800598145\n",
            "3985: loss: 6.7151713371276855\n",
            "3986: loss: 6.807452201843262\n",
            "3987: loss: 6.736204624176025\n",
            "3988: loss: 6.433949947357178\n",
            "3989: loss: 6.556034088134766\n",
            "3990: loss: 6.64709997177124\n",
            "3991: loss: 6.7169880867004395\n",
            "3992: loss: 6.8278584480285645\n",
            "3993: loss: 6.692473411560059\n",
            "3994: loss: 6.950960159301758\n",
            "3995: loss: 1.111506462097168\n",
            "3996: loss: 7.1727495193481445\n",
            "3997: loss: 7.102812767028809\n",
            "3998: loss: 7.047235488891602\n",
            "3999: loss: 7.630062580108643\n",
            "4000: loss: 6.777566909790039\n",
            "4000: valid loss 6.640183448791504\n",
            "4000: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "4001: loss: 6.898020267486572\n",
            "4002: loss: 6.894879341125488\n",
            "4003: loss: 6.719465732574463\n",
            "4004: loss: 6.651688098907471\n",
            "4005: loss: 6.62425422668457\n",
            "4006: loss: 7.034015655517578\n",
            "4007: loss: 6.785338878631592\n",
            "4008: loss: 6.606684684753418\n",
            "4009: loss: 6.438983917236328\n",
            "4010: loss: 7.078079700469971\n",
            "4011: loss: 6.378355503082275\n",
            "4012: loss: 6.910705089569092\n",
            "4013: loss: 6.970502853393555\n",
            "4014: loss: 6.743222713470459\n",
            "4015: loss: 6.703102111816406\n",
            "4016: loss: 6.794168949127197\n",
            "4017: loss: 6.641752243041992\n",
            "4018: loss: 6.930281639099121\n",
            "4019: loss: 6.733295440673828\n",
            "4020: loss: 6.759303092956543\n",
            "4020: valid loss 6.531486511230469\n",
            "4021: loss: 6.906115531921387\n",
            "4022: loss: 6.79102897644043\n",
            "4023: loss: 6.6053338050842285\n",
            "4024: loss: 7.111150741577148\n",
            "4025: loss: 6.967381954193115\n",
            "4026: loss: 6.814550876617432\n",
            "4027: loss: 6.7846879959106445\n",
            "4028: loss: 6.952723979949951\n",
            "4029: loss: 6.8677144050598145\n",
            "4030: loss: 6.779841423034668\n",
            "4031: loss: 6.854494094848633\n",
            "4032: loss: 6.805584907531738\n",
            "4033: loss: 6.90664005279541\n",
            "4034: loss: 7.050647735595703\n",
            "4035: loss: 6.882771015167236\n",
            "4036: loss: 7.120101451873779\n",
            "4037: loss: 6.797450065612793\n",
            "4038: loss: 6.626600742340088\n",
            "4039: loss: 6.68807315826416\n",
            "4040: loss: 6.857321262359619\n",
            "4040: valid loss 6.8411784172058105\n",
            "4041: loss: 6.675445079803467\n",
            "4042: loss: 6.958957195281982\n",
            "4043: loss: 6.992563724517822\n",
            "4044: loss: 6.6962056159973145\n",
            "4045: loss: 6.801299571990967\n",
            "4046: loss: 6.917997360229492\n",
            "4047: loss: 6.67826509475708\n",
            "4048: loss: 6.764130592346191\n",
            "4049: loss: 6.8034491539001465\n",
            "4050: loss: 6.549005508422852\n",
            "4051: loss: 6.760354042053223\n",
            "4052: loss: 7.1654558181762695\n",
            "4053: loss: 6.503902435302734\n",
            "4054: loss: 6.61856746673584\n",
            "4055: loss: 6.8702826499938965\n",
            "4056: loss: 6.511443138122559\n",
            "4057: loss: 6.748110771179199\n",
            "4058: loss: 6.6465840339660645\n",
            "4059: loss: 6.546940326690674\n",
            "4060: loss: 6.2923078536987305\n",
            "4060: valid loss 7.010936737060547\n",
            "4061: loss: 6.682488441467285\n",
            "4062: loss: 6.809510231018066\n",
            "4063: loss: 6.327145099639893\n",
            "4064: loss: 6.997117519378662\n",
            "4065: loss: 6.745962142944336\n",
            "4066: loss: 6.886232852935791\n",
            "4067: loss: 6.791264057159424\n",
            "4068: loss: 6.671504020690918\n",
            "4069: loss: 6.294797897338867\n",
            "4070: loss: 6.793198108673096\n",
            "4071: loss: 7.046514987945557\n",
            "4072: loss: 6.477640628814697\n",
            "4073: loss: 6.915389537811279\n",
            "4074: loss: 6.839269638061523\n",
            "4075: loss: 6.761547565460205\n",
            "4076: loss: 6.896556854248047\n",
            "4077: loss: 6.801063537597656\n",
            "4078: loss: 6.88926887512207\n",
            "4079: loss: 6.460743427276611\n",
            "4080: loss: 6.728292465209961\n",
            "4080: valid loss 6.725611209869385\n",
            "4081: loss: 6.804747104644775\n",
            "4082: loss: 6.938674449920654\n",
            "4083: loss: 7.10080099105835\n",
            "4084: loss: 6.702033996582031\n",
            "4085: loss: 7.042566299438477\n",
            "4086: loss: 6.628627300262451\n",
            "4087: loss: 6.699193954467773\n",
            "4088: loss: 7.09512186050415\n",
            "4089: loss: 6.582077980041504\n",
            "4090: loss: 6.864315032958984\n",
            "4091: loss: 6.852992057800293\n",
            "4092: loss: 6.814102649688721\n",
            "4093: loss: 6.784936904907227\n",
            "4094: loss: 7.107539176940918\n",
            "4095: loss: 7.021345138549805\n",
            "4096: loss: 6.506355285644531\n",
            "4097: loss: 6.529379367828369\n",
            "4098: loss: 6.842271327972412\n",
            "4099: loss: 6.789503574371338\n",
            "4100: loss: 6.936681747436523\n",
            "4100: valid loss 7.089872360229492\n",
            "4101: loss: 6.551731109619141\n",
            "4102: loss: 6.568399906158447\n",
            "4103: loss: 7.244647979736328\n",
            "4104: loss: 6.767566204071045\n",
            "4105: loss: 6.834524631500244\n",
            "4106: loss: 6.785973072052002\n",
            "4107: loss: 6.574522495269775\n",
            "4108: loss: 6.85528039932251\n",
            "4109: loss: 6.562160015106201\n",
            "4110: loss: 6.951742172241211\n",
            "4111: loss: 6.762324810028076\n",
            "4112: loss: 6.675704002380371\n",
            "4113: loss: 7.054956912994385\n",
            "4114: loss: 6.787772178649902\n",
            "4115: loss: 6.737216472625732\n",
            "4116: loss: 7.151481628417969\n",
            "4117: loss: 6.41629695892334\n",
            "4118: loss: 6.882315158843994\n",
            "4119: loss: 6.774018287658691\n",
            "4120: loss: 6.824753284454346\n",
            "4120: valid loss 6.910389423370361\n",
            "4121: loss: 6.707614421844482\n",
            "4122: loss: 6.538193225860596\n",
            "4123: loss: 6.867105960845947\n",
            "4124: loss: 6.348430633544922\n",
            "4125: loss: 6.970103740692139\n",
            "4126: loss: 6.6888532638549805\n",
            "4127: loss: 6.660731315612793\n",
            "4128: loss: 7.02048397064209\n",
            "4129: loss: 6.647377967834473\n",
            "4130: loss: 6.907334804534912\n",
            "4131: loss: 6.829606533050537\n",
            "4132: loss: 6.723828315734863\n",
            "4133: loss: 6.828146934509277\n",
            "4134: loss: 6.740244388580322\n",
            "4135: loss: 7.062833786010742\n",
            "4136: loss: 6.886338710784912\n",
            "4137: loss: 6.968876361846924\n",
            "4138: loss: 0.9425266981124878\n",
            "4139: loss: 6.700344562530518\n",
            "4140: loss: 6.710867881774902\n",
            "4140: valid loss 7.142066478729248\n",
            "4141: loss: 7.3477463722229\n",
            "4142: loss: 6.773665428161621\n",
            "4143: loss: 6.7451324462890625\n",
            "4144: loss: 6.580137729644775\n",
            "4145: loss: 6.756962776184082\n",
            "4146: loss: 7.000930309295654\n",
            "4147: loss: 7.000179767608643\n",
            "4148: loss: 6.6739397048950195\n",
            "4149: loss: 6.88054895401001\n",
            "4150: loss: 6.908202648162842\n",
            "4151: loss: 6.592887878417969\n",
            "4152: loss: 6.463330268859863\n",
            "4153: loss: 6.771942138671875\n",
            "4154: loss: 7.161510467529297\n",
            "4155: loss: 6.722618103027344\n",
            "4156: loss: 6.962437152862549\n",
            "4157: loss: 6.798421859741211\n",
            "4158: loss: 7.073675155639648\n",
            "4159: loss: 6.950108528137207\n",
            "4160: loss: 7.143448352813721\n",
            "4160: valid loss 6.761153221130371\n",
            "4161: loss: 6.692984580993652\n",
            "4162: loss: 6.497658729553223\n",
            "4163: loss: 6.787502765655518\n",
            "4164: loss: 6.759934425354004\n",
            "4165: loss: 6.5558061599731445\n",
            "4166: loss: 6.662656784057617\n",
            "4167: loss: 5.0771484375\n",
            "4168: loss: 6.92727518081665\n",
            "4169: loss: 6.5367560386657715\n",
            "4170: loss: 6.971905708312988\n",
            "4171: loss: 6.438361167907715\n",
            "4172: loss: 6.992894649505615\n",
            "4173: loss: 6.952932834625244\n",
            "4174: loss: 6.971194744110107\n",
            "4175: loss: 6.842876434326172\n",
            "4176: loss: 6.53915548324585\n",
            "4177: loss: 6.924406051635742\n",
            "4178: loss: 6.598487377166748\n",
            "4179: loss: 6.877438068389893\n",
            "4180: loss: 6.94228458404541\n",
            "4180: valid loss 7.395477771759033\n",
            "4181: loss: 6.594749927520752\n",
            "4182: loss: 6.753364086151123\n",
            "4183: loss: 7.762170314788818\n",
            "4184: loss: 6.927456855773926\n",
            "4185: loss: 6.6538872718811035\n",
            "4186: loss: 6.645980358123779\n",
            "4187: loss: 6.701937675476074\n",
            "4188: loss: 6.991236209869385\n",
            "4189: loss: 6.87002420425415\n",
            "4190: loss: 6.71566915512085\n",
            "4191: loss: 7.023697853088379\n",
            "4192: loss: 6.984983444213867\n",
            "4193: loss: 6.749091148376465\n",
            "4194: loss: 7.014700412750244\n",
            "4195: loss: 6.457939624786377\n",
            "4196: loss: 6.748476982116699\n",
            "4197: loss: 6.9535956382751465\n",
            "4198: loss: 6.7461066246032715\n",
            "4199: loss: 6.658506870269775\n",
            "4200: loss: 6.808201789855957\n",
            "4200: valid loss 6.590027809143066\n",
            "4200: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "4201: loss: 6.662009239196777\n",
            "4202: loss: 6.809497356414795\n",
            "4203: loss: 6.662446022033691\n",
            "4204: loss: 6.893258094787598\n",
            "4205: loss: 6.761564254760742\n",
            "4206: loss: 6.83328914642334\n",
            "4207: loss: 6.848626136779785\n",
            "4208: loss: 6.570460796356201\n",
            "4209: loss: 6.6852030754089355\n",
            "4210: loss: 7.041158676147461\n",
            "4211: loss: 6.757990837097168\n",
            "4212: loss: 7.2459635734558105\n",
            "4213: loss: 6.817061424255371\n",
            "4214: loss: 6.91742467880249\n",
            "4215: loss: 6.772462368011475\n",
            "4216: loss: 6.610501766204834\n",
            "4217: loss: 7.001637935638428\n",
            "4218: loss: 6.501996040344238\n",
            "4219: loss: 6.919745922088623\n",
            "4220: loss: 6.8789143562316895\n",
            "4220: valid loss 6.881228446960449\n",
            "4221: loss: 6.914496421813965\n",
            "4222: loss: 6.673129558563232\n",
            "4223: loss: 6.811627388000488\n",
            "4224: loss: 6.9085917472839355\n",
            "4225: loss: 6.7348127365112305\n",
            "4226: loss: 6.762982368469238\n",
            "4227: loss: 6.651442527770996\n",
            "4228: loss: 6.586858749389648\n",
            "4229: loss: 6.545348644256592\n",
            "4230: loss: 6.633754253387451\n",
            "4231: loss: 6.978915214538574\n",
            "4232: loss: 6.936062335968018\n",
            "4233: loss: 6.402050971984863\n",
            "4234: loss: 7.1221160888671875\n",
            "4235: loss: 6.708233833312988\n",
            "4236: loss: 6.886586666107178\n",
            "4237: loss: 6.772554874420166\n",
            "4238: loss: 6.902449607849121\n",
            "4239: loss: 6.582629680633545\n",
            "4240: loss: 7.007250785827637\n",
            "4240: valid loss 6.681933879852295\n",
            "4241: loss: 6.79941463470459\n",
            "4242: loss: 6.8137640953063965\n",
            "4243: loss: 6.793855667114258\n",
            "4244: loss: 6.809218883514404\n",
            "4245: loss: 7.116199493408203\n",
            "4246: loss: 6.642375469207764\n",
            "4247: loss: 6.918270587921143\n",
            "4248: loss: 7.148683547973633\n",
            "4249: loss: 6.999050140380859\n",
            "4250: loss: 6.689394950866699\n",
            "4251: loss: 7.271573066711426\n",
            "4252: loss: 6.657446384429932\n",
            "4253: loss: 6.900477409362793\n",
            "4254: loss: 7.054402828216553\n",
            "4255: loss: 6.669670104980469\n",
            "4256: loss: 6.805456638336182\n",
            "4257: loss: 6.799094200134277\n",
            "4258: loss: 6.426677227020264\n",
            "4259: loss: 6.6240010261535645\n",
            "4260: loss: 6.913365364074707\n",
            "4260: valid loss 7.131164073944092\n",
            "4261: loss: 6.698533535003662\n",
            "4262: loss: 6.820341110229492\n",
            "4263: loss: 6.606198310852051\n",
            "4264: loss: 7.0231218338012695\n",
            "4265: loss: 6.573145866394043\n",
            "4266: loss: 6.892043590545654\n",
            "4267: loss: 6.633851528167725\n",
            "4268: loss: 7.033707618713379\n",
            "4269: loss: 6.894179344177246\n",
            "4270: loss: 6.77860689163208\n",
            "4271: loss: 6.522058963775635\n",
            "4272: loss: 7.047246932983398\n",
            "4273: loss: 6.700098037719727\n",
            "4274: loss: 6.641833305358887\n",
            "4275: loss: 6.443426609039307\n",
            "4276: loss: 6.774866104125977\n",
            "4277: loss: 6.559161186218262\n",
            "4278: loss: 7.100231647491455\n",
            "4279: loss: 7.02828311920166\n",
            "4280: loss: 6.662316799163818\n",
            "4280: valid loss 6.717822074890137\n",
            "4281: loss: 7.6069254875183105\n",
            "4282: loss: 6.8015055656433105\n",
            "4283: loss: 6.874809265136719\n",
            "4284: loss: 6.860455513000488\n",
            "4285: loss: 7.364895820617676\n",
            "4286: loss: 6.791407108306885\n",
            "4287: loss: 7.027425289154053\n",
            "4288: loss: 6.822350025177002\n",
            "4289: loss: 7.298248291015625\n",
            "4290: loss: 6.941579818725586\n",
            "4291: loss: 6.571630001068115\n",
            "4292: loss: 6.7308173179626465\n",
            "4293: loss: 6.915839672088623\n",
            "4294: loss: 7.299439430236816\n",
            "4295: loss: 6.978784561157227\n",
            "4296: loss: 6.842897891998291\n",
            "4297: loss: 6.6114349365234375\n",
            "4298: loss: 6.8053717613220215\n",
            "4299: loss: 6.66750431060791\n",
            "4300: loss: 6.560827255249023\n",
            "4300: valid loss 6.942885875701904\n",
            "4301: loss: 6.757696628570557\n",
            "4302: loss: 6.870079517364502\n",
            "4303: loss: 6.791400909423828\n",
            "4304: loss: 6.624554634094238\n",
            "4305: loss: 6.688856601715088\n",
            "4306: loss: 6.445807933807373\n",
            "4307: loss: 6.894016742706299\n",
            "4308: loss: 6.697515487670898\n",
            "4309: loss: 6.620433330535889\n",
            "4310: loss: 6.798248767852783\n",
            "4311: loss: 6.582838535308838\n",
            "4312: loss: 6.7425537109375\n",
            "4313: loss: 6.680004596710205\n",
            "4314: loss: 6.557753562927246\n",
            "4315: loss: 6.857761383056641\n",
            "4316: loss: 6.74040412902832\n",
            "4317: loss: 6.946528434753418\n",
            "4318: loss: 6.738794326782227\n",
            "4319: loss: 6.790688514709473\n",
            "4320: loss: 6.776736736297607\n",
            "4320: valid loss 6.654715538024902\n",
            "4321: loss: 6.810741424560547\n",
            "4322: loss: 6.969146251678467\n",
            "4323: loss: 6.379623889923096\n",
            "4324: loss: 6.646879196166992\n",
            "4325: loss: 7.183415412902832\n",
            "4326: loss: 6.957052707672119\n",
            "4327: loss: 6.407132148742676\n",
            "4328: loss: 6.489654064178467\n",
            "4329: loss: 6.292789459228516\n",
            "4330: loss: 6.934755802154541\n",
            "4331: loss: 7.041560649871826\n",
            "4332: loss: 7.05054235458374\n",
            "4333: loss: 6.9493088722229\n",
            "4334: loss: 6.611935615539551\n",
            "4335: loss: 7.277195930480957\n",
            "4336: loss: 6.384259223937988\n",
            "4337: loss: 7.025676250457764\n",
            "4338: loss: 6.988217830657959\n",
            "4339: loss: 6.7421064376831055\n",
            "4340: loss: 7.06781530380249\n",
            "4340: valid loss 6.747698783874512\n",
            "4341: loss: 6.748809337615967\n",
            "4342: loss: 6.702826499938965\n",
            "4343: loss: 6.9743828773498535\n",
            "4344: loss: 6.552192687988281\n",
            "4345: loss: 6.543412208557129\n",
            "4346: loss: 7.118022918701172\n",
            "4347: loss: 6.746879577636719\n",
            "4348: loss: 6.726180553436279\n",
            "4349: loss: 6.668848037719727\n",
            "4350: loss: 6.808073997497559\n",
            "4351: loss: 7.07836389541626\n",
            "4352: loss: 6.862581253051758\n",
            "4353: loss: 6.882262229919434\n",
            "4354: loss: 6.778280258178711\n",
            "4355: loss: 6.6564459800720215\n",
            "4356: loss: 6.941995143890381\n",
            "4357: loss: 6.977651119232178\n",
            "4358: loss: 7.0106425285339355\n",
            "4359: loss: 6.69627571105957\n",
            "4360: loss: 6.763092041015625\n",
            "4360: valid loss 6.783995628356934\n",
            "4361: loss: 7.067017078399658\n",
            "4362: loss: 6.676988124847412\n",
            "4363: loss: 6.8884663581848145\n",
            "4364: loss: 6.531249523162842\n",
            "4365: loss: 6.635882377624512\n",
            "4366: loss: 6.741778373718262\n",
            "4367: loss: 6.975945472717285\n",
            "4368: loss: 6.788522243499756\n",
            "4369: loss: 6.902769088745117\n",
            "4370: loss: 6.840229034423828\n",
            "4371: loss: 6.952309608459473\n",
            "4372: loss: 6.8339362144470215\n",
            "4373: loss: 6.728639602661133\n",
            "4374: loss: 6.691546440124512\n",
            "4375: loss: 6.68092155456543\n",
            "4376: loss: 6.992537498474121\n",
            "4377: loss: 6.89700174331665\n",
            "4378: loss: 6.537415981292725\n",
            "4379: loss: 6.57670783996582\n",
            "4380: loss: 6.415835857391357\n",
            "4380: valid loss 6.4247965812683105\n",
            "4381: loss: 6.66480827331543\n",
            "4382: loss: 6.76981782913208\n",
            "4383: loss: 7.02242374420166\n",
            "4384: loss: 6.621466636657715\n",
            "4385: loss: 7.141690254211426\n",
            "4386: loss: 6.4993438720703125\n",
            "4387: loss: 6.871840953826904\n",
            "4388: loss: 6.890717029571533\n",
            "4389: loss: 6.665058612823486\n",
            "4390: loss: 7.211127281188965\n",
            "4391: loss: 6.43702507019043\n",
            "4392: loss: 6.752374649047852\n",
            "4393: loss: 2.2317776679992676\n",
            "4394: loss: 7.02141809463501\n",
            "4395: loss: 6.76560640335083\n",
            "4396: loss: 6.8697991371154785\n",
            "4397: loss: 6.821160793304443\n",
            "4398: loss: 6.702361583709717\n",
            "4399: loss: 6.87503719329834\n",
            "4400: loss: 6.811588287353516\n",
            "4400: valid loss 6.848280429840088\n",
            "4400: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "4401: loss: 6.5991034507751465\n",
            "4402: loss: 6.823469161987305\n",
            "4403: loss: 6.661636829376221\n",
            "4404: loss: 6.800570487976074\n",
            "4405: loss: 6.652109146118164\n",
            "4406: loss: 6.882826805114746\n",
            "4407: loss: 6.617372512817383\n",
            "4408: loss: 6.671294689178467\n",
            "4409: loss: 6.747122764587402\n",
            "4410: loss: 6.9820733070373535\n",
            "4411: loss: 6.678167343139648\n",
            "4412: loss: 6.791224956512451\n",
            "4413: loss: 6.886578559875488\n",
            "4414: loss: 7.1169047355651855\n",
            "4415: loss: 6.794658184051514\n",
            "4416: loss: 6.5027008056640625\n",
            "4417: loss: 6.524815082550049\n",
            "4418: loss: 7.034579753875732\n",
            "4419: loss: 6.742129325866699\n",
            "4420: loss: 6.726423263549805\n",
            "4420: valid loss 6.514158725738525\n",
            "4421: loss: 6.780478000640869\n",
            "4422: loss: 6.912410259246826\n",
            "4423: loss: 6.924015045166016\n",
            "4424: loss: 7.006072044372559\n",
            "4425: loss: 6.826226711273193\n",
            "4426: loss: 6.373383045196533\n",
            "4427: loss: 6.783596038818359\n",
            "4428: loss: 6.984145641326904\n",
            "4429: loss: 6.541494369506836\n",
            "4430: loss: 6.389690399169922\n",
            "4431: loss: 6.444899082183838\n",
            "4432: loss: 6.87802267074585\n",
            "4433: loss: 6.904460906982422\n",
            "4434: loss: 6.845183372497559\n",
            "4435: loss: 6.999330520629883\n",
            "4436: loss: 6.957667350769043\n",
            "4437: loss: 6.522699356079102\n",
            "4438: loss: 6.258594989776611\n",
            "4439: loss: 6.662003040313721\n",
            "4440: loss: 6.388301849365234\n",
            "4440: valid loss 6.613553524017334\n",
            "4441: loss: 6.934723854064941\n",
            "4442: loss: 6.6461591720581055\n",
            "4443: loss: 6.874974250793457\n",
            "4444: loss: 6.900300979614258\n",
            "4445: loss: 6.7417378425598145\n",
            "4446: loss: 6.845123291015625\n",
            "4447: loss: 6.5944976806640625\n",
            "4448: loss: 6.946030139923096\n",
            "4449: loss: 6.75588846206665\n",
            "4450: loss: 6.980167388916016\n",
            "4451: loss: 6.685590744018555\n",
            "4452: loss: 6.505842208862305\n",
            "4453: loss: 6.739588737487793\n",
            "4454: loss: 6.643899440765381\n",
            "4455: loss: 7.48103666305542\n",
            "4456: loss: 6.871663570404053\n",
            "4457: loss: 6.568879127502441\n",
            "4458: loss: 6.853560447692871\n",
            "4459: loss: 6.920858860015869\n",
            "4460: loss: 6.65055513381958\n",
            "4460: valid loss 6.498833656311035\n",
            "4461: loss: 6.486518383026123\n",
            "4462: loss: 6.668533802032471\n",
            "4463: loss: 6.526902675628662\n",
            "4464: loss: 6.954410076141357\n",
            "4465: loss: 6.804381847381592\n",
            "4466: loss: 6.91880989074707\n",
            "4467: loss: 7.06436824798584\n",
            "4468: loss: 6.831177234649658\n",
            "4469: loss: 6.743124485015869\n",
            "4470: loss: 6.651604175567627\n",
            "4471: loss: 6.835172653198242\n",
            "4472: loss: 6.6796345710754395\n",
            "4473: loss: 6.637451171875\n",
            "4474: loss: 6.886735916137695\n",
            "4475: loss: 7.199274063110352\n",
            "4476: loss: 6.848047733306885\n",
            "4477: loss: 6.687591075897217\n",
            "4478: loss: 7.09210205078125\n",
            "4479: loss: 6.507163047790527\n",
            "4480: loss: 6.968893051147461\n",
            "4480: valid loss 6.5404767990112305\n",
            "4481: loss: 6.708311080932617\n",
            "4482: loss: 6.720200538635254\n",
            "4483: loss: 7.227620601654053\n",
            "4484: loss: 7.085885047912598\n",
            "4485: loss: 6.7271904945373535\n",
            "4486: loss: 6.782974720001221\n",
            "4487: loss: 6.620647430419922\n",
            "4488: loss: 6.657993793487549\n",
            "4489: loss: 6.81351900100708\n",
            "4490: loss: 6.640132427215576\n",
            "4491: loss: 6.4860100746154785\n",
            "4492: loss: 6.819869518280029\n",
            "4493: loss: 6.991186141967773\n",
            "4494: loss: 9.236567497253418\n",
            "4495: loss: 6.929737091064453\n",
            "4496: loss: 6.8274126052856445\n",
            "4497: loss: 7.104386806488037\n",
            "4498: loss: 6.779213905334473\n",
            "4499: loss: 6.731074333190918\n",
            "4500: loss: 6.889608383178711\n",
            "4500: valid loss 6.79353141784668\n",
            "4501: loss: 6.7512664794921875\n",
            "4502: loss: 6.471845626831055\n",
            "4503: loss: 6.428834915161133\n",
            "4504: loss: 6.938094139099121\n",
            "4505: loss: 6.637848854064941\n",
            "4506: loss: 7.025906085968018\n",
            "4507: loss: 6.85707950592041\n",
            "4508: loss: 6.567050933837891\n",
            "4509: loss: 6.701638221740723\n",
            "4510: loss: 6.459534645080566\n",
            "4511: loss: 7.086085796356201\n",
            "4512: loss: 7.070175647735596\n",
            "4513: loss: 7.147205352783203\n",
            "4514: loss: 7.036111354827881\n",
            "4515: loss: 7.044480800628662\n",
            "4516: loss: 6.586122512817383\n",
            "4517: loss: 6.967900276184082\n",
            "4518: loss: 6.6219377517700195\n",
            "4519: loss: 6.362696647644043\n",
            "4520: loss: 6.954176425933838\n",
            "4520: valid loss 6.6908087730407715\n",
            "4521: loss: 6.903181076049805\n",
            "4522: loss: 6.9127912521362305\n",
            "4523: loss: 6.734134197235107\n",
            "4524: loss: 6.719952583312988\n",
            "4525: loss: 6.682480335235596\n",
            "4526: loss: 6.920902252197266\n",
            "4527: loss: 6.645954132080078\n",
            "4528: loss: 6.6277666091918945\n",
            "4529: loss: 6.459202289581299\n",
            "4530: loss: 6.619430065155029\n",
            "4531: loss: 6.394070148468018\n",
            "4532: loss: 6.746662616729736\n",
            "4533: loss: 6.935709476470947\n",
            "4534: loss: 6.481764793395996\n",
            "4535: loss: 6.5411272048950195\n",
            "4536: loss: 7.115558624267578\n",
            "4537: loss: 6.588624477386475\n",
            "4538: loss: 6.546571254730225\n",
            "4539: loss: 6.891543865203857\n",
            "4540: loss: 6.53265380859375\n",
            "4540: valid loss 7.027067184448242\n",
            "4541: loss: 6.905117034912109\n",
            "4542: loss: 6.690711975097656\n",
            "4543: loss: 6.668393611907959\n",
            "4544: loss: 7.071919918060303\n",
            "4545: loss: 6.760149955749512\n",
            "4546: loss: 6.453158855438232\n",
            "4547: loss: 6.768651485443115\n",
            "4548: loss: 6.779228210449219\n",
            "4549: loss: 6.5517449378967285\n",
            "4550: loss: 6.596724987030029\n",
            "4551: loss: 6.594545841217041\n",
            "4552: loss: 6.760374069213867\n",
            "4553: loss: 6.653998374938965\n",
            "4554: loss: 6.794346332550049\n",
            "4555: loss: 6.762199878692627\n",
            "4556: loss: 6.79874324798584\n",
            "4557: loss: 6.904733657836914\n",
            "4558: loss: 6.708682537078857\n",
            "4559: loss: 6.825501441955566\n",
            "4560: loss: 6.307250499725342\n",
            "4560: valid loss 6.769015312194824\n",
            "4561: loss: 6.804434299468994\n",
            "4562: loss: 6.702063083648682\n",
            "4563: loss: 6.59089994430542\n",
            "4564: loss: 6.848232746124268\n",
            "4565: loss: 6.470917224884033\n",
            "4566: loss: 6.460155963897705\n",
            "4567: loss: 6.646087169647217\n",
            "4568: loss: 6.440071105957031\n",
            "4569: loss: 6.574560165405273\n",
            "4570: loss: 6.68861198425293\n",
            "4571: loss: 6.197909832000732\n",
            "4572: loss: 7.035701274871826\n",
            "4573: loss: 6.5119309425354\n",
            "4574: loss: 6.856417179107666\n",
            "4575: loss: 7.049527645111084\n",
            "4576: loss: 6.6679840087890625\n",
            "4577: loss: 6.5543951988220215\n",
            "4578: loss: 6.44719123840332\n",
            "4579: loss: 6.574648380279541\n",
            "4580: loss: 6.546751499176025\n",
            "4580: valid loss 6.338235378265381\n",
            "4581: loss: 6.5909552574157715\n",
            "4582: loss: 6.8976263999938965\n",
            "4583: loss: 6.6024556159973145\n",
            "4584: loss: 6.563711166381836\n",
            "4585: loss: 6.710041046142578\n",
            "4586: loss: 6.877692222595215\n",
            "4587: loss: 6.612646579742432\n",
            "4588: loss: 6.677670478820801\n",
            "4589: loss: 6.461186408996582\n",
            "4590: loss: 6.470780372619629\n",
            "4591: loss: 6.722482204437256\n",
            "4592: loss: 6.550753593444824\n",
            "4593: loss: 6.3866753578186035\n",
            "4594: loss: 6.6955766677856445\n",
            "4595: loss: 6.5691142082214355\n",
            "4596: loss: 6.9431610107421875\n",
            "4597: loss: 6.596059322357178\n",
            "4598: loss: 6.9460129737854\n",
            "4599: loss: 6.561639785766602\n",
            "4600: loss: 6.497183322906494\n",
            "4600: valid loss 6.426706314086914\n",
            "4600: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "4601: loss: 6.721200942993164\n",
            "4602: loss: 6.776918888092041\n",
            "4603: loss: 7.104691982269287\n",
            "4604: loss: 7.254298686981201\n",
            "4605: loss: 6.594457149505615\n",
            "4606: loss: 6.8758225440979\n",
            "4607: loss: 6.820493221282959\n",
            "4608: loss: 6.72593355178833\n",
            "4609: loss: 6.408139705657959\n",
            "4610: loss: 6.82698392868042\n",
            "4611: loss: 6.583859920501709\n",
            "4612: loss: 6.7521257400512695\n",
            "4613: loss: 7.21010160446167\n",
            "4614: loss: 6.776124000549316\n",
            "4615: loss: 6.823257923126221\n",
            "4616: loss: 7.072338104248047\n",
            "4617: loss: 6.541589260101318\n",
            "4618: loss: 7.0600152015686035\n",
            "4619: loss: 6.732777118682861\n",
            "4620: loss: 6.999265670776367\n",
            "4620: valid loss 6.764115333557129\n",
            "4621: loss: 6.824888229370117\n",
            "4622: loss: 6.625421047210693\n",
            "4623: loss: 6.230131149291992\n",
            "4624: loss: 6.3606414794921875\n",
            "4625: loss: 6.735762119293213\n",
            "4626: loss: 6.39310884475708\n",
            "4627: loss: 6.736469268798828\n",
            "4628: loss: 6.638898849487305\n",
            "4629: loss: 6.718447685241699\n",
            "4630: loss: 7.018195629119873\n",
            "4631: loss: 6.435886859893799\n",
            "4632: loss: 6.653473377227783\n",
            "4633: loss: 6.500055313110352\n",
            "4634: loss: 6.780139923095703\n",
            "4635: loss: 6.660028457641602\n",
            "4636: loss: 6.425899028778076\n",
            "4637: loss: 6.899573802947998\n",
            "4638: loss: 6.501272201538086\n",
            "4639: loss: 6.808887004852295\n",
            "4640: loss: 6.906826972961426\n",
            "4640: valid loss 6.874290466308594\n",
            "4641: loss: 6.7583465576171875\n",
            "4642: loss: 6.9967360496521\n",
            "4643: loss: 6.94142484664917\n",
            "4644: loss: 6.520098686218262\n",
            "4645: loss: 7.2153849601745605\n",
            "4646: loss: 6.8615288734436035\n",
            "4647: loss: 6.70808219909668\n",
            "4648: loss: 6.617919445037842\n",
            "4649: loss: 6.355129718780518\n",
            "4650: loss: 6.836007595062256\n",
            "4651: loss: 6.6619720458984375\n",
            "4652: loss: 6.358181953430176\n",
            "4653: loss: 6.99566650390625\n",
            "4654: loss: 7.129241466522217\n",
            "4655: loss: 6.365904808044434\n",
            "4656: loss: 6.684817314147949\n",
            "4657: loss: 7.043702125549316\n",
            "4658: loss: 6.704710960388184\n",
            "4659: loss: 6.751646041870117\n",
            "4660: loss: 6.459190368652344\n",
            "4660: valid loss 6.81563663482666\n",
            "4661: loss: 6.6395463943481445\n",
            "4662: loss: 6.64402437210083\n",
            "4663: loss: 6.9495134353637695\n",
            "4664: loss: 6.988468647003174\n",
            "4665: loss: 6.792238712310791\n",
            "4666: loss: 6.890923023223877\n",
            "4667: loss: 6.542093276977539\n",
            "4668: loss: 6.694111347198486\n",
            "4669: loss: 6.634185314178467\n",
            "4670: loss: 6.865384578704834\n",
            "4671: loss: 6.603633880615234\n",
            "4672: loss: 6.852845191955566\n",
            "4673: loss: 6.609664440155029\n",
            "4674: loss: 7.767367839813232\n",
            "4675: loss: 6.923925399780273\n",
            "4676: loss: 7.023900032043457\n",
            "4677: loss: 6.55067253112793\n",
            "4678: loss: 6.498635292053223\n",
            "4679: loss: 6.901448726654053\n",
            "4680: loss: 7.306674003601074\n",
            "4680: valid loss 6.741909980773926\n",
            "4681: loss: 6.675769329071045\n",
            "4682: loss: 6.420131683349609\n",
            "4683: loss: 6.869709014892578\n",
            "4684: loss: 6.760502815246582\n",
            "4685: loss: 6.925329685211182\n",
            "4686: loss: 7.163250923156738\n",
            "4687: loss: 6.651788234710693\n",
            "4688: loss: 6.801494121551514\n",
            "4689: loss: 6.586550235748291\n",
            "4690: loss: 6.493243217468262\n",
            "4691: loss: 6.783908367156982\n",
            "4692: loss: 6.9299821853637695\n",
            "4693: loss: 6.9460248947143555\n",
            "4694: loss: 6.536726474761963\n",
            "4695: loss: 6.585798263549805\n",
            "4696: loss: 6.562017917633057\n",
            "4697: loss: 6.933189392089844\n",
            "4698: loss: 6.698757648468018\n",
            "4699: loss: 6.960206508636475\n",
            "4700: loss: 6.417636394500732\n",
            "4700: valid loss 7.1659464836120605\n",
            "4701: loss: 6.808101177215576\n",
            "4702: loss: 7.052011489868164\n",
            "4703: loss: 6.92282772064209\n",
            "4704: loss: 7.0843706130981445\n",
            "4705: loss: 6.6834516525268555\n",
            "4706: loss: 6.995501518249512\n",
            "4707: loss: 6.67587423324585\n",
            "4708: loss: 6.872666358947754\n",
            "4709: loss: 6.43781852722168\n",
            "4710: loss: 6.665141582489014\n",
            "4711: loss: 6.852161884307861\n",
            "4712: loss: 6.791640281677246\n",
            "4713: loss: 6.4400410652160645\n",
            "4714: loss: 6.887794017791748\n",
            "4715: loss: 6.616034030914307\n",
            "4716: loss: 6.880450248718262\n",
            "4717: loss: 6.695028781890869\n",
            "4718: loss: 6.746542930603027\n",
            "4719: loss: 6.991034984588623\n",
            "4720: loss: 6.7586870193481445\n",
            "4720: valid loss 6.924241065979004\n",
            "4721: loss: 6.8120503425598145\n",
            "4722: loss: 6.636805057525635\n",
            "4723: loss: 7.178213596343994\n",
            "4724: loss: 6.735262870788574\n",
            "4725: loss: 6.48086404800415\n",
            "4726: loss: 6.978394031524658\n",
            "4727: loss: 6.61676549911499\n",
            "4728: loss: 6.620331764221191\n",
            "4729: loss: 6.625510215759277\n",
            "4730: loss: 6.726166248321533\n",
            "4731: loss: 6.801788806915283\n",
            "4732: loss: 6.987879753112793\n",
            "4733: loss: 6.794105052947998\n",
            "4734: loss: 6.785667896270752\n",
            "4735: loss: 13.418510437011719\n",
            "4736: loss: 6.599776268005371\n",
            "4737: loss: 6.901650428771973\n",
            "4738: loss: 6.549062728881836\n",
            "4739: loss: 6.77899694442749\n",
            "4740: loss: 6.778814315795898\n",
            "4740: valid loss 6.596332550048828\n",
            "4741: loss: 7.1895623207092285\n",
            "4742: loss: 6.7601237297058105\n",
            "4743: loss: 6.58469820022583\n",
            "4744: loss: 7.003042221069336\n",
            "4745: loss: 6.961287021636963\n",
            "4746: loss: 6.569060325622559\n",
            "4747: loss: 6.333339214324951\n",
            "4748: loss: 6.543205738067627\n",
            "4749: loss: 6.6686882972717285\n",
            "4750: loss: 6.84638786315918\n",
            "4751: loss: 7.174954414367676\n",
            "4752: loss: 6.759275913238525\n",
            "4753: loss: 6.479250907897949\n",
            "4754: loss: 6.6884541511535645\n",
            "4755: loss: 6.544815540313721\n",
            "4756: loss: 6.916341304779053\n",
            "4757: loss: 6.920775413513184\n",
            "4758: loss: 6.530837535858154\n",
            "4759: loss: 7.220282077789307\n",
            "4760: loss: 6.946741580963135\n",
            "4760: valid loss 6.836057186126709\n",
            "4761: loss: 6.595075607299805\n",
            "4762: loss: 6.6294732093811035\n",
            "4763: loss: 7.08257532119751\n",
            "4764: loss: 6.718443393707275\n",
            "4765: loss: 6.344974517822266\n",
            "4766: loss: 6.873921871185303\n",
            "4767: loss: 6.8849029541015625\n",
            "4768: loss: 6.750207424163818\n",
            "4769: loss: 6.7154765129089355\n",
            "4770: loss: 6.447741508483887\n",
            "4771: loss: 6.657698154449463\n",
            "4772: loss: 6.767353057861328\n",
            "4773: loss: 7.073000907897949\n",
            "4774: loss: 6.9182963371276855\n",
            "4775: loss: 7.104013442993164\n",
            "4776: loss: 6.631918430328369\n",
            "4777: loss: 6.839312553405762\n",
            "4778: loss: 6.855942726135254\n",
            "4779: loss: 6.704291820526123\n",
            "4780: loss: 6.637364864349365\n",
            "4780: valid loss 6.045124530792236\n",
            "4781: loss: 6.440892219543457\n",
            "4782: loss: 6.538412570953369\n",
            "4783: loss: 6.4882001876831055\n",
            "4784: loss: 6.431705474853516\n",
            "4785: loss: 7.106496334075928\n",
            "4786: loss: 6.967301845550537\n",
            "4787: loss: 6.964295387268066\n",
            "4788: loss: 6.6649041175842285\n",
            "4789: loss: 6.663794040679932\n",
            "4790: loss: 7.691154956817627\n",
            "4791: loss: 6.473783016204834\n",
            "4792: loss: 6.767448902130127\n",
            "4793: loss: 6.612288475036621\n",
            "4794: loss: 6.862464904785156\n",
            "4795: loss: 6.6320061683654785\n",
            "4796: loss: 6.683952808380127\n",
            "4797: loss: 6.563337802886963\n",
            "4798: loss: 6.9203972816467285\n",
            "4799: loss: 6.704505920410156\n",
            "4800: loss: 7.0877366065979\n",
            "4800: valid loss 6.80736780166626\n",
            "4800: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "4801: loss: 6.6312255859375\n",
            "4802: loss: 6.340293884277344\n",
            "4803: loss: 6.9386796951293945\n",
            "4804: loss: 6.47395658493042\n",
            "4805: loss: 6.574141502380371\n",
            "4806: loss: 6.752249240875244\n",
            "4807: loss: 6.510615348815918\n",
            "4808: loss: 6.837955474853516\n",
            "4809: loss: 6.91948938369751\n",
            "4810: loss: 6.675642013549805\n",
            "4811: loss: 7.133700370788574\n",
            "4812: loss: 6.643172740936279\n",
            "4813: loss: 6.732461452484131\n",
            "4814: loss: 6.850038528442383\n",
            "4815: loss: 6.605887413024902\n",
            "4816: loss: 6.485204219818115\n",
            "4817: loss: 6.750521183013916\n",
            "4818: loss: 7.078150272369385\n",
            "4819: loss: 6.703779697418213\n",
            "4820: loss: 6.764228820800781\n",
            "4820: valid loss 6.520115852355957\n",
            "4821: loss: 6.508179664611816\n",
            "4822: loss: 6.4845099449157715\n",
            "4823: loss: 6.8853349685668945\n",
            "4824: loss: 6.607685565948486\n",
            "4825: loss: 6.5072760581970215\n",
            "4826: loss: 6.726640224456787\n",
            "4827: loss: 6.415469646453857\n",
            "4828: loss: 6.844720363616943\n",
            "4829: loss: 6.517938613891602\n",
            "4830: loss: 6.431405067443848\n",
            "4831: loss: 6.663450241088867\n",
            "4832: loss: 6.937692165374756\n",
            "4833: loss: 6.4727630615234375\n",
            "4834: loss: 6.604907989501953\n",
            "4835: loss: 6.887781143188477\n",
            "4836: loss: 6.883558750152588\n",
            "4837: loss: 6.75430965423584\n",
            "4838: loss: 6.550496578216553\n",
            "4839: loss: 6.402264595031738\n",
            "4840: loss: 6.9105143547058105\n",
            "4840: valid loss 6.674375057220459\n",
            "4841: loss: 6.608065128326416\n",
            "4842: loss: 6.63503360748291\n",
            "4843: loss: 6.772731781005859\n",
            "4844: loss: 6.848334312438965\n",
            "4845: loss: 6.291025161743164\n",
            "4846: loss: 6.980215072631836\n",
            "4847: loss: 6.749847412109375\n",
            "4848: loss: 6.730369567871094\n",
            "4849: loss: 6.515356540679932\n",
            "4850: loss: 6.775017738342285\n",
            "4851: loss: 6.486643314361572\n",
            "4852: loss: 6.954626083374023\n",
            "4853: loss: 6.601626873016357\n",
            "4854: loss: 7.016114711761475\n",
            "4855: loss: 6.855790138244629\n",
            "4856: loss: 6.958715438842773\n",
            "4857: loss: 6.756296634674072\n",
            "4858: loss: 6.736813068389893\n",
            "4859: loss: 6.728232383728027\n",
            "4860: loss: 6.400875091552734\n",
            "4860: valid loss 6.835933208465576\n",
            "4861: loss: 6.826147556304932\n",
            "4862: loss: 6.67466402053833\n",
            "4863: loss: 6.4690704345703125\n",
            "4864: loss: 6.8365607261657715\n",
            "4865: loss: 6.574469566345215\n",
            "4866: loss: 6.218678951263428\n",
            "4867: loss: 7.095743179321289\n",
            "4868: loss: 6.756833076477051\n",
            "4869: loss: 6.299597263336182\n",
            "4870: loss: 6.653754711151123\n",
            "4871: loss: 6.6515302658081055\n",
            "4872: loss: 6.8886613845825195\n",
            "4873: loss: 6.664858341217041\n",
            "4874: loss: 6.865493297576904\n",
            "4875: loss: 6.142504692077637\n",
            "4876: loss: 6.8683037757873535\n",
            "4877: loss: 6.771198749542236\n",
            "4878: loss: 6.7194905281066895\n",
            "4879: loss: 6.557734966278076\n",
            "4880: loss: 6.503927707672119\n",
            "4880: valid loss 6.626079082489014\n",
            "4881: loss: 6.817419528961182\n",
            "4882: loss: 7.021265983581543\n",
            "4883: loss: 6.899507522583008\n",
            "4884: loss: 6.946998596191406\n",
            "4885: loss: 6.509564399719238\n",
            "4886: loss: 6.676177024841309\n",
            "4887: loss: 6.446145534515381\n",
            "4888: loss: 6.683732032775879\n",
            "4889: loss: 6.677587032318115\n",
            "4890: loss: 6.801252365112305\n",
            "4891: loss: 6.177584171295166\n",
            "4892: loss: 6.790166854858398\n",
            "4893: loss: 6.990874767303467\n",
            "4894: loss: 6.786719799041748\n",
            "4895: loss: 6.580521106719971\n",
            "4896: loss: 6.7048211097717285\n",
            "4897: loss: 6.657858371734619\n",
            "4898: loss: 6.2147674560546875\n",
            "4899: loss: 7.152456283569336\n",
            "4900: loss: 6.60015869140625\n",
            "4900: valid loss 6.609279155731201\n",
            "4901: loss: 7.04111909866333\n",
            "4902: loss: 7.073347568511963\n",
            "4903: loss: 6.5696516036987305\n",
            "4904: loss: 6.941633224487305\n",
            "4905: loss: 6.523096084594727\n",
            "4906: loss: 6.466716766357422\n",
            "4907: loss: 6.668031692504883\n",
            "4908: loss: 6.5893168449401855\n",
            "4909: loss: 6.585147857666016\n",
            "4910: loss: 6.780003547668457\n",
            "4911: loss: 6.390275955200195\n",
            "4912: loss: 6.741199016571045\n",
            "4913: loss: 6.517285346984863\n",
            "4914: loss: 6.723189830780029\n",
            "4915: loss: 6.438109874725342\n",
            "4916: loss: 6.806737422943115\n",
            "4917: loss: 6.5254058837890625\n",
            "4918: loss: 6.706489562988281\n",
            "4919: loss: 6.570889472961426\n",
            "4920: loss: 6.829361915588379\n",
            "4920: valid loss 6.62156867980957\n",
            "4921: loss: 6.628956317901611\n",
            "4922: loss: 6.497267723083496\n",
            "4923: loss: 6.659397125244141\n",
            "4924: loss: 6.871424674987793\n",
            "4925: loss: 6.787787437438965\n",
            "4926: loss: 7.1397013664245605\n",
            "4927: loss: 7.515032768249512\n",
            "4928: loss: 6.812862396240234\n",
            "4929: loss: 6.995490550994873\n",
            "4930: loss: 6.666849136352539\n",
            "4931: loss: 6.490841865539551\n",
            "4932: loss: 6.910398006439209\n",
            "4933: loss: 6.5724101066589355\n",
            "4934: loss: 6.2529683113098145\n",
            "4935: loss: 6.9560136795043945\n",
            "4936: loss: 6.435339450836182\n",
            "4937: loss: 6.8902363777160645\n",
            "4938: loss: 7.131267547607422\n",
            "4939: loss: 6.965568542480469\n",
            "4940: loss: 6.718545913696289\n",
            "4940: valid loss 6.9008660316467285\n",
            "4941: loss: 6.927046298980713\n",
            "4942: loss: 6.801630020141602\n",
            "4943: loss: 6.904793739318848\n",
            "4944: loss: 6.481536388397217\n",
            "4945: loss: 6.748801231384277\n",
            "4946: loss: 6.314026355743408\n",
            "4947: loss: 6.782531261444092\n",
            "4948: loss: 6.851171493530273\n",
            "4949: loss: 6.910548686981201\n",
            "4950: loss: 6.672290802001953\n",
            "4951: loss: 6.701066017150879\n",
            "4952: loss: 6.4178900718688965\n",
            "4953: loss: 6.68247652053833\n",
            "4954: loss: 6.748642921447754\n",
            "4955: loss: 6.8975629806518555\n",
            "4956: loss: 6.511604309082031\n",
            "4957: loss: 6.453685283660889\n",
            "4958: loss: 6.658998012542725\n",
            "4959: loss: 6.75747013092041\n",
            "4960: loss: 7.373378753662109\n",
            "4960: valid loss 6.836616516113281\n",
            "4961: loss: 6.606063365936279\n",
            "4962: loss: 6.741784572601318\n",
            "4963: loss: 6.517258644104004\n",
            "4964: loss: 6.808067798614502\n",
            "4965: loss: 6.570704936981201\n",
            "4966: loss: 6.2600812911987305\n",
            "4967: loss: 6.487702369689941\n",
            "4968: loss: 6.406746864318848\n",
            "4969: loss: 6.5710906982421875\n",
            "4970: loss: 6.487102031707764\n",
            "4971: loss: 7.03427267074585\n",
            "4972: loss: 6.892874717712402\n",
            "4973: loss: 6.804063320159912\n",
            "4974: loss: 6.555309295654297\n",
            "4975: loss: 7.065281391143799\n",
            "4976: loss: 6.413721561431885\n",
            "4977: loss: 6.526016712188721\n",
            "4978: loss: 6.813206672668457\n",
            "4979: loss: 7.31043004989624\n",
            "4980: loss: 6.772752285003662\n",
            "4980: valid loss 6.671903610229492\n",
            "4981: loss: 6.2714972496032715\n",
            "4982: loss: 6.829064846038818\n",
            "4983: loss: 6.641623020172119\n",
            "4984: loss: 7.100131988525391\n",
            "4985: loss: 6.539487838745117\n",
            "4986: loss: 6.381836891174316\n",
            "4987: loss: 6.631710052490234\n",
            "4988: loss: 6.759963512420654\n",
            "4989: loss: 6.8556437492370605\n",
            "4990: loss: 6.498791694641113\n",
            "4991: loss: 6.61202335357666\n",
            "4992: loss: 6.903077602386475\n",
            "4993: loss: 6.8401570320129395\n",
            "4994: loss: 6.411666393280029\n",
            "4995: loss: 6.796053886413574\n",
            "4996: loss: 6.607926368713379\n",
            "4997: loss: 6.828364849090576\n",
            "4998: loss: 6.74436616897583\n",
            "4999: loss: 6.775954246520996\n",
            "5000: loss: 6.789774417877197\n",
            "5000: valid loss 6.853142261505127\n",
            "5000: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "5001: loss: 6.47014045715332\n",
            "5002: loss: 6.84617280960083\n",
            "5003: loss: 6.665431976318359\n",
            "5004: loss: 6.68053674697876\n",
            "5005: loss: 6.609043121337891\n",
            "5006: loss: 6.525270462036133\n",
            "5007: loss: 6.819635391235352\n",
            "5008: loss: 6.819339275360107\n",
            "5009: loss: 6.5559186935424805\n",
            "5010: loss: 6.361151218414307\n",
            "5011: loss: 6.589685440063477\n",
            "5012: loss: 6.442953109741211\n",
            "5013: loss: 6.794523239135742\n",
            "5014: loss: 6.791133880615234\n",
            "5015: loss: 6.826935768127441\n",
            "5016: loss: 6.365179538726807\n",
            "5017: loss: 7.098605632781982\n",
            "5018: loss: 6.911895275115967\n",
            "5019: loss: 6.972655296325684\n",
            "5020: loss: 6.526803493499756\n",
            "5020: valid loss 6.6921515464782715\n",
            "5021: loss: 6.568559646606445\n",
            "5022: loss: 6.5255866050720215\n",
            "5023: loss: 6.48668909072876\n",
            "5024: loss: 6.852413177490234\n",
            "5025: loss: 6.825338363647461\n",
            "5026: loss: 6.822153091430664\n",
            "5027: loss: 6.856018543243408\n",
            "5028: loss: 6.533202171325684\n",
            "5029: loss: 6.6326422691345215\n",
            "5030: loss: 6.800032138824463\n",
            "5031: loss: 6.604821681976318\n",
            "5032: loss: 6.485538005828857\n",
            "5033: loss: 6.757323265075684\n",
            "5034: loss: 6.3610100746154785\n",
            "5035: loss: 6.541471481323242\n",
            "5036: loss: 6.545521259307861\n",
            "5037: loss: 6.992332935333252\n",
            "5038: loss: 6.731472969055176\n",
            "5039: loss: 6.508390426635742\n",
            "5040: loss: 6.522955417633057\n",
            "5040: valid loss 6.8436102867126465\n",
            "5041: loss: 6.424526691436768\n",
            "5042: loss: 6.607950687408447\n",
            "5043: loss: 6.32496452331543\n",
            "5044: loss: 6.589059352874756\n",
            "5045: loss: 6.03372859954834\n",
            "5046: loss: 6.615217685699463\n",
            "5047: loss: 6.801825046539307\n",
            "5048: loss: 6.602020740509033\n",
            "5049: loss: 6.503945827484131\n",
            "5050: loss: 6.870009899139404\n",
            "5051: loss: 6.189228534698486\n",
            "5052: loss: 7.036393165588379\n",
            "5053: loss: 6.7187819480896\n",
            "5054: loss: 6.760443687438965\n",
            "5055: loss: 7.05597448348999\n",
            "5056: loss: 6.803503036499023\n",
            "5057: loss: 6.243571758270264\n",
            "5058: loss: 6.775080680847168\n",
            "5059: loss: 6.793370246887207\n",
            "5060: loss: 6.778953552246094\n",
            "5060: valid loss 6.768107891082764\n",
            "5061: loss: 6.744938373565674\n",
            "5062: loss: 6.459838390350342\n",
            "5063: loss: 6.611371040344238\n",
            "5064: loss: 6.660345554351807\n",
            "5065: loss: 6.896183490753174\n",
            "5066: loss: 6.336656093597412\n",
            "5067: loss: 6.631832599639893\n",
            "5068: loss: 6.495088577270508\n",
            "5069: loss: 6.801436424255371\n",
            "5070: loss: 6.543579578399658\n",
            "5071: loss: 6.6745829582214355\n",
            "5072: loss: 6.623943328857422\n",
            "5073: loss: 6.604602336883545\n",
            "5074: loss: 7.017871379852295\n",
            "5075: loss: 6.608450412750244\n",
            "5076: loss: 6.81022310256958\n",
            "5077: loss: 6.3464789390563965\n",
            "5078: loss: 6.8269124031066895\n",
            "5079: loss: 6.8556060791015625\n",
            "5080: loss: 6.534672260284424\n",
            "5080: valid loss 6.442623615264893\n",
            "5081: loss: 6.273243427276611\n",
            "5082: loss: 6.815004348754883\n",
            "5083: loss: 6.976129531860352\n",
            "5084: loss: 6.398952960968018\n",
            "5085: loss: 6.980968952178955\n",
            "5086: loss: 6.39356803894043\n",
            "5087: loss: 6.431873798370361\n",
            "5088: loss: 7.03502893447876\n",
            "5089: loss: 6.678576469421387\n",
            "5090: loss: 6.382285118103027\n",
            "5091: loss: 6.463667392730713\n",
            "5092: loss: 6.376680850982666\n",
            "5093: loss: 6.360544681549072\n",
            "5094: loss: 6.518336296081543\n",
            "5095: loss: 6.729475498199463\n",
            "5096: loss: 6.59662389755249\n",
            "5097: loss: 6.755796432495117\n",
            "5098: loss: 6.540472984313965\n",
            "5099: loss: 6.622720241546631\n",
            "5100: loss: 6.911984920501709\n",
            "5100: valid loss 6.705018520355225\n",
            "5101: loss: 6.903076171875\n",
            "5102: loss: 6.485036373138428\n",
            "5103: loss: 6.6744160652160645\n",
            "5104: loss: 6.784703254699707\n",
            "5105: loss: 6.626352787017822\n",
            "5106: loss: 6.713265419006348\n",
            "5107: loss: 6.665191173553467\n",
            "5108: loss: 6.927751541137695\n",
            "5109: loss: 6.5276103019714355\n",
            "5110: loss: 6.504143714904785\n",
            "5111: loss: 6.5426025390625\n",
            "5112: loss: 6.712332725524902\n",
            "5113: loss: 6.563450813293457\n",
            "5114: loss: 6.731741428375244\n",
            "5115: loss: 6.501323223114014\n",
            "5116: loss: 6.317494869232178\n",
            "5117: loss: 6.619531154632568\n",
            "5118: loss: 6.305670261383057\n",
            "5119: loss: 6.507443428039551\n",
            "5120: loss: 6.454136371612549\n",
            "5120: valid loss 6.531087398529053\n",
            "5121: loss: 6.586828231811523\n",
            "5122: loss: 6.953702926635742\n",
            "5123: loss: 7.13268518447876\n",
            "5124: loss: 6.544545650482178\n",
            "5125: loss: 6.411745071411133\n",
            "5126: loss: 6.892151355743408\n",
            "5127: loss: 6.852481842041016\n",
            "5128: loss: 6.588026523590088\n",
            "5129: loss: 6.698861122131348\n",
            "5130: loss: 7.069947719573975\n",
            "5131: loss: 6.590967655181885\n",
            "5132: loss: 6.772840976715088\n",
            "5133: loss: 6.971395492553711\n",
            "5134: loss: 6.574482440948486\n",
            "5135: loss: 6.606587886810303\n",
            "5136: loss: 6.626802921295166\n",
            "5137: loss: 6.640875339508057\n",
            "5138: loss: 6.779840469360352\n",
            "5139: loss: 6.705728054046631\n",
            "5140: loss: 6.860218524932861\n",
            "5140: valid loss 6.8937296867370605\n",
            "5141: loss: 6.45786190032959\n",
            "5142: loss: 6.8019304275512695\n",
            "5143: loss: 6.764651298522949\n",
            "5144: loss: 6.619889259338379\n",
            "5145: loss: 6.640811443328857\n",
            "5146: loss: 6.588008403778076\n",
            "5147: loss: 6.761112689971924\n",
            "5148: loss: 7.028800964355469\n",
            "5149: loss: 6.781020164489746\n",
            "5150: loss: 6.689699172973633\n",
            "5151: loss: 6.916714668273926\n",
            "5152: loss: 6.5129475593566895\n",
            "5153: loss: 6.923064231872559\n",
            "5154: loss: 6.506776809692383\n",
            "5155: loss: 6.826242923736572\n",
            "5156: loss: 6.6977458000183105\n",
            "5157: loss: 6.46151876449585\n",
            "5158: loss: 6.555414199829102\n",
            "5159: loss: 6.474504470825195\n",
            "5160: loss: 6.285290241241455\n",
            "5160: valid loss 6.6473917961120605\n",
            "5161: loss: 6.594315052032471\n",
            "5162: loss: 6.404730796813965\n",
            "5163: loss: 6.87269926071167\n",
            "5164: loss: 6.644174098968506\n",
            "5165: loss: 6.715152263641357\n",
            "5166: loss: 6.902207374572754\n",
            "5167: loss: 6.724716663360596\n",
            "5168: loss: 6.571909427642822\n",
            "5169: loss: 6.530429840087891\n",
            "5170: loss: 6.691691875457764\n",
            "5171: loss: 6.404115200042725\n",
            "5172: loss: 6.984462261199951\n",
            "5173: loss: 6.226382255554199\n",
            "5174: loss: 6.847293376922607\n",
            "5175: loss: 6.547885417938232\n",
            "5176: loss: 6.610494136810303\n",
            "5177: loss: 6.662208080291748\n",
            "5178: loss: 6.836835861206055\n",
            "5179: loss: 6.645770072937012\n",
            "5180: loss: 6.828254222869873\n",
            "5180: valid loss 6.071537494659424\n",
            "5181: loss: 6.603775978088379\n",
            "5182: loss: 6.754715919494629\n",
            "5183: loss: 6.633095741271973\n",
            "5184: loss: 6.558403968811035\n",
            "5185: loss: 6.963541507720947\n",
            "5186: loss: 6.693432331085205\n",
            "5187: loss: 6.703763961791992\n",
            "5188: loss: 6.558385372161865\n",
            "5189: loss: 5.9567413330078125\n",
            "5190: loss: 6.8149733543396\n",
            "5191: loss: 6.804379463195801\n",
            "5192: loss: 6.291360378265381\n",
            "5193: loss: 6.6974663734436035\n",
            "5194: loss: 6.44253396987915\n",
            "5195: loss: 6.7022905349731445\n",
            "5196: loss: 6.30062198638916\n",
            "5197: loss: 6.938088417053223\n",
            "5198: loss: 6.938904762268066\n",
            "5199: loss: 6.41424036026001\n",
            "5200: loss: 5.661317825317383\n",
            "5200: valid loss 6.356943607330322\n",
            "5200: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "5201: loss: 6.39188814163208\n",
            "5202: loss: 6.634317874908447\n",
            "5203: loss: 6.777466773986816\n",
            "5204: loss: 6.955781936645508\n",
            "5205: loss: 6.080604076385498\n",
            "5206: loss: 6.605161666870117\n",
            "5207: loss: 6.616678714752197\n",
            "5208: loss: 6.836102485656738\n",
            "5209: loss: 6.968903541564941\n",
            "5210: loss: 6.447634696960449\n",
            "5211: loss: 6.633366584777832\n",
            "5212: loss: 6.964217185974121\n",
            "5213: loss: 6.733798503875732\n",
            "5214: loss: 6.489255428314209\n",
            "5215: loss: 6.603607654571533\n",
            "5216: loss: 6.662105560302734\n",
            "5217: loss: 6.8087477684021\n",
            "5218: loss: 6.780603408813477\n",
            "5219: loss: 6.244678497314453\n",
            "5220: loss: 6.530275821685791\n",
            "5220: valid loss 6.778595447540283\n",
            "5221: loss: 6.438290119171143\n",
            "5222: loss: 6.6031341552734375\n",
            "5223: loss: 6.543409824371338\n",
            "5224: loss: 6.438782215118408\n",
            "5225: loss: 6.337625026702881\n",
            "5226: loss: 6.448441982269287\n",
            "5227: loss: 5.978899002075195\n",
            "5228: loss: 6.7864155769348145\n",
            "5229: loss: 6.308332920074463\n",
            "5230: loss: 6.925100326538086\n",
            "5231: loss: 6.782723903656006\n",
            "5232: loss: 6.53233528137207\n",
            "5233: loss: 6.339148044586182\n",
            "5234: loss: 6.744997501373291\n",
            "5235: loss: 6.282824516296387\n",
            "5236: loss: 6.751533508300781\n",
            "5237: loss: 6.6239333152771\n",
            "5238: loss: 6.640908718109131\n",
            "5239: loss: 6.675507068634033\n",
            "5240: loss: 7.180841445922852\n",
            "5240: valid loss 6.820156097412109\n",
            "5241: loss: 6.5267014503479\n",
            "5242: loss: 6.677975177764893\n",
            "5243: loss: 6.526768684387207\n",
            "5244: loss: 6.618194103240967\n",
            "5245: loss: 6.540185451507568\n",
            "5246: loss: 6.3991241455078125\n",
            "5247: loss: 6.828649997711182\n",
            "5248: loss: 6.677116394042969\n",
            "5249: loss: 6.458089351654053\n",
            "5250: loss: 6.714725971221924\n",
            "5251: loss: 6.7974700927734375\n",
            "5252: loss: 6.555387020111084\n",
            "5253: loss: 6.869439601898193\n",
            "5254: loss: 6.575720310211182\n",
            "5255: loss: 6.813140392303467\n",
            "5256: loss: 6.714773178100586\n",
            "5257: loss: 6.630395412445068\n",
            "5258: loss: 6.754039287567139\n",
            "5259: loss: 6.146117687225342\n",
            "5260: loss: 6.576415061950684\n",
            "5260: valid loss 6.848697185516357\n",
            "5261: loss: 6.725924491882324\n",
            "5262: loss: 6.926222324371338\n",
            "5263: loss: 6.957281589508057\n",
            "5264: loss: 6.476010322570801\n",
            "5265: loss: 6.654177188873291\n",
            "5266: loss: 6.6852521896362305\n",
            "5267: loss: 6.70722770690918\n",
            "5268: loss: 6.488165378570557\n",
            "5269: loss: 6.397552967071533\n",
            "5270: loss: 6.253194332122803\n",
            "5271: loss: 6.839560508728027\n",
            "5272: loss: 6.67522668838501\n",
            "5273: loss: 7.176745891571045\n",
            "5274: loss: 6.408847332000732\n",
            "5275: loss: 6.801579475402832\n",
            "5276: loss: 6.521243572235107\n",
            "5277: loss: 6.040645122528076\n",
            "5278: loss: 6.939779758453369\n",
            "5279: loss: 6.671780586242676\n",
            "5280: loss: 6.574167251586914\n",
            "5280: valid loss 6.528823375701904\n",
            "5281: loss: 6.530490398406982\n",
            "5282: loss: 6.521579265594482\n",
            "5283: loss: 6.337216854095459\n",
            "5284: loss: 6.578705310821533\n",
            "5285: loss: 6.402745723724365\n",
            "5286: loss: 7.025519371032715\n",
            "5287: loss: 6.499429225921631\n",
            "5288: loss: 6.783452987670898\n",
            "5289: loss: 6.7127366065979\n",
            "5290: loss: 6.632922172546387\n",
            "5291: loss: 6.633882522583008\n",
            "5292: loss: 6.312053680419922\n",
            "5293: loss: 6.406438827514648\n",
            "5294: loss: 6.853282928466797\n",
            "5295: loss: 6.704758167266846\n",
            "5296: loss: 6.489893436431885\n",
            "5297: loss: 6.770549297332764\n",
            "5298: loss: 6.844951629638672\n",
            "5299: loss: 6.716309547424316\n",
            "5300: loss: 6.55111837387085\n",
            "5300: valid loss 7.0456109046936035\n",
            "5301: loss: 6.428699970245361\n",
            "5302: loss: 6.485062122344971\n",
            "5303: loss: 6.076719760894775\n",
            "5304: loss: 6.030756950378418\n",
            "5305: loss: 6.243131160736084\n",
            "5306: loss: 6.381054878234863\n",
            "5307: loss: 6.755889892578125\n",
            "5308: loss: 6.568581581115723\n",
            "5309: loss: 6.744988441467285\n",
            "5310: loss: 6.820079326629639\n",
            "5311: loss: 6.3574323654174805\n",
            "5312: loss: 6.649440288543701\n",
            "5313: loss: 6.371701240539551\n",
            "5314: loss: 6.6915507316589355\n",
            "5315: loss: 7.146441459655762\n",
            "5316: loss: 6.892117023468018\n",
            "5317: loss: 6.436021327972412\n",
            "5318: loss: 6.117146015167236\n",
            "5319: loss: 6.438371658325195\n",
            "5320: loss: 6.301087379455566\n",
            "5320: valid loss 6.5407233238220215\n",
            "5321: loss: 6.675536632537842\n",
            "5322: loss: 6.182182788848877\n",
            "5323: loss: 6.649866580963135\n",
            "5324: loss: 6.522134304046631\n",
            "5325: loss: 6.347268104553223\n",
            "5326: loss: 6.609098434448242\n",
            "5327: loss: 6.5674591064453125\n",
            "5328: loss: 6.692811012268066\n",
            "5329: loss: 6.457331657409668\n",
            "5330: loss: 6.51260232925415\n",
            "5331: loss: 6.670647621154785\n",
            "5332: loss: 6.268545627593994\n",
            "5333: loss: 6.651458263397217\n",
            "5334: loss: 6.228357791900635\n",
            "5335: loss: 6.92098331451416\n",
            "5336: loss: 6.60705041885376\n",
            "5337: loss: 6.562633514404297\n",
            "5338: loss: 6.850920677185059\n",
            "5339: loss: 6.422821998596191\n",
            "5340: loss: 6.916966438293457\n",
            "5340: valid loss 6.831340312957764\n",
            "5341: loss: 6.763117790222168\n",
            "5342: loss: 6.593738555908203\n",
            "5343: loss: 6.772257328033447\n",
            "5344: loss: 6.765694618225098\n",
            "5345: loss: 5.716158866882324\n",
            "5346: loss: 6.850679874420166\n",
            "5347: loss: 6.44671106338501\n",
            "5348: loss: 6.286072254180908\n",
            "5349: loss: 6.463762283325195\n",
            "5350: loss: 7.0963664054870605\n",
            "5351: loss: 6.30232572555542\n",
            "5352: loss: 6.52121639251709\n",
            "5353: loss: 6.274173259735107\n",
            "5354: loss: 6.371397018432617\n",
            "5355: loss: 6.730489730834961\n",
            "5356: loss: 6.718605041503906\n",
            "5357: loss: 6.773846626281738\n",
            "5358: loss: 6.442681312561035\n",
            "5359: loss: 6.7276201248168945\n",
            "5360: loss: 7.166052341461182\n",
            "5360: valid loss 7.010769367218018\n",
            "5361: loss: 6.585547924041748\n",
            "5362: loss: 6.365690231323242\n",
            "5363: loss: 6.1570868492126465\n",
            "5364: loss: 7.003383159637451\n",
            "5365: loss: 6.539162635803223\n",
            "5366: loss: 6.951485633850098\n",
            "5367: loss: 6.910156726837158\n",
            "5368: loss: 6.283083438873291\n",
            "5369: loss: 6.846867084503174\n",
            "5370: loss: 6.76326847076416\n",
            "5371: loss: 6.637203693389893\n",
            "5372: loss: 6.816126346588135\n",
            "5373: loss: 6.699895858764648\n",
            "5374: loss: 6.225219249725342\n",
            "5375: loss: 6.637732028961182\n",
            "5376: loss: 6.741526126861572\n",
            "5377: loss: 6.348351001739502\n",
            "5378: loss: 6.076991558074951\n",
            "5379: loss: 6.6126251220703125\n",
            "5380: loss: 6.6293768882751465\n",
            "5380: valid loss 6.480973243713379\n",
            "5381: loss: 6.545640468597412\n",
            "5382: loss: 6.5483880043029785\n",
            "5383: loss: 6.099060535430908\n",
            "5384: loss: 6.58494234085083\n",
            "5385: loss: 6.737416744232178\n",
            "5386: loss: 5.786868572235107\n",
            "5387: loss: 6.565133571624756\n",
            "5388: loss: 6.9229888916015625\n",
            "5389: loss: 6.781200408935547\n",
            "5390: loss: 6.815562725067139\n",
            "5391: loss: 6.607058048248291\n",
            "5392: loss: 6.919856071472168\n",
            "5393: loss: 6.50093936920166\n",
            "5394: loss: 6.366530895233154\n",
            "5395: loss: 6.946109294891357\n",
            "5396: loss: 6.8506855964660645\n",
            "5397: loss: 6.646572113037109\n",
            "5398: loss: 6.304654598236084\n",
            "5399: loss: 6.715381145477295\n",
            "5400: loss: 6.63552188873291\n",
            "5400: valid loss 6.616508483886719\n",
            "5400: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "5401: loss: 6.707287788391113\n",
            "5402: loss: 6.7436137199401855\n",
            "5403: loss: 6.809727668762207\n",
            "5404: loss: 6.776562690734863\n",
            "5405: loss: 6.672903060913086\n",
            "5406: loss: 6.58088493347168\n",
            "5407: loss: 6.884389400482178\n",
            "5408: loss: 6.62356424331665\n",
            "5409: loss: 6.618122100830078\n",
            "5410: loss: 6.476135730743408\n",
            "5411: loss: 6.431715965270996\n",
            "5412: loss: 6.761260509490967\n",
            "5413: loss: 6.523138999938965\n",
            "5414: loss: 6.491932392120361\n",
            "5415: loss: 6.630773067474365\n",
            "5416: loss: 6.747871398925781\n",
            "5417: loss: 6.677634239196777\n",
            "5418: loss: 6.614028453826904\n",
            "5419: loss: 6.40044641494751\n",
            "5420: loss: 6.558187007904053\n",
            "5420: valid loss 6.772347927093506\n",
            "5421: loss: 6.763814926147461\n",
            "5422: loss: 6.0186848640441895\n",
            "5423: loss: 6.683955192565918\n",
            "5424: loss: 6.308345794677734\n",
            "5425: loss: 6.753467559814453\n",
            "5426: loss: 6.511190414428711\n",
            "5427: loss: 6.303329944610596\n",
            "5428: loss: 6.83353328704834\n",
            "5429: loss: 7.578375816345215\n",
            "5430: loss: 6.736227512359619\n",
            "5431: loss: 6.650557994842529\n",
            "5432: loss: 6.427286148071289\n",
            "5433: loss: 6.601357460021973\n",
            "5434: loss: 6.730778694152832\n",
            "5435: loss: 6.26221227645874\n",
            "5436: loss: 6.5470967292785645\n",
            "5437: loss: 6.762329578399658\n",
            "5438: loss: 6.031907558441162\n",
            "5439: loss: 6.480297565460205\n",
            "5440: loss: 6.520045280456543\n",
            "5440: valid loss 6.468349456787109\n",
            "5441: loss: 6.640383243560791\n",
            "5442: loss: 6.740261077880859\n",
            "5443: loss: 6.495745658874512\n",
            "5444: loss: 6.447669982910156\n",
            "5445: loss: 6.269308090209961\n",
            "5446: loss: 6.713387966156006\n",
            "5447: loss: 6.481285572052002\n",
            "5448: loss: 6.934156894683838\n",
            "5449: loss: 6.675450325012207\n",
            "5450: loss: 6.509899139404297\n",
            "5451: loss: 6.7561726570129395\n",
            "5452: loss: 6.7865495681762695\n",
            "5453: loss: 6.445218563079834\n",
            "5454: loss: 6.192152500152588\n",
            "5455: loss: 6.556582927703857\n",
            "5456: loss: 6.405088901519775\n",
            "5457: loss: 6.781438827514648\n",
            "5458: loss: 6.505385398864746\n",
            "5459: loss: 6.738314151763916\n",
            "5460: loss: 6.843783378601074\n",
            "5460: valid loss 6.343684673309326\n",
            "5461: loss: 6.823134899139404\n",
            "5462: loss: 6.524735927581787\n",
            "5463: loss: 6.91190767288208\n",
            "5464: loss: 6.356453895568848\n",
            "5465: loss: 6.814638137817383\n",
            "5466: loss: 6.548254013061523\n",
            "5467: loss: 6.400568008422852\n",
            "5468: loss: 6.424851894378662\n",
            "5469: loss: 6.67102575302124\n",
            "5470: loss: 6.450050354003906\n",
            "5471: loss: 6.360898971557617\n",
            "5472: loss: 7.062969207763672\n",
            "5473: loss: 6.4970269203186035\n",
            "5474: loss: 6.467737197875977\n",
            "5475: loss: 6.495373249053955\n",
            "5476: loss: 6.511038303375244\n",
            "5477: loss: 6.891168117523193\n",
            "5478: loss: 6.509666442871094\n",
            "5479: loss: 6.446350574493408\n",
            "5480: loss: 6.596465587615967\n",
            "5480: valid loss 6.8235907554626465\n",
            "5481: loss: 7.0475945472717285\n",
            "5482: loss: 6.375863552093506\n",
            "5483: loss: 6.663752555847168\n",
            "5484: loss: 6.672672748565674\n",
            "5485: loss: 6.753200531005859\n",
            "5486: loss: 6.672980785369873\n",
            "5487: loss: 6.449496746063232\n",
            "5488: loss: 6.534299373626709\n",
            "5489: loss: 6.490416049957275\n",
            "5490: loss: 6.512152194976807\n",
            "5491: loss: 6.459099769592285\n",
            "5492: loss: 6.6303205490112305\n",
            "5493: loss: 6.514532566070557\n",
            "5494: loss: 6.5454912185668945\n",
            "5495: loss: 6.47461462020874\n",
            "5496: loss: 6.117692470550537\n",
            "5497: loss: 6.928528308868408\n",
            "5498: loss: 6.017375469207764\n",
            "5499: loss: 6.329958438873291\n",
            "5500: loss: 6.581027507781982\n",
            "5500: valid loss 7.003452301025391\n",
            "5501: loss: 6.290611267089844\n",
            "5502: loss: 6.312075138092041\n",
            "5503: loss: 6.0531768798828125\n",
            "5504: loss: 6.474909782409668\n",
            "5505: loss: 6.728031158447266\n",
            "5506: loss: 5.334201812744141\n",
            "5507: loss: 6.4121785163879395\n",
            "5508: loss: 6.584327220916748\n",
            "5509: loss: 6.398972511291504\n",
            "5510: loss: 6.689185619354248\n",
            "5511: loss: 6.192399501800537\n",
            "5512: loss: 6.679299354553223\n",
            "5513: loss: 7.215286731719971\n",
            "5514: loss: 6.631642818450928\n",
            "5515: loss: 6.623167991638184\n",
            "5516: loss: 6.775691509246826\n",
            "5517: loss: 6.303758144378662\n",
            "5518: loss: 6.627328395843506\n",
            "5519: loss: 6.282191753387451\n",
            "5520: loss: 6.509043216705322\n",
            "5520: valid loss 6.736439228057861\n",
            "5521: loss: 6.2896952629089355\n",
            "5522: loss: 6.628127098083496\n",
            "5523: loss: 6.26011323928833\n",
            "5524: loss: 6.251003742218018\n",
            "5525: loss: 6.691598415374756\n",
            "5526: loss: 6.683262825012207\n",
            "5527: loss: 6.471757411956787\n",
            "5528: loss: 6.4514079093933105\n",
            "5529: loss: 6.367306232452393\n",
            "5530: loss: 6.520092487335205\n",
            "5531: loss: 6.210249423980713\n",
            "5532: loss: 6.525463581085205\n",
            "5533: loss: 6.839344501495361\n",
            "5534: loss: 6.521565914154053\n",
            "5535: loss: 6.460822582244873\n",
            "5536: loss: 6.370390892028809\n",
            "5537: loss: 6.710506439208984\n",
            "5538: loss: 6.815070629119873\n",
            "5539: loss: 6.72179651260376\n",
            "5540: loss: 6.510452747344971\n",
            "5540: valid loss 6.546723365783691\n",
            "5541: loss: 6.496529579162598\n",
            "5542: loss: 6.650537014007568\n",
            "5543: loss: 6.624808311462402\n",
            "5544: loss: 6.561286449432373\n",
            "5545: loss: 6.74308967590332\n",
            "5546: loss: 6.992346286773682\n",
            "5547: loss: 6.28994607925415\n",
            "5548: loss: 6.210305690765381\n",
            "5549: loss: 6.917709827423096\n",
            "5550: loss: 6.518866539001465\n",
            "5551: loss: 7.093661308288574\n",
            "5552: loss: 6.875814914703369\n",
            "5553: loss: 6.880318641662598\n",
            "5554: loss: 6.132267475128174\n",
            "5555: loss: 6.892759323120117\n",
            "5556: loss: 6.338309288024902\n",
            "5557: loss: 6.487107276916504\n",
            "5558: loss: 6.690247058868408\n",
            "5559: loss: 6.547416687011719\n",
            "5560: loss: 6.683236122131348\n",
            "5560: valid loss 6.7108049392700195\n",
            "5561: loss: 6.3850321769714355\n",
            "5562: loss: 6.614591121673584\n",
            "5563: loss: 6.521968841552734\n",
            "5564: loss: 6.9887189865112305\n",
            "5565: loss: 6.5800347328186035\n",
            "5566: loss: 6.811653137207031\n",
            "5567: loss: 5.877120018005371\n",
            "5568: loss: 6.719388484954834\n",
            "5569: loss: 6.6102213859558105\n",
            "5570: loss: 6.955068111419678\n",
            "5571: loss: 6.354907035827637\n",
            "5572: loss: 6.536125183105469\n",
            "5573: loss: 6.566031455993652\n",
            "5574: loss: 6.632056713104248\n",
            "5575: loss: 7.030116081237793\n",
            "5576: loss: 6.4779534339904785\n",
            "5577: loss: 6.618188381195068\n",
            "5578: loss: 6.798753261566162\n",
            "5579: loss: 6.844729900360107\n",
            "5580: loss: 6.530471324920654\n",
            "5580: valid loss 6.496413707733154\n",
            "5581: loss: 6.834485054016113\n",
            "5582: loss: 6.702446460723877\n",
            "5583: loss: 6.643869400024414\n",
            "5584: loss: 6.598654270172119\n",
            "5585: loss: 6.492020130157471\n",
            "5586: loss: 6.854005813598633\n",
            "5587: loss: 6.57596492767334\n",
            "5588: loss: 6.695442199707031\n",
            "5589: loss: 6.112334251403809\n",
            "5590: loss: 6.673386096954346\n",
            "5591: loss: 6.774042129516602\n",
            "5592: loss: 6.776397705078125\n",
            "5593: loss: 6.790456771850586\n",
            "5594: loss: 6.862269878387451\n",
            "5595: loss: 6.8936285972595215\n",
            "5596: loss: 6.5787529945373535\n",
            "5597: loss: 6.377058982849121\n",
            "5598: loss: 6.635976314544678\n",
            "5599: loss: 6.4739460945129395\n",
            "5600: loss: 6.6463847160339355\n",
            "5600: valid loss 5.559610843658447\n",
            "5600: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "5601: loss: 6.608308792114258\n",
            "5602: loss: 6.536900520324707\n",
            "5603: loss: 6.407943248748779\n",
            "5604: loss: 6.733685493469238\n",
            "5605: loss: 6.70713472366333\n",
            "5606: loss: 6.756819248199463\n",
            "5607: loss: 6.7326273918151855\n",
            "5608: loss: 6.522716522216797\n",
            "5609: loss: 6.6303205490112305\n",
            "5610: loss: 5.792181015014648\n",
            "5611: loss: 6.660884380340576\n",
            "5612: loss: 6.735164165496826\n",
            "5613: loss: 6.65615177154541\n",
            "5614: loss: 6.7971625328063965\n",
            "5615: loss: 6.572493553161621\n",
            "5616: loss: 7.173159599304199\n",
            "5617: loss: 6.623410701751709\n",
            "5618: loss: 6.694483757019043\n",
            "5619: loss: 6.819786548614502\n",
            "5620: loss: 6.80268669128418\n",
            "5620: valid loss 6.794541835784912\n",
            "5621: loss: 6.6096343994140625\n",
            "5622: loss: 6.580554485321045\n",
            "5623: loss: 6.518115520477295\n",
            "5624: loss: 6.490870952606201\n",
            "5625: loss: 6.141389846801758\n",
            "5626: loss: 6.367796897888184\n",
            "5627: loss: 6.548372268676758\n",
            "5628: loss: 6.761782646179199\n",
            "5629: loss: 6.690138816833496\n",
            "5630: loss: 6.531171798706055\n",
            "5631: loss: 6.648859977722168\n",
            "5632: loss: 6.553678512573242\n",
            "5633: loss: 6.301261901855469\n",
            "5634: loss: 6.459390163421631\n",
            "5635: loss: 6.589831829071045\n",
            "5636: loss: 6.33368444442749\n",
            "5637: loss: 6.459122180938721\n",
            "5638: loss: 6.573301792144775\n",
            "5639: loss: 6.628492832183838\n",
            "5640: loss: 6.522060394287109\n",
            "5640: valid loss 6.317583084106445\n",
            "5641: loss: 6.3909101486206055\n",
            "5642: loss: 6.23167610168457\n",
            "5643: loss: 6.810429573059082\n",
            "5644: loss: 6.438233375549316\n",
            "5645: loss: 5.794521331787109\n",
            "5646: loss: 6.397163391113281\n",
            "5647: loss: 6.260455131530762\n",
            "5648: loss: 6.651055812835693\n",
            "5649: loss: 6.630579471588135\n",
            "5650: loss: 7.011053562164307\n",
            "5651: loss: 6.109405040740967\n",
            "5652: loss: 5.898731231689453\n",
            "5653: loss: 7.056096076965332\n",
            "5654: loss: 6.223884582519531\n",
            "5655: loss: 6.764158725738525\n",
            "5656: loss: 6.9852094650268555\n",
            "5657: loss: 6.492964744567871\n",
            "5658: loss: 6.654716491699219\n",
            "5659: loss: 6.837989807128906\n",
            "5660: loss: 6.597585678100586\n",
            "5660: valid loss 6.597132205963135\n",
            "5661: loss: 6.709282875061035\n",
            "5662: loss: 6.802483558654785\n",
            "5663: loss: 6.382576942443848\n",
            "5664: loss: 6.485035419464111\n",
            "5665: loss: 6.609045028686523\n",
            "5666: loss: 6.734714031219482\n",
            "5667: loss: 6.741982460021973\n",
            "5668: loss: 6.548421859741211\n",
            "5669: loss: 6.541297912597656\n",
            "5670: loss: 6.159620761871338\n",
            "5671: loss: 6.773889064788818\n",
            "5672: loss: 6.541666507720947\n",
            "5673: loss: 6.765746116638184\n",
            "5674: loss: 6.8012566566467285\n",
            "5675: loss: 7.083172798156738\n",
            "5676: loss: 6.093966960906982\n",
            "5677: loss: 6.5689263343811035\n",
            "5678: loss: 6.4016032218933105\n",
            "5679: loss: 6.804415225982666\n",
            "5680: loss: 6.894896984100342\n",
            "5680: valid loss 6.369473457336426\n",
            "5681: loss: 5.859840393066406\n",
            "5682: loss: 6.601781845092773\n",
            "5683: loss: 6.677573204040527\n",
            "5684: loss: 6.553653717041016\n",
            "5685: loss: 6.507664680480957\n",
            "5686: loss: 6.587892055511475\n",
            "5687: loss: 6.549093723297119\n",
            "5688: loss: 6.636416912078857\n",
            "5689: loss: 6.63554048538208\n",
            "5690: loss: 6.609833240509033\n",
            "5691: loss: 6.557537078857422\n",
            "5692: loss: 6.291563034057617\n",
            "5693: loss: 6.667403221130371\n",
            "5694: loss: 6.849656105041504\n",
            "5695: loss: 6.378270626068115\n",
            "5696: loss: 6.60847282409668\n",
            "5697: loss: 6.653731822967529\n",
            "5698: loss: 6.484142780303955\n",
            "5699: loss: 6.738797187805176\n",
            "5700: loss: 6.324642181396484\n",
            "5700: valid loss 6.391379356384277\n",
            "5701: loss: 6.434113025665283\n",
            "5702: loss: 6.57020902633667\n",
            "5703: loss: 6.7176432609558105\n",
            "5704: loss: 6.2143425941467285\n",
            "5705: loss: 6.268249034881592\n",
            "5706: loss: 6.443163871765137\n",
            "5707: loss: 6.534274578094482\n",
            "5708: loss: 6.803905487060547\n",
            "5709: loss: 6.424909591674805\n",
            "5710: loss: 6.7839460372924805\n",
            "5711: loss: 6.449658393859863\n",
            "5712: loss: 6.534684658050537\n",
            "5713: loss: 6.468381404876709\n",
            "5714: loss: 6.7239484786987305\n",
            "5715: loss: 6.537357807159424\n",
            "5716: loss: 6.302940845489502\n",
            "5717: loss: 6.414491176605225\n",
            "5718: loss: 6.767486095428467\n",
            "5719: loss: 6.314060688018799\n",
            "5720: loss: 6.781584739685059\n",
            "5720: valid loss 6.6291728019714355\n",
            "5721: loss: 6.804024696350098\n",
            "5722: loss: 6.485163688659668\n",
            "5723: loss: 6.383465766906738\n",
            "5724: loss: 6.582993984222412\n",
            "5725: loss: 5.918328285217285\n",
            "5726: loss: 6.910368919372559\n",
            "5727: loss: 6.531026840209961\n",
            "5728: loss: 6.86892032623291\n",
            "5729: loss: 6.592662811279297\n",
            "5730: loss: 6.6723246574401855\n",
            "5731: loss: 6.852463722229004\n",
            "5732: loss: 6.763243198394775\n",
            "5733: loss: 6.483712673187256\n",
            "5734: loss: 6.786614894866943\n",
            "5735: loss: 6.631577968597412\n",
            "5736: loss: 6.566662311553955\n",
            "5737: loss: 6.757827281951904\n",
            "5738: loss: 6.55075740814209\n",
            "5739: loss: 6.718496799468994\n",
            "5740: loss: 6.705221176147461\n",
            "5740: valid loss 6.575328826904297\n",
            "5741: loss: 6.448772430419922\n",
            "5742: loss: 6.5001983642578125\n",
            "5743: loss: 6.35841178894043\n",
            "5744: loss: 6.455606460571289\n",
            "5745: loss: 6.662393093109131\n",
            "5746: loss: 6.70734167098999\n",
            "5747: loss: 6.495733737945557\n",
            "5748: loss: 6.522589683532715\n",
            "5749: loss: 6.831116676330566\n",
            "5750: loss: 6.250234603881836\n",
            "5751: loss: 6.797842979431152\n",
            "5752: loss: 6.4729790687561035\n",
            "5753: loss: 6.662845134735107\n",
            "5754: loss: 6.398306846618652\n",
            "5755: loss: 6.613055229187012\n",
            "5756: loss: 6.695478439331055\n",
            "5757: loss: 6.33142614364624\n",
            "5758: loss: 6.2694525718688965\n",
            "5759: loss: 6.766720771789551\n",
            "5760: loss: 6.6833624839782715\n",
            "5760: valid loss 6.371240139007568\n",
            "5761: loss: 6.306936264038086\n",
            "5762: loss: 6.57426118850708\n",
            "5763: loss: 6.578755855560303\n",
            "5764: loss: 6.943526744842529\n",
            "5765: loss: 6.871119499206543\n",
            "5766: loss: 6.461846828460693\n",
            "5767: loss: 6.383906841278076\n",
            "5768: loss: 6.73043155670166\n",
            "5769: loss: 6.434319019317627\n",
            "5770: loss: 6.711275577545166\n",
            "5771: loss: 6.593135833740234\n",
            "5772: loss: 6.5140767097473145\n",
            "5773: loss: 6.3994951248168945\n",
            "5774: loss: 6.477061748504639\n",
            "5775: loss: 6.414406776428223\n",
            "5776: loss: 6.656749248504639\n",
            "5777: loss: 6.613247871398926\n",
            "5778: loss: 6.542016983032227\n",
            "5779: loss: 6.629072666168213\n",
            "5780: loss: 6.694118022918701\n",
            "5780: valid loss 6.595887660980225\n",
            "5781: loss: 6.3695969581604\n",
            "5782: loss: 6.004175662994385\n",
            "5783: loss: 6.255120277404785\n",
            "5784: loss: 6.949366092681885\n",
            "5785: loss: 6.525304317474365\n",
            "5786: loss: 6.468745708465576\n",
            "5787: loss: 6.617959499359131\n",
            "5788: loss: 6.110262870788574\n",
            "5789: loss: 6.331140995025635\n",
            "5790: loss: 6.775791645050049\n",
            "5791: loss: 6.866426944732666\n",
            "5792: loss: 6.95424747467041\n",
            "5793: loss: 6.663849830627441\n",
            "5794: loss: 6.726224899291992\n",
            "5795: loss: 6.7337846755981445\n",
            "5796: loss: 6.701834201812744\n",
            "5797: loss: 6.599193096160889\n",
            "5798: loss: 6.371647834777832\n",
            "5799: loss: 6.5477519035339355\n",
            "5800: loss: 6.51473331451416\n",
            "5800: valid loss 6.653059005737305\n",
            "5800: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "5801: loss: 6.485604763031006\n",
            "5802: loss: 6.671230792999268\n",
            "5803: loss: 6.620713710784912\n",
            "5804: loss: 6.324179172515869\n",
            "5805: loss: 6.823835849761963\n",
            "5806: loss: 6.904114246368408\n",
            "5807: loss: 6.317129135131836\n",
            "5808: loss: 6.5319085121154785\n",
            "5809: loss: 6.528341770172119\n",
            "5810: loss: 6.594003200531006\n",
            "5811: loss: 6.467640399932861\n",
            "5812: loss: 6.643616676330566\n",
            "5813: loss: 6.503167152404785\n",
            "5814: loss: 6.7969746589660645\n",
            "5815: loss: 6.560226917266846\n",
            "5816: loss: 6.662085056304932\n",
            "5817: loss: 6.399200916290283\n",
            "5818: loss: 6.367237091064453\n",
            "5819: loss: 6.324252605438232\n",
            "5820: loss: 6.371970176696777\n",
            "5820: valid loss 6.235139846801758\n",
            "5821: loss: 6.566959857940674\n",
            "5822: loss: 6.483401298522949\n",
            "5823: loss: 6.695593357086182\n",
            "5824: loss: 6.322540760040283\n",
            "5825: loss: 6.314388275146484\n",
            "5826: loss: 6.640253067016602\n",
            "5827: loss: 6.326696872711182\n",
            "5828: loss: 6.922191143035889\n",
            "5829: loss: 6.768242835998535\n",
            "5830: loss: 6.570384502410889\n",
            "5831: loss: 6.880160808563232\n",
            "5832: loss: 6.450286388397217\n",
            "5833: loss: 6.491290092468262\n",
            "5834: loss: 6.76145601272583\n",
            "5835: loss: 6.64450216293335\n",
            "5836: loss: 6.32637357711792\n",
            "5837: loss: 6.252822399139404\n",
            "5838: loss: 6.791927337646484\n",
            "5839: loss: 6.395163059234619\n",
            "5840: loss: 7.0062055587768555\n",
            "5840: valid loss 6.7761454582214355\n",
            "5841: loss: 6.704748630523682\n",
            "5842: loss: 6.261416912078857\n",
            "5843: loss: 6.946763038635254\n",
            "5844: loss: 6.907983303070068\n",
            "5845: loss: 6.650300025939941\n",
            "5846: loss: 6.255267143249512\n",
            "5847: loss: 6.6583991050720215\n",
            "5848: loss: 6.540912628173828\n",
            "5849: loss: 6.597850322723389\n",
            "5850: loss: 6.8606648445129395\n",
            "5851: loss: 6.281985282897949\n",
            "5852: loss: 6.880269527435303\n",
            "5853: loss: 6.896047592163086\n",
            "5854: loss: 6.664550304412842\n",
            "5855: loss: 6.566372394561768\n",
            "5856: loss: 6.598415374755859\n",
            "5857: loss: 6.7598090171813965\n",
            "5858: loss: 6.680139064788818\n",
            "5859: loss: 6.652429103851318\n",
            "5860: loss: 6.540359973907471\n",
            "5860: valid loss 6.723888397216797\n",
            "5861: loss: 6.216699600219727\n",
            "5862: loss: 6.880488872528076\n",
            "5863: loss: 6.629595756530762\n",
            "5864: loss: 6.514361381530762\n",
            "5865: loss: 6.445342540740967\n",
            "5866: loss: 6.391495704650879\n",
            "5867: loss: 6.580129623413086\n",
            "5868: loss: 6.425848484039307\n",
            "5869: loss: 6.609683990478516\n",
            "5870: loss: 6.578256607055664\n",
            "5871: loss: 6.567647933959961\n",
            "5872: loss: 6.564009666442871\n",
            "5873: loss: 6.7912821769714355\n",
            "5874: loss: 6.4956231117248535\n",
            "5875: loss: 6.378090858459473\n",
            "5876: loss: 6.8609490394592285\n",
            "5877: loss: 6.827504634857178\n",
            "5878: loss: 6.3838114738464355\n",
            "5879: loss: 6.367574214935303\n",
            "5880: loss: 6.659691333770752\n",
            "5880: valid loss 6.51023006439209\n",
            "5881: loss: 6.6379523277282715\n",
            "5882: loss: 6.5910234451293945\n",
            "5883: loss: 6.2047576904296875\n",
            "5884: loss: 6.109588623046875\n",
            "5885: loss: 6.889406204223633\n",
            "5886: loss: 6.545629024505615\n",
            "5887: loss: 6.890959739685059\n",
            "5888: loss: 6.677908897399902\n",
            "5889: loss: 6.5039753913879395\n",
            "5890: loss: 6.1925506591796875\n",
            "5891: loss: 6.836795330047607\n",
            "5892: loss: 6.596108436584473\n",
            "5893: loss: 6.7208571434021\n",
            "5894: loss: 6.69984769821167\n",
            "5895: loss: 6.414268493652344\n",
            "5896: loss: 6.68989896774292\n",
            "5897: loss: 6.836983680725098\n",
            "5898: loss: 6.439826965332031\n",
            "5899: loss: 6.462386131286621\n",
            "5900: loss: 6.811703205108643\n",
            "5900: valid loss 6.614309787750244\n",
            "5901: loss: 6.582299709320068\n",
            "5902: loss: 6.512607097625732\n",
            "5903: loss: 6.362802982330322\n",
            "5904: loss: 6.633989334106445\n",
            "5905: loss: 6.4748687744140625\n",
            "5906: loss: 6.7832794189453125\n",
            "5907: loss: 6.560732364654541\n",
            "5908: loss: 6.734856128692627\n",
            "5909: loss: 6.553059101104736\n",
            "5910: loss: 6.506217956542969\n",
            "5911: loss: 7.032689094543457\n",
            "5912: loss: 6.392373085021973\n",
            "5913: loss: 6.3875732421875\n",
            "5914: loss: 6.651923656463623\n",
            "5915: loss: 6.29449462890625\n",
            "5916: loss: 6.599841117858887\n",
            "5917: loss: 6.660080909729004\n",
            "5918: loss: 6.334184169769287\n",
            "5919: loss: 6.835015296936035\n",
            "5920: loss: 6.889588356018066\n",
            "5920: valid loss 6.3588762283325195\n",
            "5921: loss: 7.016251087188721\n",
            "5922: loss: 6.201573371887207\n",
            "5923: loss: 6.473625183105469\n",
            "5924: loss: 6.648116111755371\n",
            "5925: loss: 6.055908203125\n",
            "5926: loss: 6.479310512542725\n",
            "5927: loss: 6.603725433349609\n",
            "5928: loss: 6.375539779663086\n",
            "5929: loss: 6.7318501472473145\n",
            "5930: loss: 6.7883734703063965\n",
            "5931: loss: 6.299520015716553\n",
            "5932: loss: 6.791814804077148\n",
            "5933: loss: 5.879844665527344\n",
            "5934: loss: 6.347398281097412\n",
            "5935: loss: 6.923630237579346\n",
            "5936: loss: 6.650552749633789\n",
            "5937: loss: 6.237220287322998\n",
            "5938: loss: 6.7227911949157715\n",
            "5939: loss: 6.418405532836914\n",
            "5940: loss: 6.682401657104492\n",
            "5940: valid loss 6.311872959136963\n",
            "5941: loss: 6.765974998474121\n",
            "5942: loss: 6.731642723083496\n",
            "5943: loss: 6.721487998962402\n",
            "5944: loss: 6.4709296226501465\n",
            "5945: loss: 6.713479518890381\n",
            "5946: loss: 6.444944858551025\n",
            "5947: loss: 6.814150333404541\n",
            "5948: loss: 6.682925701141357\n",
            "5949: loss: 6.871139049530029\n",
            "5950: loss: 6.511179447174072\n",
            "5951: loss: 6.88206672668457\n",
            "5952: loss: 6.662353992462158\n",
            "5953: loss: 6.296489238739014\n",
            "5954: loss: 6.2692131996154785\n",
            "5955: loss: 6.701414585113525\n",
            "5956: loss: 6.264708518981934\n",
            "5957: loss: 6.202723026275635\n",
            "5958: loss: 6.668204307556152\n",
            "5959: loss: 6.787600517272949\n",
            "5960: loss: 6.160376071929932\n",
            "5960: valid loss 6.433513164520264\n",
            "5961: loss: 6.666342258453369\n",
            "5962: loss: 6.542115688323975\n",
            "5963: loss: 6.636027812957764\n",
            "5964: loss: 6.731623649597168\n",
            "5965: loss: 6.947678089141846\n",
            "5966: loss: 6.760671138763428\n",
            "5967: loss: 6.290383815765381\n",
            "5968: loss: 6.550116062164307\n",
            "5969: loss: 6.390359878540039\n",
            "5970: loss: 6.3260884284973145\n",
            "5971: loss: 6.456259250640869\n",
            "5972: loss: 6.352894306182861\n",
            "5973: loss: 6.647984504699707\n",
            "5974: loss: 6.386179447174072\n",
            "5975: loss: 6.706778526306152\n",
            "5976: loss: 6.392315864562988\n",
            "5977: loss: 6.7881622314453125\n",
            "5978: loss: 6.2864603996276855\n",
            "5979: loss: 6.526258945465088\n",
            "5980: loss: 6.444212436676025\n",
            "5980: valid loss 6.785548686981201\n",
            "5981: loss: 6.424126148223877\n",
            "5982: loss: 6.583054065704346\n",
            "5983: loss: 6.173856258392334\n",
            "5984: loss: 5.860114097595215\n",
            "5985: loss: 6.634402751922607\n",
            "5986: loss: 6.234428405761719\n",
            "5987: loss: 6.500815391540527\n",
            "5988: loss: 6.806976318359375\n",
            "5989: loss: 6.701578617095947\n",
            "5990: loss: 6.622771263122559\n",
            "5991: loss: 6.5703654289245605\n",
            "5992: loss: 6.794904708862305\n",
            "5993: loss: 6.504832744598389\n",
            "5994: loss: 6.66298246383667\n",
            "5995: loss: 6.340200424194336\n",
            "5996: loss: 6.2006072998046875\n",
            "5997: loss: 6.585504055023193\n",
            "5998: loss: 6.730625629425049\n",
            "5999: loss: 6.446484088897705\n",
            "6000: loss: 6.370057582855225\n",
            "6000: valid loss 6.275415420532227\n",
            "6000: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "6001: loss: 6.331210613250732\n",
            "6002: loss: 6.79097843170166\n",
            "6003: loss: 6.848476409912109\n",
            "6004: loss: 6.8917059898376465\n",
            "6005: loss: 6.855425834655762\n",
            "6006: loss: 6.462972640991211\n",
            "6007: loss: 6.997441291809082\n",
            "6008: loss: 6.7508344650268555\n",
            "6009: loss: 6.319676876068115\n",
            "6010: loss: 6.636455059051514\n",
            "6011: loss: 6.501701354980469\n",
            "6012: loss: 6.7099714279174805\n",
            "6013: loss: 6.710720062255859\n",
            "6014: loss: 6.141892910003662\n",
            "6015: loss: 6.679890155792236\n",
            "6016: loss: 6.395349502563477\n",
            "6017: loss: 6.608184337615967\n",
            "6018: loss: 6.5021233558654785\n",
            "6019: loss: 6.513297080993652\n",
            "6020: loss: 6.532470703125\n",
            "6020: valid loss 6.450870513916016\n",
            "6021: loss: 6.452751159667969\n",
            "6022: loss: 6.4306559562683105\n",
            "6023: loss: 6.177079677581787\n",
            "6024: loss: 6.137502193450928\n",
            "6025: loss: 6.277834892272949\n",
            "6026: loss: 6.700500965118408\n",
            "6027: loss: 6.749785900115967\n",
            "6028: loss: 6.466867923736572\n",
            "6029: loss: 6.732963562011719\n",
            "6030: loss: 6.824045658111572\n",
            "6031: loss: 6.434045314788818\n",
            "6032: loss: 6.570756435394287\n",
            "6033: loss: 6.748370170593262\n",
            "6034: loss: 6.389642715454102\n",
            "6035: loss: 6.205384254455566\n",
            "6036: loss: 6.668269634246826\n",
            "6037: loss: 6.546127796173096\n",
            "6038: loss: 6.55553674697876\n",
            "6039: loss: 6.106851577758789\n",
            "6040: loss: 6.555809497833252\n",
            "6040: valid loss 6.3547563552856445\n",
            "6041: loss: 6.60709285736084\n",
            "6042: loss: 6.536007881164551\n",
            "6043: loss: 6.684782028198242\n",
            "6044: loss: 6.4062347412109375\n",
            "6045: loss: 6.4810051918029785\n",
            "6046: loss: 6.6972856521606445\n",
            "6047: loss: 6.296529769897461\n",
            "6048: loss: 6.467356204986572\n",
            "6049: loss: 6.883733749389648\n",
            "6050: loss: 6.1817169189453125\n",
            "6051: loss: 6.652956962585449\n",
            "6052: loss: 6.383117198944092\n",
            "6053: loss: 6.669543266296387\n",
            "6054: loss: 6.543586254119873\n",
            "6055: loss: 6.707451820373535\n",
            "6056: loss: 6.515923976898193\n",
            "6057: loss: 6.363893032073975\n",
            "6058: loss: 6.591868877410889\n",
            "6059: loss: 6.448916435241699\n",
            "6060: loss: 6.550307750701904\n",
            "6060: valid loss 6.319840908050537\n",
            "6061: loss: 6.449001312255859\n",
            "6062: loss: 6.9752116203308105\n",
            "6063: loss: 6.748754024505615\n",
            "6064: loss: 6.630373001098633\n",
            "6065: loss: 6.388433933258057\n",
            "6066: loss: 6.382282257080078\n",
            "6067: loss: 6.6222076416015625\n",
            "6068: loss: 6.176527976989746\n",
            "6069: loss: 6.267358303070068\n",
            "6070: loss: 6.371062755584717\n",
            "6071: loss: 6.79937219619751\n",
            "6072: loss: 6.734150409698486\n",
            "6073: loss: 6.466589450836182\n",
            "6074: loss: 6.205377101898193\n",
            "6075: loss: 6.3362016677856445\n",
            "6076: loss: 6.46219539642334\n",
            "6077: loss: 6.452383518218994\n",
            "6078: loss: 6.322112560272217\n",
            "6079: loss: 6.456604480743408\n",
            "6080: loss: 6.17782735824585\n",
            "6080: valid loss 6.31149959564209\n",
            "6081: loss: 6.883127212524414\n",
            "6082: loss: 6.854875564575195\n",
            "6083: loss: 6.338372230529785\n",
            "6084: loss: 6.672296524047852\n",
            "6085: loss: 6.263596057891846\n",
            "6086: loss: 6.843523025512695\n",
            "6087: loss: 5.6800031661987305\n",
            "6088: loss: 6.245347023010254\n",
            "6089: loss: 6.590178966522217\n",
            "6090: loss: 6.442459583282471\n",
            "6091: loss: 6.209466934204102\n",
            "6092: loss: 6.5152974128723145\n",
            "6093: loss: 6.503881931304932\n",
            "6094: loss: 6.795380115509033\n",
            "6095: loss: 6.405563831329346\n",
            "6096: loss: 6.20220947265625\n",
            "6097: loss: 6.499954700469971\n",
            "6098: loss: 6.381359577178955\n",
            "6099: loss: 6.681307792663574\n",
            "6100: loss: 6.7749342918396\n",
            "6100: valid loss 6.6885986328125\n",
            "6101: loss: 6.384445667266846\n",
            "6102: loss: 6.613110542297363\n",
            "6103: loss: 6.323939800262451\n",
            "6104: loss: 6.6830902099609375\n",
            "6105: loss: 6.763334274291992\n",
            "6106: loss: 6.827956199645996\n",
            "6107: loss: 6.418592929840088\n",
            "6108: loss: 6.325741291046143\n",
            "6109: loss: 6.660521030426025\n",
            "6110: loss: 6.642446041107178\n",
            "6111: loss: 6.6545939445495605\n",
            "6112: loss: 6.56768798828125\n",
            "6113: loss: 6.805016040802002\n",
            "6114: loss: 6.679940700531006\n",
            "6115: loss: 6.331349849700928\n",
            "6116: loss: 6.348032474517822\n",
            "6117: loss: 6.670995235443115\n",
            "6118: loss: 6.396796703338623\n",
            "6119: loss: 6.603824138641357\n",
            "6120: loss: 6.677311420440674\n",
            "6120: valid loss 6.701883792877197\n",
            "6121: loss: 6.429855823516846\n",
            "6122: loss: 6.790040493011475\n",
            "6123: loss: 6.975633144378662\n",
            "6124: loss: 6.4115071296691895\n",
            "6125: loss: 6.470895767211914\n",
            "6126: loss: 6.327225208282471\n",
            "6127: loss: 6.785497665405273\n",
            "6128: loss: 6.399395942687988\n",
            "6129: loss: 6.254518508911133\n",
            "6130: loss: 6.508955478668213\n",
            "6131: loss: 6.3628129959106445\n",
            "6132: loss: 6.469209671020508\n",
            "6133: loss: 6.5417656898498535\n",
            "6134: loss: 6.431492805480957\n",
            "6135: loss: 6.559138298034668\n",
            "6136: loss: 0.4314977824687958\n",
            "6137: loss: 6.681131839752197\n",
            "6138: loss: 6.534523010253906\n",
            "6139: loss: 6.638126373291016\n",
            "6140: loss: 6.365264892578125\n",
            "6140: valid loss 6.670654773712158\n",
            "6141: loss: 6.279804229736328\n",
            "6142: loss: 6.368524074554443\n",
            "6143: loss: 6.574585437774658\n",
            "6144: loss: 6.557504653930664\n",
            "6145: loss: 6.491557598114014\n",
            "6146: loss: 6.2871856689453125\n",
            "6147: loss: 6.522477626800537\n",
            "6148: loss: 6.840590000152588\n",
            "6149: loss: 6.5191969871521\n",
            "6150: loss: 6.583045482635498\n",
            "6151: loss: 6.527811050415039\n",
            "6152: loss: 6.169249534606934\n",
            "6153: loss: 6.345059871673584\n",
            "6154: loss: 6.267243385314941\n",
            "6155: loss: 6.758748531341553\n",
            "6156: loss: 6.8329315185546875\n",
            "6157: loss: 6.455178737640381\n",
            "6158: loss: 6.45812463760376\n",
            "6159: loss: 6.6208391189575195\n",
            "6160: loss: 6.591383934020996\n",
            "6160: valid loss 6.5314249992370605\n",
            "6161: loss: 5.962625026702881\n",
            "6162: loss: 6.166899681091309\n",
            "6163: loss: 6.604450702667236\n",
            "6164: loss: 6.43960428237915\n",
            "6165: loss: 6.109198570251465\n",
            "6166: loss: 6.33980655670166\n",
            "6167: loss: 6.108705043792725\n",
            "6168: loss: 6.696103572845459\n",
            "6169: loss: 6.842175483703613\n",
            "6170: loss: 6.695552825927734\n",
            "6171: loss: 6.4153313636779785\n",
            "6172: loss: 6.226943016052246\n",
            "6173: loss: 6.373194694519043\n",
            "6174: loss: 6.664705276489258\n",
            "6175: loss: 6.680776596069336\n",
            "6176: loss: 6.384829521179199\n",
            "6177: loss: 6.670578956604004\n",
            "6178: loss: 6.892148494720459\n",
            "6179: loss: 6.495182514190674\n",
            "6180: loss: 6.720931053161621\n",
            "6180: valid loss 6.329761981964111\n",
            "6181: loss: 6.553014755249023\n",
            "6182: loss: 6.572230339050293\n",
            "6183: loss: 6.603602409362793\n",
            "6184: loss: 6.1301589012146\n",
            "6185: loss: 6.972829341888428\n",
            "6186: loss: 6.779409885406494\n",
            "6187: loss: 6.5445556640625\n",
            "6188: loss: 6.509125709533691\n",
            "6189: loss: 6.645101547241211\n",
            "6190: loss: 5.935708522796631\n",
            "6191: loss: 6.621532917022705\n",
            "6192: loss: 6.66388463973999\n",
            "6193: loss: 6.570502281188965\n",
            "6194: loss: 6.147185325622559\n",
            "6195: loss: 6.510846138000488\n",
            "6196: loss: 6.6998114585876465\n",
            "6197: loss: 6.5133514404296875\n",
            "6198: loss: 6.6142754554748535\n",
            "6199: loss: 6.341187953948975\n",
            "6200: loss: 6.816497325897217\n",
            "6200: valid loss 6.458950042724609\n",
            "6200: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "6201: loss: 6.016641139984131\n",
            "6202: loss: 6.469084739685059\n",
            "6203: loss: 6.508740425109863\n",
            "6204: loss: 6.432234764099121\n",
            "6205: loss: 6.295532703399658\n",
            "6206: loss: 6.7605180740356445\n",
            "6207: loss: 6.504215717315674\n",
            "6208: loss: 6.500425815582275\n",
            "6209: loss: 6.581214904785156\n",
            "6210: loss: 6.752978324890137\n",
            "6211: loss: 6.7012434005737305\n",
            "6212: loss: 6.527468681335449\n",
            "6213: loss: 6.327017784118652\n",
            "6214: loss: 6.576087474822998\n",
            "6215: loss: 6.434963226318359\n",
            "6216: loss: 6.453780651092529\n",
            "6217: loss: 6.5382304191589355\n",
            "6218: loss: 6.507899284362793\n",
            "6219: loss: 6.624569416046143\n",
            "6220: loss: 6.55701208114624\n",
            "6220: valid loss 6.522458076477051\n",
            "6221: loss: 6.611067295074463\n",
            "6222: loss: 6.46022367477417\n",
            "6223: loss: 6.440857887268066\n",
            "6224: loss: 6.209774017333984\n",
            "6225: loss: 6.6994757652282715\n",
            "6226: loss: 6.463867664337158\n",
            "6227: loss: 6.3356146812438965\n",
            "6228: loss: 6.626140117645264\n",
            "6229: loss: 6.493437767028809\n",
            "6230: loss: 6.560458660125732\n",
            "6231: loss: 6.469357967376709\n",
            "6232: loss: 6.455918312072754\n",
            "6233: loss: 6.202309608459473\n",
            "6234: loss: 6.497290134429932\n",
            "6235: loss: 5.882789611816406\n",
            "6236: loss: 6.224584102630615\n",
            "6237: loss: 6.458248615264893\n",
            "6238: loss: 6.45897102355957\n",
            "6239: loss: 6.710208415985107\n",
            "6240: loss: 6.583255290985107\n",
            "6240: valid loss 6.8083343505859375\n",
            "6241: loss: 7.039464473724365\n",
            "6242: loss: 6.523965358734131\n",
            "6243: loss: 6.422388076782227\n",
            "6244: loss: 6.692502975463867\n",
            "6245: loss: 6.716861724853516\n",
            "6246: loss: 6.639898777008057\n",
            "6247: loss: 6.817629337310791\n",
            "6248: loss: 6.660961627960205\n",
            "6249: loss: 6.497931480407715\n",
            "6250: loss: 6.636169910430908\n",
            "6251: loss: 6.263528347015381\n",
            "6252: loss: 6.452226161956787\n",
            "6253: loss: 6.586286544799805\n",
            "6254: loss: 6.659060001373291\n",
            "6255: loss: 6.752382755279541\n",
            "6256: loss: 6.540431499481201\n",
            "6257: loss: 6.454392910003662\n",
            "6258: loss: 6.601917266845703\n",
            "6259: loss: 6.300626277923584\n",
            "6260: loss: 6.498574733734131\n",
            "6260: valid loss 6.709639549255371\n",
            "6261: loss: 6.324756145477295\n",
            "6262: loss: 6.491488933563232\n",
            "6263: loss: 6.575965404510498\n",
            "6264: loss: 7.096355438232422\n",
            "6265: loss: 6.477565288543701\n",
            "6266: loss: 6.6029462814331055\n",
            "6267: loss: 6.590596675872803\n",
            "6268: loss: 6.67033576965332\n",
            "6269: loss: 6.580577373504639\n",
            "6270: loss: 6.5296735763549805\n",
            "6271: loss: 6.583427906036377\n",
            "6272: loss: 6.523622989654541\n",
            "6273: loss: 6.457149982452393\n",
            "6274: loss: 6.526559829711914\n",
            "6275: loss: 6.584798812866211\n",
            "6276: loss: 6.664314270019531\n",
            "6277: loss: 6.63162899017334\n",
            "6278: loss: 6.670304775238037\n",
            "6279: loss: 6.58937931060791\n",
            "6280: loss: 6.462812423706055\n",
            "6280: valid loss 6.607511043548584\n",
            "6281: loss: 6.455967426300049\n",
            "6282: loss: 6.373812198638916\n",
            "6283: loss: 6.332600116729736\n",
            "6284: loss: 6.529019355773926\n",
            "6285: loss: 6.5201568603515625\n",
            "6286: loss: 6.195659160614014\n",
            "6287: loss: 6.55279541015625\n",
            "6288: loss: 6.796209335327148\n",
            "6289: loss: 6.536345958709717\n",
            "6290: loss: 6.590559482574463\n",
            "6291: loss: 6.922421455383301\n",
            "6292: loss: 6.261198997497559\n",
            "6293: loss: 6.3166656494140625\n",
            "6294: loss: 6.537790298461914\n",
            "6295: loss: 6.726467609405518\n",
            "6296: loss: 6.382600784301758\n",
            "6297: loss: 6.590283393859863\n",
            "6298: loss: 6.377937316894531\n",
            "6299: loss: 6.742746353149414\n",
            "6300: loss: 6.539512634277344\n",
            "6300: valid loss 6.623356819152832\n",
            "6301: loss: 6.440279483795166\n",
            "6302: loss: 6.395314693450928\n",
            "6303: loss: 6.709849834442139\n",
            "6304: loss: 6.48895263671875\n",
            "6305: loss: 6.544440269470215\n",
            "6306: loss: 6.802957057952881\n",
            "6307: loss: 6.592446804046631\n",
            "6308: loss: 6.692967414855957\n",
            "6309: loss: 6.562393665313721\n",
            "6310: loss: 6.454179286956787\n",
            "6311: loss: 6.456634998321533\n",
            "6312: loss: 6.338421821594238\n",
            "6313: loss: 6.4994940757751465\n",
            "6314: loss: 6.584124565124512\n",
            "6315: loss: 6.491002559661865\n",
            "6316: loss: 6.698399066925049\n",
            "6317: loss: 6.436446666717529\n",
            "6318: loss: 6.443384647369385\n",
            "6319: loss: 6.362829208374023\n",
            "6320: loss: 6.389002323150635\n",
            "6320: valid loss 6.6363325119018555\n",
            "6321: loss: 6.392385482788086\n",
            "6322: loss: 6.722912311553955\n",
            "6323: loss: 6.223175525665283\n",
            "6324: loss: 6.388617515563965\n",
            "6325: loss: 6.825808048248291\n",
            "6326: loss: 6.840550422668457\n",
            "6327: loss: 6.6116156578063965\n",
            "6328: loss: 6.720247268676758\n",
            "6329: loss: 6.206443786621094\n",
            "6330: loss: 6.536826133728027\n",
            "6331: loss: 6.747818946838379\n",
            "6332: loss: 6.733473777770996\n",
            "6333: loss: 6.513250350952148\n",
            "6334: loss: 6.515883922576904\n",
            "6335: loss: 6.254536151885986\n",
            "6336: loss: 6.649735927581787\n",
            "6337: loss: 6.245480060577393\n",
            "6338: loss: 6.682031631469727\n",
            "6339: loss: 6.626472473144531\n",
            "6340: loss: 6.214820861816406\n",
            "6340: valid loss 6.126123428344727\n",
            "6341: loss: 6.539582252502441\n",
            "6342: loss: 6.413983345031738\n",
            "6343: loss: 6.233668804168701\n",
            "6344: loss: 6.585697650909424\n",
            "6345: loss: 6.810730934143066\n",
            "6346: loss: 6.863968372344971\n",
            "6347: loss: 6.435102462768555\n",
            "6348: loss: 6.534008502960205\n",
            "6349: loss: 6.66454553604126\n",
            "6350: loss: 6.76983642578125\n",
            "6351: loss: 6.498889923095703\n",
            "6352: loss: 6.597292900085449\n",
            "6353: loss: 6.503410339355469\n",
            "6354: loss: 6.264740943908691\n",
            "6355: loss: 6.774680137634277\n",
            "6356: loss: 6.502830982208252\n",
            "6357: loss: 6.504482269287109\n",
            "6358: loss: 6.73360538482666\n",
            "6359: loss: 5.790066242218018\n",
            "6360: loss: 6.76911735534668\n",
            "6360: valid loss 6.825862884521484\n",
            "6361: loss: 6.671627998352051\n",
            "6362: loss: 6.5432586669921875\n",
            "6363: loss: 6.726796627044678\n",
            "6364: loss: 6.618653297424316\n",
            "6365: loss: 6.723179340362549\n",
            "6366: loss: 6.548084259033203\n",
            "6367: loss: 6.75505256652832\n",
            "6368: loss: 6.512634754180908\n",
            "6369: loss: 6.528207778930664\n",
            "6370: loss: 6.443860054016113\n",
            "6371: loss: 6.790470123291016\n",
            "6372: loss: 6.975950241088867\n",
            "6373: loss: 6.605774402618408\n",
            "6374: loss: 6.522800922393799\n",
            "6375: loss: 6.311665058135986\n",
            "6376: loss: 6.5504937171936035\n",
            "6377: loss: 6.332536697387695\n",
            "6378: loss: 6.479504585266113\n",
            "6379: loss: 5.6674628257751465\n",
            "6380: loss: 6.337193012237549\n",
            "6380: valid loss 6.778555870056152\n",
            "6381: loss: 5.632752418518066\n",
            "6382: loss: 6.735804080963135\n",
            "6383: loss: 6.308566570281982\n",
            "6384: loss: 6.873348712921143\n",
            "6385: loss: 6.8516645431518555\n",
            "6386: loss: 6.651333332061768\n",
            "6387: loss: 6.482122898101807\n",
            "6388: loss: 6.619808673858643\n",
            "6389: loss: 6.333195686340332\n",
            "6390: loss: 6.284454822540283\n",
            "6391: loss: 6.2054595947265625\n",
            "6392: loss: 6.645402908325195\n",
            "6393: loss: 6.792084217071533\n",
            "6394: loss: 6.481869220733643\n",
            "6395: loss: 6.540833950042725\n",
            "6396: loss: 6.870074272155762\n",
            "6397: loss: 6.391994953155518\n",
            "6398: loss: 6.447461128234863\n",
            "6399: loss: 6.264115333557129\n",
            "6400: loss: 6.574312210083008\n",
            "6400: valid loss 6.373157978057861\n",
            "6400: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "6401: loss: 6.608207702636719\n",
            "6402: loss: 6.635869979858398\n",
            "6403: loss: 6.399629592895508\n",
            "6404: loss: 6.532370567321777\n",
            "6405: loss: 6.654170036315918\n",
            "6406: loss: 6.806262016296387\n",
            "6407: loss: 6.538100242614746\n",
            "6408: loss: 6.520164966583252\n",
            "6409: loss: 6.459068298339844\n",
            "6410: loss: 6.823516845703125\n",
            "6411: loss: 6.428281784057617\n",
            "6412: loss: 6.517894744873047\n",
            "6413: loss: 6.663688659667969\n",
            "6414: loss: 6.310244083404541\n",
            "6415: loss: 6.492271900177002\n",
            "6416: loss: 6.3137993812561035\n",
            "6417: loss: 6.513143539428711\n",
            "6418: loss: 6.422447681427002\n",
            "6419: loss: 6.3385233879089355\n",
            "6420: loss: 6.567353248596191\n",
            "6420: valid loss 6.404248237609863\n",
            "6421: loss: 6.770626068115234\n",
            "6422: loss: 6.338779449462891\n",
            "6423: loss: 6.767944812774658\n",
            "6424: loss: 6.789015769958496\n",
            "6425: loss: 6.280423641204834\n",
            "6426: loss: 6.637374401092529\n",
            "6427: loss: 6.555598258972168\n",
            "6428: loss: 6.345848560333252\n",
            "6429: loss: 6.559777736663818\n",
            "6430: loss: 6.7616963386535645\n",
            "6431: loss: 6.548424243927002\n",
            "6432: loss: 6.607723712921143\n",
            "6433: loss: 6.751008033752441\n",
            "6434: loss: 6.507957935333252\n",
            "6435: loss: 6.5447773933410645\n",
            "6436: loss: 6.870876789093018\n",
            "6437: loss: 6.363592624664307\n",
            "6438: loss: 6.745761394500732\n",
            "6439: loss: 5.950616359710693\n",
            "6440: loss: 6.531092166900635\n",
            "6440: valid loss 6.650856018066406\n",
            "6441: loss: 5.879937648773193\n",
            "6442: loss: 6.812143325805664\n",
            "6443: loss: 6.4254889488220215\n",
            "6444: loss: 6.606346607208252\n",
            "6445: loss: 6.554648399353027\n",
            "6446: loss: 6.236727237701416\n",
            "6447: loss: 6.575318813323975\n",
            "6448: loss: 6.274534225463867\n",
            "6449: loss: 5.978934288024902\n",
            "6450: loss: 6.484813213348389\n",
            "6451: loss: 6.497575283050537\n",
            "6452: loss: 6.70395565032959\n",
            "6453: loss: 6.380795955657959\n",
            "6454: loss: 6.601212501525879\n",
            "6455: loss: 6.720929145812988\n",
            "6456: loss: 6.264567852020264\n",
            "6457: loss: 6.756052494049072\n",
            "6458: loss: 5.877016544342041\n",
            "6459: loss: 6.653572082519531\n",
            "6460: loss: 6.161569118499756\n",
            "6460: valid loss 6.431309700012207\n",
            "6461: loss: 6.562321662902832\n",
            "6462: loss: 6.767213821411133\n",
            "6463: loss: 6.628653526306152\n",
            "6464: loss: 6.652059078216553\n",
            "6465: loss: 6.736403465270996\n",
            "6466: loss: 6.520590782165527\n",
            "6467: loss: 6.650516033172607\n",
            "6468: loss: 6.539435386657715\n",
            "6469: loss: 6.741397380828857\n",
            "6470: loss: 6.901660442352295\n",
            "6471: loss: 6.485805988311768\n",
            "6472: loss: 6.4159159660339355\n",
            "6473: loss: 5.843754768371582\n",
            "6474: loss: 6.379030227661133\n",
            "6475: loss: 6.4282612800598145\n",
            "6476: loss: 6.3928022384643555\n",
            "6477: loss: 6.174822807312012\n",
            "6478: loss: 6.455305576324463\n",
            "6479: loss: 6.449822425842285\n",
            "6480: loss: 6.4807353019714355\n",
            "6480: valid loss 6.423655986785889\n",
            "6481: loss: 6.5629563331604\n",
            "6482: loss: 6.531711101531982\n",
            "6483: loss: 6.2236433029174805\n",
            "6484: loss: 6.342663764953613\n",
            "6485: loss: 6.90622091293335\n",
            "6486: loss: 6.448962688446045\n",
            "6487: loss: 6.555403232574463\n",
            "6488: loss: 6.2196269035339355\n",
            "6489: loss: 6.505189418792725\n",
            "6490: loss: 6.538962364196777\n",
            "6491: loss: 6.43677282333374\n",
            "6492: loss: 6.57573938369751\n",
            "6493: loss: 6.608025550842285\n",
            "6494: loss: 6.728387832641602\n",
            "6495: loss: 6.515226364135742\n",
            "6496: loss: 6.323913097381592\n",
            "6497: loss: 6.865696907043457\n",
            "6498: loss: 6.645936489105225\n",
            "6499: loss: 6.656142711639404\n",
            "6500: loss: 6.589028358459473\n",
            "6500: valid loss 6.522949695587158\n",
            "6501: loss: 6.494359016418457\n",
            "6502: loss: 6.290316581726074\n",
            "6503: loss: 6.345865726470947\n",
            "6504: loss: 6.310212135314941\n",
            "6505: loss: 6.362537860870361\n",
            "6506: loss: 6.562719345092773\n",
            "6507: loss: 6.501130104064941\n",
            "6508: loss: 4.628121376037598\n",
            "6509: loss: 6.394507884979248\n",
            "6510: loss: 6.318111896514893\n",
            "6511: loss: 6.636538505554199\n",
            "6512: loss: 6.31456184387207\n",
            "6513: loss: 6.708466529846191\n",
            "6514: loss: 6.622186183929443\n",
            "6515: loss: 6.848752498626709\n",
            "6516: loss: 6.611611366271973\n",
            "6517: loss: 6.2609148025512695\n",
            "6518: loss: 6.575139999389648\n",
            "6519: loss: 6.654052257537842\n",
            "6520: loss: 6.704261302947998\n",
            "6520: valid loss 6.585778713226318\n",
            "6521: loss: 6.495562553405762\n",
            "6522: loss: 6.205376625061035\n",
            "6523: loss: 6.444119930267334\n",
            "6524: loss: 6.412603855133057\n",
            "6525: loss: 6.241334438323975\n",
            "6526: loss: 6.42418098449707\n",
            "6527: loss: 6.772742748260498\n",
            "6528: loss: 6.685147762298584\n",
            "6529: loss: 6.098092555999756\n",
            "6530: loss: 6.337745189666748\n",
            "6531: loss: 6.655522346496582\n",
            "6532: loss: 6.708588123321533\n",
            "6533: loss: 6.976271152496338\n",
            "6534: loss: 6.481174468994141\n",
            "6535: loss: 6.706310749053955\n",
            "6536: loss: 6.758606433868408\n",
            "6537: loss: 6.453707218170166\n",
            "6538: loss: 6.558323860168457\n",
            "6539: loss: 6.443887710571289\n",
            "6540: loss: 6.445614337921143\n",
            "6540: valid loss 6.45775032043457\n",
            "6541: loss: 6.669729709625244\n",
            "6542: loss: 6.752963066101074\n",
            "6543: loss: 6.615281581878662\n",
            "6544: loss: 6.595161437988281\n",
            "6545: loss: 6.742976665496826\n",
            "6546: loss: 6.422267913818359\n",
            "6547: loss: 6.630941390991211\n",
            "6548: loss: 6.436582565307617\n",
            "6549: loss: 6.34499979019165\n",
            "6550: loss: 6.448711395263672\n",
            "6551: loss: 6.521071910858154\n",
            "6552: loss: 6.554183483123779\n",
            "6553: loss: 6.694197654724121\n",
            "6554: loss: 6.5965800285339355\n",
            "6555: loss: 6.58482551574707\n",
            "6556: loss: 6.59635591506958\n",
            "6557: loss: 6.556825160980225\n",
            "6558: loss: 6.537640571594238\n",
            "6559: loss: 6.514069557189941\n",
            "6560: loss: 6.365020275115967\n",
            "6560: valid loss 6.581851959228516\n",
            "6561: loss: 6.424493312835693\n",
            "6562: loss: 6.320130348205566\n",
            "6563: loss: 6.233985424041748\n",
            "6564: loss: 6.648889541625977\n",
            "6565: loss: 6.234031677246094\n",
            "6566: loss: 6.02531099319458\n",
            "6567: loss: 6.372627258300781\n",
            "6568: loss: 6.563074588775635\n",
            "6569: loss: 6.445568084716797\n",
            "6570: loss: 6.042471408843994\n",
            "6571: loss: 6.695505619049072\n",
            "6572: loss: 6.362321376800537\n",
            "6573: loss: 6.472811698913574\n",
            "6574: loss: 6.661584854125977\n",
            "6575: loss: 6.093336582183838\n",
            "6576: loss: 6.384040355682373\n",
            "6577: loss: 6.741453647613525\n",
            "6578: loss: 6.319148540496826\n",
            "6579: loss: 6.347163677215576\n",
            "6580: loss: 6.482812404632568\n",
            "6580: valid loss 5.995418071746826\n",
            "6581: loss: 6.59388542175293\n",
            "6582: loss: 6.417298316955566\n",
            "6583: loss: 6.428715705871582\n",
            "6584: loss: 6.72256326675415\n",
            "6585: loss: 6.698791980743408\n",
            "6586: loss: 6.393579483032227\n",
            "6587: loss: 6.1565117835998535\n",
            "6588: loss: 6.187939643859863\n",
            "6589: loss: 6.634718418121338\n",
            "6590: loss: 6.545664310455322\n",
            "6591: loss: 6.607550144195557\n",
            "6592: loss: 6.087911128997803\n",
            "6593: loss: 6.213170528411865\n",
            "6594: loss: 6.4212422370910645\n",
            "6595: loss: 6.376076698303223\n",
            "6596: loss: 6.689594745635986\n",
            "6597: loss: 6.200642108917236\n",
            "6598: loss: 6.580013751983643\n",
            "6599: loss: 6.1190409660339355\n",
            "6600: loss: 6.690465450286865\n",
            "6600: valid loss 6.446875095367432\n",
            "6600: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_coarse\n",
            "6601: loss: 6.591856956481934\n",
            "6602: loss: 1.0731916427612305\n",
            "6603: loss: 6.716792106628418\n",
            "6604: loss: 6.034053802490234\n",
            "6605: loss: 6.793252468109131\n",
            "6606: loss: 6.5067572593688965\n",
            "6607: loss: 6.283862590789795\n",
            "6608: loss: 6.784295558929443\n",
            "6609: loss: 6.6360392570495605\n",
            "6610: loss: 6.588390827178955\n",
            "6611: loss: 6.4076666831970215\n",
            "6612: loss: 6.421492576599121\n",
            "6613: loss: 6.620625019073486\n",
            "6614: loss: 6.575887203216553\n",
            "6615: loss: 6.756313323974609\n",
            "6616: loss: 6.464001178741455\n",
            "6617: loss: 6.465000152587891\n",
            "6618: loss: 6.56354284286499\n",
            "6619: loss: 6.884636878967285\n",
            "6620: loss: 6.2942986488342285\n",
            "6620: valid loss 6.784849643707275\n",
            "6621: loss: 6.4803290367126465\n",
            "6622: loss: 6.515911102294922\n",
            "6623: loss: 6.916853427886963\n",
            "6624: loss: 6.321127891540527\n",
            "6625: loss: 6.57871150970459\n",
            "6626: loss: 6.750306606292725\n",
            "6627: loss: 6.5113525390625\n",
            "6628: loss: 6.438412189483643\n",
            "6629: loss: 6.269687652587891\n",
            "6630: loss: 6.516787528991699\n",
            "6631: loss: 6.45978307723999\n",
            "6632: loss: 6.5722784996032715\n",
            "6633: loss: 6.572788715362549\n",
            "6634: loss: 6.660638809204102\n",
            "6635: loss: 6.431943416595459\n",
            "6636: loss: 6.563818454742432\n",
            "6637: loss: 6.548224449157715\n",
            "6638: loss: 6.663227558135986\n",
            "6639: loss: 6.494089126586914\n",
            "6640: loss: 6.517417907714844\n",
            "6640: valid loss 6.857000350952148\n",
            "6641: loss: 6.251960277557373\n",
            "6642: loss: 5.824685573577881\n",
            "6643: loss: 6.224422454833984\n",
            "6644: loss: 6.250847339630127\n",
            "6645: loss: 6.251311779022217\n",
            "6646: loss: 6.518978595733643\n",
            "6647: loss: 6.512846946716309\n",
            "6648: loss: 6.074767112731934\n",
            "6649: loss: 5.945937633514404\n",
            "6650: loss: 6.422799110412598\n",
            "6651: loss: 6.017546653747559\n",
            "6652: loss: 6.210422039031982\n",
            "6653: loss: 6.550424575805664\n",
            "6654: loss: 6.481845378875732\n",
            "6655: loss: 6.594470977783203\n",
            "6656: loss: 6.585725784301758\n",
            "6657: loss: 6.2440009117126465\n",
            "6658: loss: 6.055532932281494\n",
            "6659: loss: 6.460147857666016\n",
            "6660: loss: 6.354443073272705\n",
            "6660: valid loss 6.7099528312683105\n",
            "6661: loss: 6.533368110656738\n",
            "6662: loss: 6.153641223907471\n",
            "6663: loss: 6.177123069763184\n",
            "6664: loss: 6.409482002258301\n",
            "6665: loss: 6.1630754470825195\n",
            "6666: loss: 6.3489603996276855\n",
            "6667: loss: 6.873662948608398\n",
            "6668: loss: 6.288600921630859\n",
            "6669: loss: 7.785729885101318\n",
            "6670: loss: 6.486666679382324\n",
            "6671: loss: 5.671257495880127\n",
            "6672: loss: 6.517484188079834\n",
            "6673: loss: 6.539090156555176\n",
            "6674: loss: 6.879918098449707\n",
            "6675: loss: 6.588723659515381\n",
            "6676: loss: 6.664709091186523\n",
            "6677: loss: 6.326382160186768\n",
            "6678: loss: 6.010180473327637\n",
            "6679: loss: 6.004573345184326\n",
            "6680: loss: 6.497790336608887\n",
            "6680: valid loss 6.468143939971924\n",
            "6681: loss: 6.63021183013916\n",
            "6682: loss: 6.083004951477051\n",
            "6683: loss: 6.774285793304443\n",
            "6684: loss: 6.274184226989746\n",
            "6685: loss: 6.332941055297852\n",
            "6686: loss: 6.4976348876953125\n",
            "6687: loss: 6.595514297485352\n",
            "6688: loss: 6.554378509521484\n",
            "6689: loss: 6.2101731300354\n",
            "6690: loss: 6.428380489349365\n",
            "6691: loss: 6.533138275146484\n",
            "6692: loss: 6.615796089172363\n",
            "6693: loss: 6.430313587188721\n",
            "6694: loss: 6.5989155769348145\n",
            "6695: loss: 6.153974533081055\n",
            "6696: loss: 6.564319610595703\n",
            "6697: loss: 6.431310653686523\n",
            "6698: loss: 6.400060653686523\n",
            "6699: loss: 6.617125511169434\n",
            "6700: loss: 6.493105411529541\n",
            "6700: valid loss 6.732800483703613\n",
            "6701: loss: 6.760608196258545\n",
            "6702: loss: 6.119022369384766\n",
            "6703: loss: 6.710635662078857\n",
            "6704: loss: 6.561381816864014\n",
            "6705: loss: 6.5868239402771\n",
            "6706: loss: 6.326644420623779\n",
            "6707: loss: 6.750868320465088\n",
            "6708: loss: 6.703237056732178\n",
            "6709: loss: 6.541019439697266\n",
            "6710: loss: 6.363654136657715\n",
            "6711: loss: 6.225045204162598\n",
            "6712: loss: 6.116952419281006\n",
            "6713: loss: 6.748950004577637\n",
            "6714: loss: 6.3549041748046875\n",
            "6715: loss: 6.502847194671631\n",
            "6716: loss: 5.681662559509277\n",
            "6717: loss: 6.513948917388916\n",
            "6718: loss: 6.6172637939453125\n",
            "6719: loss: 6.597443103790283\n",
            "6720: loss: 6.780305862426758\n",
            "6720: valid loss 5.933596134185791\n",
            "6721: loss: 6.233856201171875\n",
            "6722: loss: 6.264020919799805\n",
            "6723: loss: 6.036402702331543\n",
            "6724: loss: 6.8158392906188965\n",
            "6725: loss: 6.644514560699463\n",
            "6726: loss: 6.253271579742432\n",
            "6727: loss: 6.561508655548096\n",
            "6728: loss: 6.560923099517822\n",
            "6729: loss: 6.515377998352051\n",
            "6730: loss: 6.135486602783203\n",
            "6731: loss: 6.608963966369629\n",
            "6732: loss: 6.431151390075684\n",
            "6733: loss: 6.5947265625\n",
            "6734: loss: 6.243068218231201\n",
            "6735: loss: 6.55419397354126\n",
            "6736: loss: 6.348724365234375\n",
            "6737: loss: 6.315394878387451\n",
            "6738: loss: 6.594074249267578\n",
            "6739: loss: 6.655991077423096\n",
            "6740: loss: 6.335110187530518\n",
            "6740: valid loss 6.2888922691345215\n",
            "6741: loss: 6.244982719421387\n",
            "6742: loss: 6.461301326751709\n",
            "6743: loss: 6.4304094314575195\n",
            "6744: loss: 6.429056644439697\n",
            "6745: loss: 6.498724460601807\n",
            "6746: loss: 6.677433013916016\n",
            "6747: loss: 6.699517726898193\n",
            "6748: loss: 6.401170253753662\n",
            "6749: loss: 6.545162677764893\n",
            "6750: loss: 6.418197154998779\n",
            "6751: loss: 6.43366003036499\n",
            "6752: loss: 6.142906188964844\n",
            "6753: loss: 6.785287857055664\n",
            "6754: loss: 6.615769386291504\n",
            "6755: loss: 6.656985759735107\n",
            "6756: loss: 5.920335292816162\n",
            "6757: loss: 5.8794169425964355\n",
            "6758: loss: 6.516303539276123\n",
            "6759: loss: 6.163415431976318\n",
            "6760: loss: 6.929977893829346\n",
            "6760: valid loss 6.451961517333984\n",
            "6761: loss: 6.6969780921936035\n",
            "6762: loss: 6.531245708465576\n",
            "6763: loss: 6.5061726570129395\n",
            "6764: loss: 6.615965366363525\n",
            "6765: loss: 6.019184112548828\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 41>\u001b[0m:\u001b[94m42\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mcustom_train\u001b[0m:\u001b[94m15\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/audiolm_pytorch/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m965\u001b[0m in \u001b[92mtrain_step\u001b[0m             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 962 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mreturn_loss = \u001b[94mTrue\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 963 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 964 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 965 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.accelerator.backward(loss / \u001b[96mself\u001b[0m.grad_accum_every)                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 966 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 967 \u001b[0m\u001b[2m│   │   │   \u001b[0maccum_log(logs, {\u001b[33m'\u001b[0m\u001b[33mloss\u001b[0m\u001b[33m'\u001b[0m: loss.item() / \u001b[96mself\u001b[0m.grad_accum_every})                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 968 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/accelerate/\u001b[0m\u001b[1;33maccelerator.py\u001b[0m:\u001b[94m1683\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1680 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.scaler \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1681 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.scaler.scale(loss).backward(**kwargs)                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1682 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1683 \u001b[2m│   │   │   \u001b[0mloss.backward(**kwargs)                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1684 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1685 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92munscale_gradients\u001b[0m(\u001b[96mself\u001b[0m, optimizer=\u001b[94mNone\u001b[0m):                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1686 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 41&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">42</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">custom_train</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">15</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/audiolm_pytorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">965</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_step</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 962 │   │   │   │   </span>return_loss = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 963 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 964 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 965 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.backward(loss / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.grad_accum_every)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 966 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 967 │   │   │   </span>accum_log(logs, {<span style=\"color: #808000; text-decoration-color: #808000\">'loss'</span>: loss.item() / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.grad_accum_every})                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 968 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">accelerator.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1683</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1680 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1681 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.scale(loss).backward(**kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1682 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1683 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward(**kwargs)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1684 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1685 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">unscale_gradients</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>):                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1686 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers.trainer_utils import SchedulerType\n",
        "torch.cuda.empty_cache()\n",
        "coarse_transformer = CoarseTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    codebook_size = 1024,\n",
        "    num_coarse_quantizers = 3,\n",
        "    dim = config[\"embedding_dim\"],\n",
        "    depth = config[\"depth\"],\n",
        "    heads = config[\"heads\"],\n",
        "    audio_text_condition = True,\n",
        "    attn_dropout = config[\"attn_dropout\"],\n",
        "    ff_dropout = config[\"ff_dropout\"],\n",
        ")\n",
        "\n",
        "coarse_trainer = CoarseTransformerTrainer(\n",
        "    transformer = coarse_transformer,\n",
        "    codec = soundstream,\n",
        "    wav2vec = wav2vec,\n",
        "    audio_conditioner = quantizer,\n",
        "    folder = dataset_folder,\n",
        "    lr=config[\"lr\"],\n",
        "    batch_size = config[\"batch_size\"],\n",
        "    data_max_length_seconds = config.get(\"data_max_length_seconds\"),\n",
        "    data_max_length = config.get(\"data_max_length\"),\n",
        "    save_results_every = VAL_EVERY_STEPS,\n",
        "    wd = config[\"wd\"],\n",
        "    grad_accum_every = config[\"grad_accum_every\"],\n",
        "    max_grad_norm = config[\"max_grad_norm\"],\n",
        "    save_model_every = SAVE_MODEL_EVERY,\n",
        "    num_train_steps = STEPS,\n",
        "    results_folder = RESULTS_ROOT_PATH+'/results_coarse',\n",
        "    force_clear_prev_results = False,\n",
        "    accelerate_kwargs = get_accelerate_kwargs(config)\n",
        ")\n",
        "\n",
        "scheduler = get_scheduler(coarse_trainer.optim, config)\n",
        "print(scheduler)\n",
        "if load_ckpt:\n",
        "  coarse_trainer.load(coarse_load_path)\n",
        "\n",
        "if train:\n",
        "  custom_train(coarse_trainer,\n",
        "               config,\n",
        "               scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRvj7qOJWzmw"
      },
      "source": [
        "### FineTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kacUP_8DHOVy"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  if train: \n",
        "    free_gpu_memory(fine_transformer)\n",
        "    free_gpu_memory(coarse_transformer)\n",
        "    del coarse_transformer, coarse_trainer\n",
        "except:\n",
        "  pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZRaEhRRKWg8F",
        "outputId": "2ffe7953-2328-401c-d340-cf62115a12ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training with dataset of 207 samples and validating with randomly splitted 11 samples\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230501_083023-1gkfhyxn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ols/fine/runs/1gkfhyxn' target=\"_blank\">whole-terrain-14</a></strong> to <a href='https://wandb.ai/ols/fine' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ols/fine' target=\"_blank\">https://wandb.ai/ols/fine</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ols/fine/runs/1gkfhyxn' target=\"_blank\">https://wandb.ai/ols/fine/runs/1gkfhyxn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: loss: 101.54593658447266\n",
            "0: valid loss 322.3030700683594\n",
            "0: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_fine\n",
            "1: loss: 308.6904296875\n",
            "2: loss: 107.06201934814453\n",
            "3: loss: 117.20203399658203\n",
            "4: loss: 98.0593490600586\n",
            "5: loss: 86.17723846435547\n",
            "6: loss: 73.23721313476562\n",
            "7: loss: 60.25212860107422\n",
            "8: loss: 65.65185546875\n",
            "9: loss: 39.11012268066406\n",
            "10: loss: 56.667179107666016\n",
            "11: loss: 35.663360595703125\n",
            "12: loss: 78.9594955444336\n",
            "13: loss: 73.133544921875\n",
            "14: loss: 56.949462890625\n",
            "15: loss: 68.40687561035156\n",
            "16: loss: 65.73347473144531\n",
            "17: loss: 64.81417083740234\n",
            "18: loss: 50.48391342163086\n",
            "19: loss: 61.67243957519531\n",
            "20: loss: 60.393409729003906\n",
            "20: valid loss 46.88066864013672\n",
            "21: loss: 58.63340377807617\n",
            "22: loss: 56.05731964111328\n",
            "23: loss: 55.43968963623047\n",
            "24: loss: 44.234622955322266\n",
            "25: loss: 54.661869049072266\n",
            "26: loss: 53.57736587524414\n",
            "27: loss: 51.458099365234375\n",
            "28: loss: 41.613739013671875\n",
            "29: loss: 39.04848098754883\n",
            "30: loss: 56.880130767822266\n",
            "31: loss: 50.06425476074219\n",
            "32: loss: 48.549739837646484\n",
            "33: loss: 40.27046203613281\n",
            "34: loss: 39.27682876586914\n",
            "35: loss: 48.03751754760742\n",
            "36: loss: 46.42497253417969\n",
            "37: loss: 46.30478286743164\n",
            "38: loss: 34.83402633666992\n",
            "39: loss: 34.300479888916016\n",
            "40: loss: 44.926856994628906\n",
            "40: valid loss 27.98839569091797\n",
            "41: loss: 7.091862678527832\n",
            "42: loss: 34.75084686279297\n",
            "43: loss: 33.68757247924805\n",
            "44: loss: 43.99396896362305\n",
            "45: loss: 41.96709442138672\n",
            "46: loss: 40.95492172241211\n",
            "47: loss: 34.27244567871094\n",
            "48: loss: 40.20986557006836\n",
            "49: loss: 36.647979736328125\n",
            "50: loss: 38.32140350341797\n",
            "50: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_fine\n",
            "51: loss: 38.10813522338867\n",
            "52: loss: 30.82183074951172\n",
            "53: loss: 29.589771270751953\n",
            "54: loss: 25.747716903686523\n",
            "55: loss: 60.991546630859375\n",
            "56: loss: 107.84825897216797\n",
            "57: loss: 60.84410858154297\n",
            "58: loss: 50.3091926574707\n",
            "59: loss: 43.72136306762695\n",
            "60: loss: 40.57718276977539\n",
            "60: valid loss 27.952335357666016\n",
            "61: loss: 38.04367446899414\n",
            "62: loss: 34.67672348022461\n",
            "63: loss: 36.83267593383789\n",
            "64: loss: 36.964115142822266\n",
            "65: loss: 35.14218521118164\n",
            "66: loss: 34.19615936279297\n",
            "67: loss: 32.84956359863281\n",
            "68: loss: 33.64310836791992\n",
            "69: loss: 33.64011001586914\n",
            "70: loss: 28.103317260742188\n",
            "71: loss: 27.03510093688965\n",
            "72: loss: 36.34697723388672\n",
            "73: loss: 26.54912567138672\n",
            "74: loss: 25.102813720703125\n",
            "75: loss: 32.670021057128906\n",
            "76: loss: 30.927976608276367\n",
            "77: loss: 35.97976303100586\n",
            "78: loss: 31.487688064575195\n",
            "79: loss: 48.286720275878906\n",
            "80: loss: 29.75590705871582\n",
            "80: valid loss 18.163923263549805\n",
            "81: loss: 29.10959243774414\n",
            "82: loss: 25.634033203125\n",
            "Epoch 00083: reducing learning rate of group 0 to 7.0000e-04.\n",
            "83: loss: 29.79680824279785\n",
            "84: loss: 30.923871994018555\n",
            "85: loss: 29.530113220214844\n",
            "86: loss: 29.354372024536133\n",
            "87: loss: 27.784128189086914\n",
            "88: loss: 26.663299560546875\n",
            "89: loss: 33.78031921386719\n",
            "90: loss: 25.12476921081543\n",
            "91: loss: 25.26650619506836\n",
            "92: loss: 31.315317153930664\n",
            "93: loss: 21.62797737121582\n",
            "94: loss: 28.764007568359375\n",
            "95: loss: 27.340526580810547\n",
            "96: loss: 20.663633346557617\n",
            "97: loss: 24.493616104125977\n",
            "98: loss: 16.158313751220703\n",
            "99: loss: 33.04817199707031\n",
            "100: loss: 33.83627700805664\n",
            "100: valid loss 20.291929244995117\n",
            "100: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_fine\n",
            "101: loss: 30.748950958251953\n",
            "102: loss: 22.339340209960938\n",
            "103: loss: 28.463298797607422\n",
            "104: loss: 23.336654663085938\n",
            "105: loss: 26.889570236206055\n",
            "106: loss: 22.145456314086914\n",
            "107: loss: 20.956422805786133\n",
            "108: loss: 28.54277801513672\n",
            "109: loss: 26.90386390686035\n",
            "110: loss: 25.530187606811523\n",
            "111: loss: 25.652904510498047\n",
            "112: loss: 25.274200439453125\n",
            "113: loss: 22.414445877075195\n",
            "114: loss: 21.13325309753418\n",
            "115: loss: 27.0208683013916\n",
            "116: loss: 20.317359924316406\n",
            "117: loss: 26.65433692932129\n",
            "118: loss: 25.022705078125\n",
            "119: loss: 21.16033172607422\n",
            "120: loss: 23.655366897583008\n",
            "120: valid loss 14.237567901611328\n",
            "121: loss: 23.021902084350586\n",
            "122: loss: 16.631118774414062\n",
            "123: loss: 23.750778198242188\n",
            "124: loss: 23.150365829467773\n",
            "125: loss: 22.338420867919922\n",
            "126: loss: 20.25139617919922\n",
            "127: loss: 22.63002586364746\n",
            "128: loss: 21.799518585205078\n",
            "Epoch 00129: reducing learning rate of group 0 to 4.9000e-04.\n",
            "129: loss: 20.920202255249023\n",
            "130: loss: 21.37647247314453\n",
            "131: loss: 20.840909957885742\n",
            "132: loss: 20.32550048828125\n",
            "133: loss: 20.45925521850586\n",
            "134: loss: 20.581867218017578\n",
            "135: loss: 21.862674713134766\n",
            "136: loss: 17.914289474487305\n",
            "137: loss: 21.511898040771484\n",
            "138: loss: 11.115537643432617\n",
            "139: loss: 18.137001037597656\n",
            "140: loss: 17.43406867980957\n",
            "140: valid loss 19.17742156982422\n",
            "141: loss: 22.793725967407227\n",
            "142: loss: 22.193965911865234\n",
            "143: loss: 21.03949737548828\n",
            "144: loss: 18.54038429260254\n",
            "145: loss: 20.37346649169922\n",
            "146: loss: 21.110132217407227\n",
            "147: loss: 20.31513786315918\n",
            "148: loss: 19.649879455566406\n",
            "149: loss: 19.400306701660156\n",
            "150: loss: 20.662742614746094\n",
            "150: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_fine\n",
            "151: loss: 19.557018280029297\n",
            "152: loss: 18.303110122680664\n",
            "153: loss: 20.36765480041504\n",
            "154: loss: 19.91145896911621\n",
            "155: loss: 16.033292770385742\n",
            "156: loss: 15.951553344726562\n",
            "157: loss: 15.407651901245117\n",
            "158: loss: 21.567501068115234\n",
            "159: loss: 22.64746856689453\n",
            "160: loss: 15.505062103271484\n",
            "160: valid loss 14.33497142791748\n",
            "161: loss: 20.542104721069336\n",
            "162: loss: 16.351552963256836\n",
            "163: loss: 19.90839195251465\n",
            "164: loss: 19.2335262298584\n",
            "165: loss: 18.375591278076172\n",
            "166: loss: 16.602672576904297\n",
            "167: loss: 19.629558563232422\n",
            "168: loss: 19.08083724975586\n",
            "169: loss: 19.71873664855957\n",
            "170: loss: 16.819171905517578\n",
            "171: loss: 19.342674255371094\n",
            "172: loss: 15.372527122497559\n",
            "173: loss: 18.990943908691406\n",
            "174: loss: 18.988508224487305\n",
            "Epoch 00175: reducing learning rate of group 0 to 3.4300e-04.\n",
            "175: loss: 15.587932586669922\n",
            "176: loss: 18.941879272460938\n",
            "177: loss: 18.349864959716797\n",
            "178: loss: 18.420452117919922\n",
            "179: loss: 18.27713966369629\n",
            "180: loss: 17.20871925354004\n",
            "180: valid loss 7.482067108154297\n",
            "181: loss: 17.57640838623047\n",
            "182: loss: 18.137483596801758\n",
            "183: loss: 18.363422393798828\n",
            "184: loss: 18.378599166870117\n",
            "185: loss: 18.19538116455078\n",
            "186: loss: 18.398176193237305\n",
            "187: loss: 17.776790618896484\n",
            "188: loss: 16.357933044433594\n",
            "189: loss: 17.982925415039062\n",
            "190: loss: 14.96727466583252\n",
            "191: loss: 14.298111915588379\n",
            "192: loss: 20.357789993286133\n",
            "193: loss: 22.005464553833008\n",
            "194: loss: 9.40433406829834\n",
            "195: loss: 14.287806510925293\n",
            "196: loss: 14.433679580688477\n",
            "197: loss: 16.363391876220703\n",
            "198: loss: 19.852109909057617\n",
            "199: loss: 14.700423240661621\n",
            "200: loss: 17.981491088867188\n",
            "200: valid loss 9.353034019470215\n",
            "200: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_fine\n",
            "201: loss: 18.33018684387207\n",
            "202: loss: 17.675325393676758\n",
            "203: loss: 17.641399383544922\n",
            "204: loss: 17.19489097595215\n",
            "205: loss: 15.972131729125977\n",
            "206: loss: 15.216907501220703\n",
            "207: loss: 15.508025169372559\n",
            "208: loss: 18.887453079223633\n",
            "209: loss: 18.721248626708984\n",
            "210: loss: 13.988896369934082\n",
            "211: loss: 15.700525283813477\n",
            "212: loss: 23.198177337646484\n",
            "213: loss: 19.362974166870117\n",
            "214: loss: 20.853858947753906\n",
            "215: loss: 15.213871955871582\n",
            "216: loss: 15.155878067016602\n",
            "217: loss: 14.520464897155762\n",
            "218: loss: 14.819108009338379\n",
            "219: loss: 19.297134399414062\n",
            "220: loss: 19.103965759277344\n",
            "220: valid loss 11.03951644897461\n",
            "Epoch 00221: reducing learning rate of group 0 to 2.4010e-04.\n",
            "221: loss: 17.5862979888916\n",
            "222: loss: 17.920047760009766\n",
            "223: loss: 17.506929397583008\n",
            "224: loss: 14.333512306213379\n",
            "225: loss: 17.441783905029297\n",
            "226: loss: 17.546424865722656\n",
            "227: loss: 17.96479034423828\n",
            "228: loss: 16.804119110107422\n",
            "229: loss: 13.621474266052246\n",
            "230: loss: 16.67193603515625\n",
            "231: loss: 13.599534034729004\n",
            "232: loss: 13.100022315979004\n",
            "233: loss: 17.610368728637695\n",
            "234: loss: 17.17656135559082\n",
            "235: loss: 16.965972900390625\n",
            "236: loss: 12.325187683105469\n",
            "237: loss: 12.176156997680664\n",
            "238: loss: 16.764623641967773\n",
            "239: loss: 8.538427352905273\n",
            "240: loss: 16.598854064941406\n",
            "240: valid loss 8.277867317199707\n",
            "241: loss: 16.813159942626953\n",
            "242: loss: 16.13669776916504\n",
            "243: loss: 20.29470443725586\n",
            "244: loss: 16.358444213867188\n",
            "245: loss: 16.13723373413086\n",
            "246: loss: 16.12298583984375\n",
            "247: loss: 16.10546875\n",
            "248: loss: 11.615431785583496\n",
            "249: loss: 16.106342315673828\n",
            "250: loss: 12.669283866882324\n",
            "250: saving model to /content/gdrive/MyDrive/IDL_Project/models/results_fine\n",
            "251: loss: 16.11626625061035\n",
            "252: loss: 12.20798397064209\n",
            "253: loss: 15.71507453918457\n",
            "254: loss: 11.664981842041016\n",
            "255: loss: 11.45191764831543\n",
            "256: loss: 11.050459861755371\n",
            "257: loss: 10.94775390625\n",
            "258: loss: 15.308424949645996\n",
            "259: loss: 11.030890464782715\n",
            "260: loss: 15.61223316192627\n",
            "260: valid loss 6.513073921203613\n",
            "261: loss: 15.035343170166016\n",
            "262: loss: 14.936589241027832\n",
            "263: loss: 15.235963821411133\n",
            "264: loss: 15.17475700378418\n",
            "265: loss: 14.871689796447754\n",
            "266: loss: 7.528777122497559\n",
            "Epoch 00267: reducing learning rate of group 0 to 1.6807e-04.\n",
            "267: loss: 10.185773849487305\n",
            "268: loss: 6.416925430297852\n",
            "269: loss: 14.26892375946045\n",
            "270: loss: 12.58237075805664\n",
            "271: loss: 14.631062507629395\n",
            "272: loss: 14.514212608337402\n",
            "273: loss: 14.197354316711426\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 38&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">39</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">custom_train</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">15</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/audiolm_pytorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1212</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1209 │   │   │   </span>data_kwargs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.data_tuple_to_kwargs(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dl_iter))                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1210 │   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.train_wrapper(**data_kwargs, return_loss = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1211 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1212 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.backward(loss / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.grad_accum_every)                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1213 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1214 │   │   │   </span>accum_log(logs, {<span style=\"color: #808000; text-decoration-color: #808000\">'loss'</span>: loss.item() / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.grad_accum_every})                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1215 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">accelerator.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1683</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1680 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1681 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.scale(loss).backward(**kwargs)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1682 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1683 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss.backward(**kwargs)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1684 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1685 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">unscale_gradients</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>):                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1686 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">487</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 484 │   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 │   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 487 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 │   # some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   # calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>200 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">203 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 38>\u001b[0m:\u001b[94m39\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mcustom_train\u001b[0m:\u001b[94m15\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/audiolm_pytorch/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1212\u001b[0m in \u001b[92mtrain_step\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1209 \u001b[0m\u001b[2m│   │   │   \u001b[0mdata_kwargs = \u001b[96mself\u001b[0m.data_tuple_to_kwargs(\u001b[96mnext\u001b[0m(\u001b[96mself\u001b[0m.dl_iter))                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1210 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.train_wrapper(**data_kwargs, return_loss = \u001b[94mTrue\u001b[0m)                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1211 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1212 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.accelerator.backward(loss / \u001b[96mself\u001b[0m.grad_accum_every)                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1213 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1214 \u001b[0m\u001b[2m│   │   │   \u001b[0maccum_log(logs, {\u001b[33m'\u001b[0m\u001b[33mloss\u001b[0m\u001b[33m'\u001b[0m: loss.item() / \u001b[96mself\u001b[0m.grad_accum_every})                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1215 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/accelerate/\u001b[0m\u001b[1;33maccelerator.py\u001b[0m:\u001b[94m1683\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1680 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.scaler \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1681 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.scaler.scale(loss).backward(**kwargs)                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1682 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1683 \u001b[2m│   │   │   \u001b[0mloss.backward(**kwargs)                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1684 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1685 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92munscale_gradients\u001b[0m(\u001b[96mself\u001b[0m, optimizer=\u001b[94mNone\u001b[0m):                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1686 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m487\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 487 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m200\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m200 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "fine_transformer = FineTransformer(\n",
        "    num_coarse_quantizers = 3,\n",
        "    num_fine_quantizers = 5,\n",
        "    codebook_size = 1024,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    audio_text_condition = True,\n",
        "    attn_dropout = config[\"attn_dropout\"],\n",
        "    ff_dropout = config[\"ff_dropout\"],\n",
        ")\n",
        "\n",
        "fine_trainer = FineTransformerTrainer(\n",
        "    transformer = fine_transformer,\n",
        "    codec = soundstream,\n",
        "    audio_conditioner = quantizer,\n",
        "    folder = dataset_folder,\n",
        "    lr=config[\"lr\"],\n",
        "    batch_size = config[\"batch_size\"],\n",
        "    data_max_length_seconds = config.get(\"data_max_length_seconds\"),\n",
        "    data_max_length = config.get(\"data_max_length\"),\n",
        "    save_results_every = VAL_EVERY_STEPS,\n",
        "    wd = config[\"wd\"],\n",
        "    grad_accum_every = config[\"grad_accum_every\"],\n",
        "    max_grad_norm = config[\"max_grad_norm\"],\n",
        "    save_model_every = SAVE_MODEL_EVERY,\n",
        "    num_train_steps = STEPS,\n",
        "    results_folder = RESULTS_ROOT_PATH+'/results_fine',\n",
        "    force_clear_prev_results = False,\n",
        "    accelerate_kwargs = get_accelerate_kwargs(config)\n",
        ")\n",
        "\n",
        "scheduler = get_scheduler(fine_trainer.optim, config)\n",
        "\n",
        "if load_ckpt:\n",
        "  fine_trainer.load(fine_load_path)\n",
        "\n",
        "if train:\n",
        "  custom_train(fine_trainer,\n",
        "               config,\n",
        "               scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoHgkgA3XKXH"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiY1uTyic2vv"
      },
      "outputs": [],
      "source": [
        "#!nvidia-smi --gpu-reset\n",
        "#!sudo rmmod nvidia_uvm ; sudo modprobe nvidia_uvm\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzghrux5WinW",
        "outputId": "4d4bddf7-8562-44b4-e361-126a64d8e3b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "generating semantic: 100%|██████████| 2048/2048 [05:16<00:00,  6.46it/s]\n",
            "generating coarse: 100%|██████████| 512/512 [14:33<00:00,  1.71s/it]\n",
            "generating fine: 100%|██████████| 512/512 [26:12<00:00,  3.07s/it]\n"
          ]
        }
      ],
      "source": [
        "# Everything together\n",
        "audiolm = AudioLM(\n",
        "    wav2vec = wav2vec,\n",
        "    codec = soundstream,\n",
        "    semantic_transformer = semantic_transformer,\n",
        "    coarse_transformer = coarse_transformer,\n",
        "    fine_transformer = fine_transformer\n",
        ")\n",
        "\n",
        "text_embeddings = quantizer(texts=[\"Experimental high quality metal song with flute\", \"\"])[0:1]\n",
        "seconds_to_generate = 10\n",
        "generated_wav = audiolm(text_embeds=text_embeddings, batch_size = 1, max_length=seconds_to_generate * wav2vec.target_sample_hz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rQPHTSRngEr"
      },
      "outputs": [],
      "source": [
        "output_path = RESULTS_ROOT_PATH + \"/generated/out1.wav\"\n",
        "sample_rate = 44100\n",
        "torchaudio.save(output_path, generated_wav.cpu(), sample_rate)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "D6zkgFFzkqEc",
        "QBaqCz5jb_l_",
        "SglfcvOvmS_Z",
        "eEFSss2wqCtL",
        "oiEBo6goqFRE",
        "PhK_3FSVqF2t",
        "jBxNK5cKW--_",
        "T7GiyBcBWiZV",
        "aKAFNMrqpMBC",
        "hsLvQ0-6T8Cr",
        "uhKGvSDSC-Mr",
        "fRvj7qOJWzmw"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0f82a8ec2b64f66a9d2aa6c3fa69865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d589e64142994af6b4703fa708adecb7",
              "IPY_MODEL_403e52fb75154038addbf23795f4bf83"
            ],
            "layout": "IPY_MODEL_c45978fb10674c6c864517118fbf8736"
          }
        },
        "d589e64142994af6b4703fa708adecb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_159b18f1df504487a89b8002a23b56e8",
            "placeholder": "​",
            "style": "IPY_MODEL_1e1353106fd24520b827e01567c43e4c",
            "value": "0.013 MB of 0.013 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "403e52fb75154038addbf23795f4bf83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fb5a72d28c642c891498129429fb235",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2567923cfe9f41bba9fbd2a28e7d37cf",
            "value": 1
          }
        },
        "c45978fb10674c6c864517118fbf8736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "159b18f1df504487a89b8002a23b56e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e1353106fd24520b827e01567c43e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fb5a72d28c642c891498129429fb235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2567923cfe9f41bba9fbd2a28e7d37cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4940d9988d548b4b3efd04ade6618aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76e3632ad2e34456b0dc43f4b8a734b9",
              "IPY_MODEL_da34fff74e3c42969270c70cb38e61e8"
            ],
            "layout": "IPY_MODEL_0df94dc33d3d49169ed2bccd6cafc5ab"
          }
        },
        "76e3632ad2e34456b0dc43f4b8a734b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c8402148c3647de900311e97758e7d2",
            "placeholder": "​",
            "style": "IPY_MODEL_54bffd551b6f4944b880b76c32fd3564",
            "value": "0.064 MB of 0.065 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "da34fff74e3c42969270c70cb38e61e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7fb474363924f5dabd9dcc3e0f24a89",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f8e31cb14334bb0a58727e131d08dcd",
            "value": 0.9802437384925622
          }
        },
        "0df94dc33d3d49169ed2bccd6cafc5ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c8402148c3647de900311e97758e7d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54bffd551b6f4944b880b76c32fd3564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7fb474363924f5dabd9dcc3e0f24a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f8e31cb14334bb0a58727e131d08dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}