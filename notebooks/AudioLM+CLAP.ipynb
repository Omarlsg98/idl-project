{"cells":[{"cell_type":"markdown","metadata":{"id":"txujENPmw_hW"},"source":["#Configs"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18808,"status":"ok","timestamp":1682396613366,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"zAehkQc7s6R2","outputId":"39bac9c8-b3dd-4098-f230-3c5bb48209e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ql8PXeeytRDh","executionInfo":{"status":"ok","timestamp":1682399183537,"user_tz":240,"elapsed":2,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}}},"outputs":[],"source":["release_path = \"/content/gdrive/MyDrive/IDL_Project/data/releases/v1.0\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":465600,"status":"ok","timestamp":1682397079357,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"PXKyOlMW8nyf","outputId":"c3420f98-b385-4e21-c265-d0d1f200ae43"},"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot open '/content/gdrive/MyDrive/IDL_Project/data/releases/v1.0/metadata (1).gsheet' for reading: Operation not supported\n","cp: cannot open '/content/gdrive/MyDrive/IDL_Project/data/releases/v1.0/metadata.gsheet' for reading: Operation not supported\n"]}],"source":["!cp -r {release_path} data/"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1682397079358,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"fCIDUEgLtR_h","outputId":"eb535ba6-8deb-4e05-a017-2f87f5063279"},"outputs":[{"output_type":"stream","name":"stdout","text":["02no_qTZgqE.wav\n","035rfqdoq3g.wav\n","06XMRpAMLBI.wav\n","08Lcqs5haF8.wav\n","0ADIhD-23hY.wav\n","0AEtGk3ySKA.wav\n","0ajU3pdb5qA.wav\n","0BVFnF5I6NE.wav\n","0CJNjvbgecg.wav\n","0cnUaVrpoR4.wav\n"]}],"source":["!ls -1 data/data/train/ | head"]},{"cell_type":"code","source":["!du -h data/data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nghLvbkAsQwd","executionInfo":{"status":"ok","timestamp":1682397276690,"user_tz":240,"elapsed":332,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}},"outputId":"16b7ab4d-eabd-403b-dc14-8ce5ad16fbc6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["11G\tdata/data/train\n","3.4G\tdata/data/test\n","3.8G\tdata/data/val\n","18G\tdata/data\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"aqdm3SEqmVGQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682397144636,"user_tz":240,"elapsed":65284,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}},"outputId":"add0565b-c217-4f97-f217-a36e1bf7f98e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.0/708.0 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.6/269.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install audiolm-pytorch -q"]},{"cell_type":"code","source":["!pip install laion-clap -q #RESTART after this(?)"],"metadata":{"id":"36uDXDNesF7a"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1682399188298,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"n337KoD2om3L","outputId":"f223c79f-e515-4fd9-bdbe-5e2d7604dab3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Apr 25 05:06:27 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   75C    P8    13W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"SglfcvOvmS_Z"},"source":["#CLAP"]},{"cell_type":"markdown","metadata":{"id":"eEFSss2wqCtL"},"source":["### Model"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"yU8Jawod3kN3","executionInfo":{"status":"ok","timestamp":1682405128497,"user_tz":240,"elapsed":4123,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}}},"outputs":[],"source":["!pip install huggingface_hub -q"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"BNVJt9bj08XT","executionInfo":{"status":"ok","timestamp":1682405128498,"user_tz":240,"elapsed":5,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}}},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n","import joblib\n","\n","REPO_ID = \"lukewys/laion_clap\"\n","FILENAME = \"music_audioset_epoch_15_esc_90.14.pt\"\n","\n","clap_ckp_path=hf_hub_download(repo_id=REPO_ID, filename=FILENAME)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18633,"status":"ok","timestamp":1682405147127,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"6cv0KTESo4Fn","outputId":"86644887-1a5b-4b49-923e-5d3bb45876cd"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Load the specified checkpoint /root/.cache/huggingface/hub/models--lukewys--laion_clap/snapshots/b3708341862f581175dba5c356a4ebf74a9b6651/music_audioset_epoch_15_esc_90.14.pt from users.\n","Load Checkpoint...\n","logit_scale_a \t Loaded\n","logit_scale_t \t Loaded\n","audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n","audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n","audio_branch.logmel_extractor.melW \t Loaded\n","audio_branch.bn0.weight \t Loaded\n","audio_branch.bn0.bias \t Loaded\n","audio_branch.patch_embed.proj.weight \t Loaded\n","audio_branch.patch_embed.proj.bias \t Loaded\n","audio_branch.patch_embed.norm.weight \t Loaded\n","audio_branch.patch_embed.norm.bias \t Loaded\n","audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n","audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n","audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n","audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n","audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n","audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n","audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n","audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n","audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n","audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n","audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n","audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n","audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n","audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n","audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n","audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n","audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n","audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n","audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n","audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n","audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n","audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n","audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n","audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n","audio_branch.layers.0.downsample.reduction.weight \t Loaded\n","audio_branch.layers.0.downsample.norm.weight \t Loaded\n","audio_branch.layers.0.downsample.norm.bias \t Loaded\n","audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n","audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n","audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n","audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n","audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n","audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n","audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n","audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n","audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n","audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n","audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n","audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n","audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n","audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n","audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n","audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n","audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n","audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n","audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n","audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n","audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n","audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n","audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n","audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n","audio_branch.layers.1.downsample.reduction.weight \t Loaded\n","audio_branch.layers.1.downsample.norm.weight \t Loaded\n","audio_branch.layers.1.downsample.norm.bias \t Loaded\n","audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.6.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.6.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.6.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.6.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.6.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.6.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.6.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.6.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.6.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.6.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.6.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.6.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.6.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.7.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.7.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.7.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.7.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.7.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.7.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.7.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.7.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.7.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.7.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.7.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.7.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.7.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.8.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.8.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.8.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.8.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.8.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.8.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.8.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.8.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.8.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.8.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.8.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.8.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.8.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.9.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.9.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.9.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.9.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.9.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.9.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.9.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.9.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.9.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.9.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.9.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.9.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.9.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.10.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.10.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.10.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.10.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.10.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.10.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.10.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.10.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.10.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.10.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.10.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.10.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.10.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.blocks.11.norm1.weight \t Loaded\n","audio_branch.layers.2.blocks.11.norm1.bias \t Loaded\n","audio_branch.layers.2.blocks.11.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.2.blocks.11.attn.qkv.weight \t Loaded\n","audio_branch.layers.2.blocks.11.attn.qkv.bias \t Loaded\n","audio_branch.layers.2.blocks.11.attn.proj.weight \t Loaded\n","audio_branch.layers.2.blocks.11.attn.proj.bias \t Loaded\n","audio_branch.layers.2.blocks.11.norm2.weight \t Loaded\n","audio_branch.layers.2.blocks.11.norm2.bias \t Loaded\n","audio_branch.layers.2.blocks.11.mlp.fc1.weight \t Loaded\n","audio_branch.layers.2.blocks.11.mlp.fc1.bias \t Loaded\n","audio_branch.layers.2.blocks.11.mlp.fc2.weight \t Loaded\n","audio_branch.layers.2.blocks.11.mlp.fc2.bias \t Loaded\n","audio_branch.layers.2.downsample.reduction.weight \t Loaded\n","audio_branch.layers.2.downsample.norm.weight \t Loaded\n","audio_branch.layers.2.downsample.norm.bias \t Loaded\n","audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n","audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n","audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n","audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n","audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n","audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n","audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n","audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n","audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n","audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n","audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n","audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n","audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n","audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n","audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n","audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n","audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n","audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n","audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n","audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n","audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n","audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n","audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n","audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n","audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n","audio_branch.norm.weight \t Loaded\n","audio_branch.norm.bias \t Loaded\n","audio_branch.tscam_conv.weight \t Loaded\n","audio_branch.tscam_conv.bias \t Loaded\n","audio_branch.head.weight \t Loaded\n","audio_branch.head.bias \t Loaded\n","text_branch.embeddings.word_embeddings.weight \t Loaded\n","text_branch.embeddings.position_embeddings.weight \t Loaded\n","text_branch.embeddings.token_type_embeddings.weight \t Loaded\n","text_branch.embeddings.LayerNorm.weight \t Loaded\n","text_branch.embeddings.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.0.output.dense.weight \t Loaded\n","text_branch.encoder.layer.0.output.dense.bias \t Loaded\n","text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.1.output.dense.weight \t Loaded\n","text_branch.encoder.layer.1.output.dense.bias \t Loaded\n","text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.2.output.dense.weight \t Loaded\n","text_branch.encoder.layer.2.output.dense.bias \t Loaded\n","text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.3.output.dense.weight \t Loaded\n","text_branch.encoder.layer.3.output.dense.bias \t Loaded\n","text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.4.output.dense.weight \t Loaded\n","text_branch.encoder.layer.4.output.dense.bias \t Loaded\n","text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.5.output.dense.weight \t Loaded\n","text_branch.encoder.layer.5.output.dense.bias \t Loaded\n","text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.6.output.dense.weight \t Loaded\n","text_branch.encoder.layer.6.output.dense.bias \t Loaded\n","text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.7.output.dense.weight \t Loaded\n","text_branch.encoder.layer.7.output.dense.bias \t Loaded\n","text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.8.output.dense.weight \t Loaded\n","text_branch.encoder.layer.8.output.dense.bias \t Loaded\n","text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.9.output.dense.weight \t Loaded\n","text_branch.encoder.layer.9.output.dense.bias \t Loaded\n","text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.10.output.dense.weight \t Loaded\n","text_branch.encoder.layer.10.output.dense.bias \t Loaded\n","text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n","text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n","text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n","text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n","text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n","text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n","text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n","text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n","text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n","text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n","text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n","text_branch.encoder.layer.11.output.dense.weight \t Loaded\n","text_branch.encoder.layer.11.output.dense.bias \t Loaded\n","text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n","text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n","text_branch.pooler.dense.weight \t Loaded\n","text_branch.pooler.dense.bias \t Loaded\n","text_transform.sequential.0.weight \t Loaded\n","text_transform.sequential.0.bias \t Loaded\n","text_transform.sequential.3.weight \t Loaded\n","text_transform.sequential.3.bias \t Loaded\n","text_projection.0.weight \t Loaded\n","text_projection.0.bias \t Loaded\n","text_projection.2.weight \t Loaded\n","text_projection.2.bias \t Loaded\n","audio_transform.sequential.0.weight \t Loaded\n","audio_transform.sequential.0.bias \t Loaded\n","audio_transform.sequential.3.weight \t Loaded\n","audio_transform.sequential.3.bias \t Loaded\n","audio_projection.0.weight \t Loaded\n","audio_projection.0.bias \t Loaded\n","audio_projection.2.weight \t Loaded\n","audio_projection.2.bias \t Loaded\n"]}],"source":["import numpy as np\n","import librosa\n","import torch\n","import laion_clap\n","\n","# quantization\n","def int16_to_float32(x):\n","    return (x / 32767.0).astype(np.float32)\n","\n","\n","def float32_to_int16(x):\n","    x = np.clip(x, a_min=-1., a_max=1.)\n","    return (x * 32767.).astype(np.int16)\n","\n","clap_model = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base')\n","clap_model.load_ckpt(clap_ckp_path) "]},{"cell_type":"markdown","metadata":{"id":"oiEBo6goqFRE"},"source":["### Usage"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7178,"status":"ok","timestamp":1682405154292,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"wJJrpfgUqNhE","outputId":"2ce2c26b-a31d-4e74-be8d-be8a5c05e959"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/laion_clap/hook.py:137: UserWarning: PySoundFile failed. Trying audioread instead.\n","  audio_waveform, _ = librosa.load(f, sr=48000)\n","/usr/local/lib/python3.9/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","/usr/local/lib/python3.9/dist-packages/laion_clap/hook.py:137: UserWarning: PySoundFile failed. Trying audioread instead.\n","  audio_waveform, _ = librosa.load(f, sr=48000)\n","/usr/local/lib/python3.9/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"]},{"output_type":"stream","name":"stdout","text":["[[-0.03125608 -0.03322504 -0.03928095  0.00383821 -0.02353179  0.02097818\n","   0.03423237 -0.00495442  0.05971711 -0.0580765  -0.05625628  0.01339109\n","   0.0523831   0.02230659  0.07409521  0.043075    0.0228069   0.02822227\n","  -0.03153085  0.01357142]\n"," [-0.0934386  -0.00691546 -0.00153525 -0.05428723 -0.00323282  0.051808\n","   0.05226986 -0.02217935  0.04347937 -0.01493837 -0.07677202 -0.00586797\n","   0.0508899  -0.01006258  0.09496401  0.07044034 -0.01302771 -0.03778823\n","   0.04158276 -0.04409483]]\n","(2, 512)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-f8da7383ca2d>:11: UserWarning: PySoundFile failed. Trying audioread instead.\n","  audio_data, _ = librosa.load('/content/data/data/train/-y3Abv2ovkU.wav', sr=48000) # sample rate should be 48000\n","/usr/local/lib/python3.9/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"]},{"output_type":"stream","name":"stdout","text":["[[-0.02368575 -0.03112708 -0.03111554  0.02309774 -0.03877378  0.03650819\n","   0.03023502 -0.0161311   0.03330233 -0.07896581 -0.05960624  0.03490806\n","   0.02713726  0.00966631  0.06749677  0.0529283   0.02623483  0.03851531\n","  -0.05575174 -0.01084571]]\n","(1, 512)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/laion_clap/hook.py:137: UserWarning: PySoundFile failed. Trying audioread instead.\n","  audio_waveform, _ = librosa.load(f, sr=48000)\n","/usr/local/lib/python3.9/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","/usr/local/lib/python3.9/dist-packages/laion_clap/hook.py:137: UserWarning: PySoundFile failed. Trying audioread instead.\n","  audio_waveform, _ = librosa.load(f, sr=48000)\n","/usr/local/lib/python3.9/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[-0.0466, -0.0212, -0.0368, -0.0225, -0.0483, -0.0213, -0.0128,  0.0081,\n","         -0.0206, -0.0896, -0.0636, -0.0032,  0.0473, -0.0014, -0.0008, -0.0027,\n","          0.0683,  0.0142, -0.0439,  0.0341],\n","        [-0.0288, -0.0062, -0.0065,  0.0588,  0.0053,  0.0606,  0.0597, -0.0714,\n","          0.0530, -0.0327,  0.0193,  0.0401,  0.0548, -0.0046,  0.0822,  0.0495,\n","          0.0622,  0.0095,  0.0142, -0.0050]], device='cuda:0',\n","       grad_fn=<SliceBackward0>)\n","torch.Size([2, 512])\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-f8da7383ca2d>:23: UserWarning: PySoundFile failed. Trying audioread instead.\n","  audio_data, _ = librosa.load('/content/data/data/train/-y3Abv2ovkU.wav', sr=48000) # sample rate should be 48000\n","/usr/local/lib/python3.9/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"]},{"output_type":"stream","name":"stdout","text":["tensor([[-0.0312, -0.0222, -0.0469,  0.0005, -0.0514,  0.0062,  0.0152, -0.0201,\n","          0.0209, -0.0982, -0.0557,  0.0040,  0.0407,  0.0444,  0.0129,  0.0181,\n","          0.0341,  0.0294, -0.0549,  0.0265]], device='cuda:0',\n","       grad_fn=<SliceBackward0>)\n","torch.Size([1, 512])\n","[[ 0.00123356  0.01328493 -0.03407465 ...  0.00113716 -0.04714473\n","  -0.00768401]\n"," [-0.06046474  0.01317477 -0.01736685 ... -0.01735982 -0.04660317\n","  -0.05558019]]\n","(2, 512)\n","tensor([[ 0.0012,  0.0133, -0.0341,  ...,  0.0011, -0.0471, -0.0077],\n","        [-0.0605,  0.0132, -0.0174,  ..., -0.0174, -0.0466, -0.0556]],\n","       device='cuda:0', grad_fn=<DivBackward0>)\n","torch.Size([2, 512])\n"]}],"source":["# Directly get audio embeddings from audio files\n","audio_file = [\n","    '/content/data/data/train/-y3Abv2ovkU.wav',\n","    '/content/data/data/train/BNyoM9gvGMs.wav'\n","]\n","audio_embed = clap_model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=False)\n","print(audio_embed[:,-20:])\n","print(audio_embed.shape)\n","\n","# Get audio embeddings from audio data\n","audio_data, _ = librosa.load('/content/data/data/train/-y3Abv2ovkU.wav', sr=48000) # sample rate should be 48000\n","audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n","audio_embed = clap_model.get_audio_embedding_from_data(x = audio_data, use_tensor=False)\n","print(audio_embed[:,-20:])\n","print(audio_embed.shape)\n","\n","# Directly get audio embeddings from audio files, but return torch tensor\n","audio_embed = clap_model.get_audio_embedding_from_filelist(x = audio_file, use_tensor=True)\n","print(audio_embed[:,-20:])\n","print(audio_embed.shape)\n","\n","# Get audio embeddings from audio data\n","audio_data, _ = librosa.load('/content/data/data/train/-y3Abv2ovkU.wav', sr=48000) # sample rate should be 48000\n","audio_data = audio_data.reshape(1, -1) # Make it (1,T) or (N,T)\n","audio_data = torch.from_numpy(int16_to_float32(float32_to_int16(audio_data))).float() # quantize before send it in to the model\n","audio_embed = clap_model.get_audio_embedding_from_data(x = audio_data, use_tensor=True)\n","print(audio_embed[:,-20:])\n","print(audio_embed.shape)\n","\n","# Get text embedings from texts:\n","text_data = [\"I love the contrastive learning\", \"I love the pretrain model\"] \n","text_embed = clap_model.get_text_embedding(text_data)\n","print(text_embed)\n","print(text_embed.shape)\n","\n","# Get text embedings from texts, but return torch tensor:\n","text_data = [\"I love the contrastive learning\", \"I love the pretrain model\"] \n","text_embed = clap_model.get_text_embedding(text_data, use_tensor=True)\n","print(text_embed)\n","print(text_embed.shape)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682405154293,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"ZizSIRjnwDlI","outputId":"8d026caf-c07f-4696-a175-f2fa0ab54388"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 512])"]},"metadata":{},"execution_count":5}],"source":["wavs = torch.randn(2, 1024)\n","texts = torch.randint(0, 20000, (2, 256))\n","clap_model.get_audio_embedding_from_data(x = wavs, use_tensor=True).shape"]},{"cell_type":"markdown","metadata":{"id":"PhK_3FSVqF2t"},"source":["### Quantizer for AudioML"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2055,"status":"ok","timestamp":1682405156345,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"Ihv8jANAqN-J"},"outputs":[],"source":["# Based on MuLaNEmbedQuantizer implementation on https://github.com/lucidrains/musiclm-pytorch\n","from audiolm_pytorch.utils import AudioConditionerBase\n","from beartype.typing import List, Optional, Tuple\n","from beartype import beartype\n","from einops import rearrange, repeat, reduce, pack, unpack\n","from einops.layers.torch import Rearrange\n","from vector_quantize_pytorch import ResidualVQ\n","from torch import nn\n","\n","\n","def exists(val):\n","    return val is not None\n","\n","def default(val, d):\n","    return val if exists(val) else d\n","\n","class CLAPEmbedQuantizer(AudioConditionerBase):\n","    @beartype\n","    def __init__(\n","        self,\n","        clap: laion_clap.CLAP_Module,\n","        conditioning_dims: Tuple[int, ...],\n","        rq_num_quantizers = 8,\n","        rq_ema_decay = 0.9,\n","        codebook_size = 1024,\n","        namespaces: Tuple[str, ...] = ('semantic', 'coarse', 'fine'),\n","\n","    ):\n","        super().__init__()\n","        self.clap = clap\n","\n","        assert len(namespaces) > 0\n","        self.namespaces = namespaces\n","        self.conditioning_dims = conditioning_dims\n","\n","        assert len(conditioning_dims) == len(namespaces), 'number of conditioning dimensions must be equal to number of namespaces'\n","\n","        dim = clap.model.audio_projection[-1].out_features\n","\n","        self.rq = ResidualVQ(\n","            dim = dim,\n","            num_quantizers = rq_num_quantizers,\n","            codebook_size = codebook_size,\n","            decay = rq_ema_decay,\n","            commitment_weight = 0,    # only use EMA to update codebooks\n","            kmeans_init = True,\n","            threshold_ema_dead_code = 2,\n","            quantize_dropout = False  # no quantize dropout\n","        )\n","\n","        self.dim = dim\n","        self.num_codebooks = rq_num_quantizers\n","\n","        self.cond_embeddings = nn.ParameterDict({})\n","\n","        for namespace, conditioning_dim in zip(namespaces, conditioning_dims):\n","            cond_embeddings = nn.Parameter(torch.randn(rq_num_quantizers, codebook_size, conditioning_dim))\n","            nn.init.normal_(cond_embeddings, std = 0.02)\n","\n","            self.cond_embeddings[namespace] = cond_embeddings\n","\n","        self.set_default_namespace(namespaces[0])\n","\n","    def parameters(self):\n","        return self.cond_embeddings.parameters()\n","\n","    def set_default_namespace(self, namespace):\n","        self._default_namespace = namespace\n","\n","    def forward(\n","        self,\n","        wavs = None,\n","        texts = None,\n","        namespace = None\n","    ):\n","        assert exists(wavs) ^ exists(texts)\n","\n","        namespace = default(namespace, self._default_namespace)\n","        assert namespace in self.namespaces, f'namespace {namespace} not found'\n","        cond_embeddings = self.cond_embeddings[namespace]\n","\n","        with torch.no_grad():\n","            self.clap.eval()\n","\n","            # sound and language live in joint embedding space because of contrastive learning\n","\n","            if exists(wavs):\n","                latents = self.clap.get_audio_embedding_from_data(x = wavs, use_tensor=True) \n","            elif exists(texts):\n","                latents = self.clap.get_text_embedding(texts, use_tensor=True) \n","\n","        _, indices, _ = self.rq(latents)\n","\n","        batch, num_codebooks, dim = indices.shape[0], self.num_codebooks, cond_embeddings.shape[-1]\n","\n","        cond_embeddings = repeat(cond_embeddings, 'q c d -> b q c d', b = batch)\n","        indices = repeat(indices, 'b q -> b q 1 d', q = num_codebooks, d = dim)\n","\n","        cond_embeddings = cond_embeddings.gather(2, indices)\n","        return rearrange(cond_embeddings, 'b q 1 d -> b q d')"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1682405156346,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"ExxNRIaavxk4"},"outputs":[],"source":["# setup the quantizer with the namespaced conditioning embeddings, \n","# unique per quantizer as well as namespace (per transformer)\n","quantizer = CLAPEmbedQuantizer(\n","    clap = clap_model,                         \n","    conditioning_dims = (1024, 1024, 1024), # say all three transformers have model dimensions of 1024\n","    namespaces = ('semantic', 'coarse', 'fine')\n",").cuda()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1682405156346,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"2HUYIETzxSib","outputId":"34d328a1-3fc0-4e8c-fe20-0695e287235e"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 8, 1024])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[[-0.0102, -0.0155, -0.0046,  ..., -0.0237, -0.0015,  0.0019],\n","         [ 0.0090,  0.0213,  0.0069,  ...,  0.0137,  0.0057,  0.0121],\n","         [-0.0305, -0.0495, -0.0396,  ..., -0.0147,  0.0182, -0.0216],\n","         ...,\n","         [ 0.0075, -0.0099,  0.0311,  ..., -0.0082,  0.0213, -0.0036],\n","         [-0.0136, -0.0115, -0.0033,  ...,  0.0262, -0.0228, -0.0068],\n","         [-0.0154,  0.0110, -0.0099,  ..., -0.0406, -0.0114, -0.0223]],\n","\n","        [[ 0.0047, -0.0170, -0.0233,  ...,  0.0301, -0.0113, -0.0149],\n","         [ 0.0090,  0.0213,  0.0069,  ...,  0.0137,  0.0057,  0.0121],\n","         [-0.0305, -0.0495, -0.0396,  ..., -0.0147,  0.0182, -0.0216],\n","         ...,\n","         [ 0.0075, -0.0099,  0.0311,  ..., -0.0082,  0.0213, -0.0036],\n","         [-0.0136, -0.0115, -0.0033,  ...,  0.0262, -0.0228, -0.0068],\n","         [-0.0154,  0.0110, -0.0099,  ..., -0.0406, -0.0114, -0.0223]]],\n","       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)"]},"metadata":{},"execution_count":8}],"source":["# now say you want the conditioning embeddings for semantic transformer\n","wavs = torch.randn(2, 1024).cuda()\n","conds = quantizer(wavs = wavs, namespace = 'coarse')\n","print(conds.shape)\n","conds"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682405156347,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"Q1fIZMgIFGNJ","outputId":"8ff17594-d2f0-4ed8-eab5-9866ce2b958e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 8, 1024])"]},"metadata":{},"execution_count":9}],"source":["text_embbeddings = quantizer(texts=[\"I love the contrastive learning\", \"\"])[0:1]\n","text_embbeddings.shape"]},{"cell_type":"markdown","metadata":{"id":"IYEwiQq93j11"},"source":["#AudioLM"]},{"cell_type":"markdown","metadata":{"id":"jBxNK5cKW--_"},"source":["### Imports & paths"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682405156347,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"OrNeKngVVM0L"},"outputs":[],"source":["# imports\n","import math\n","import wave\n","import struct\n","import os\n","import urllib.request\n","import tarfile\n","from audiolm_pytorch import SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n","from torch import nn\n","import torch\n","import torchaudio\n","import gc\n","\n","# define all dataset paths, checkpoints, etc\n","RESULTS_ROOT_PATH=\"/content/gdrive/MyDrive/IDL_Project/models\"\n","\n","dataset_folder = \"data/data/\"\n","soundstream_ckpt = RESULTS_ROOT_PATH+\"/results_soundstream/soundstream.8.pt\" # this can change depending on number of steps\n","hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n","hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer"]},{"cell_type":"markdown","metadata":{"id":"PYcI0aXEwuxR"},"source":["## Training\n","\n","Now that we have a dataset, we can train AudioLM.\n","\n","**Note**: do NOT type \"y\" to overwrite previous experiments/ checkpoints when running through the cells here unless you're ready to the entire results folder! Otherwise you will end up erasing things (e.g. you train SoundStream first, and if you choose \"overwrite\" then you lose the SoundStream checkpoint when you then train SemanticTransformer)."]},{"cell_type":"markdown","metadata":{"id":"T7GiyBcBWiZV"},"source":["### SoundStream / encodec"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682405156347,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"Oz4Mll6lVPbt"},"outputs":[],"source":["using_encodec=True"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682405156348,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"wSVRf1Z7V8dT"},"outputs":[],"source":["# from audiolm_pytorch import AudioLMSoundStream, MusicLMSoundStream\n","# MusicLMSoundStream()\n","from audiolm_pytorch import EncodecWrapper\n","soundstream = EncodecWrapper()"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1682405156348,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"nGU0OZiOwPEO"},"outputs":[],"source":["if not using_encodec:\n","  soundstream = SoundStream(\n","      codebook_size = 1024,\n","      rq_num_quantizers = 8,\n","  )\n","\n","  trainer = SoundStreamTrainer( \n","      soundstream,\n","      folder = dataset_folder,\n","      batch_size = 4,\n","      grad_accum_every = 8,         # effective batch size of 32\n","      data_max_length = 320 * 32,\n","      save_results_every = 2,\n","      save_model_every = 4,\n","      num_train_steps = 9,\n","      results_folder = RESULTS_ROOT_PATH+'/results_soundstream',\n","      force_clear_prev_results = True,\n","  ).cuda()\n","  # NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n","  # adjusting save_*_every variables for the same reason\n","\n","  trainer.train()"]},{"cell_type":"markdown","source":["### wav2Vec"],"metadata":{"id":"aKAFNMrqpMBC"}},{"cell_type":"code","source":["# hubert checkpoints can be downloaded at\n","# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n","if not os.path.isdir(\"hubert\"):\n","  os.makedirs(\"hubert\")\n","if not os.path.isfile(hubert_ckpt):\n","  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n","  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n","if not os.path.isfile(hubert_quantizer):\n","  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n","  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n","\n","wav2vec = HubertWithKmeans(\n","    checkpoint_path = f'./{hubert_ckpt}',\n","    kmeans_path = f'./{hubert_quantizer}'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Bu8Ve0-pO9J","executionInfo":{"status":"ok","timestamp":1682405163329,"user_tz":240,"elapsed":6988,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}},"outputId":"7bb697e0-cd73-4234-b244-50f5082aa99d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["wav2vec.target_sample_hz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOCVHaqWtC3D","executionInfo":{"status":"ok","timestamp":1682405163330,"user_tz":240,"elapsed":17,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}},"outputId":"3b245ebf-74f1-4a60-b2e4-11a88e8552cf"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16000"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"lqjN28L4Wc5Q"},"source":["### SemanticTransformer"]},{"cell_type":"code","source":["# del semantic_transformer, semantic_trainer\n","torch.cuda.empty_cache()\n","load_ckpt = True\n","train = False\n","semantic_load_path = RESULTS_ROOT_PATH+'/results_semantic/semantic.transformer.50.pt' \n","coarse_load_path = RESULTS_ROOT_PATH+'/results_coarse/coarse.transformer.300.pt'\n","fine_load_path = RESULTS_ROOT_PATH+'/results_fine/fine.transformer.300.pt'"],"metadata":{"id":"HbrCDurgx47j","executionInfo":{"status":"ok","timestamp":1682406293927,"user_tz":240,"elapsed":1422,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgd962eSvDzS","executionInfo":{"status":"ok","timestamp":1682406301446,"user_tz":240,"elapsed":4881,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}},"outputId":"8ea3f09d-cbdd-4524-89ec-9acf0ac5d5a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["training with dataset of 3979 samples and validating with randomly splitted 210 samples\n"]}],"source":["torch.cuda.empty_cache()\n","semantic_transformer = SemanticTransformer(\n","    num_semantic_tokens = wav2vec.codebook_size,\n","    dim = 1024,\n","    depth = 6,\n","    audio_text_condition = True,\n",").cuda()\n","\n","\n","semantic_trainer = SemanticTransformerTrainer(\n","    transformer = semantic_transformer,\n","    wav2vec = wav2vec,\n","    audio_conditioner = quantizer,\n","    folder = dataset_folder,\n","    batch_size = 4,\n","    data_max_length_seconds = 30,\n","    save_results_every = 50,\n","    save_model_every = 50,\n","    num_train_steps = 300+1,\n","    results_folder = RESULTS_ROOT_PATH+'/results_semantic',\n","    force_clear_prev_results = False,\n",")\n","if load_ckpt:\n","  semantic_trainer.load(semantic_load_path)\n","\n","if train:\n","  semantic_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"4eEvIzhEWwRz"},"source":["### CoarseTransformer"]},{"cell_type":"code","source":["try:\n","  if train:\n","    del semantic_transformer, semantic_trainer\n","except:\n","  pass\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"bx3OGKzFECEa","executionInfo":{"status":"ok","timestamp":1682406312627,"user_tz":240,"elapsed":4,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1LeWmaNHzzY9","executionInfo":{"status":"ok","timestamp":1682406313213,"user_tz":240,"elapsed":589,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}},"outputId":"28c091e6-46ed-41dc-a071-5ff61be676ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["training with dataset of 3979 samples and validating with randomly splitted 210 samples\n"]}],"source":["torch.cuda.empty_cache()\n","coarse_transformer = CoarseTransformer(\n","    num_semantic_tokens = wav2vec.codebook_size,\n","    codebook_size = 1024,\n","    num_coarse_quantizers = 3,\n","    dim = 1024,\n","    depth = 6,\n","    audio_text_condition = True,\n",")\n","\n","coarse_trainer = CoarseTransformerTrainer(\n","    transformer = coarse_transformer,\n","    codec = soundstream,\n","    wav2vec = wav2vec,\n","    audio_conditioner = quantizer,\n","    folder = dataset_folder,\n","    batch_size = 1,\n","    data_max_length_seconds = 10,\n","    save_results_every = 50,\n","    save_model_every = 50,\n","    num_train_steps = 300+1,\n","    results_folder = RESULTS_ROOT_PATH+'/results_coarse',\n","    force_clear_prev_results = False,\n",")\n","# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n","# adjusting save_*_every variables for the same reason\n","if load_ckpt:\n","  coarse_trainer.load(coarse_load_path)\n","\n","if train:\n","  coarse_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"fRvj7qOJWzmw"},"source":["### FineTransformer"]},{"cell_type":"code","source":["try:\n","  if train:\n","    del coarse_transformer, coarse_trainer\n","except:\n","  pass\n","gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"kacUP_8DHOVy","executionInfo":{"status":"ok","timestamp":1682406314027,"user_tz":240,"elapsed":4,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZRaEhRRKWg8F","executionInfo":{"status":"ok","timestamp":1682406316303,"user_tz":240,"elapsed":2279,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}},"outputId":"9665a0bd-28f0-47d8-e42d-443157ee4aca"},"outputs":[{"output_type":"stream","name":"stdout","text":["training with dataset of 3979 samples and validating with randomly splitted 210 samples\n"]}],"source":["torch.cuda.empty_cache()\n","fine_transformer = FineTransformer(\n","    num_coarse_quantizers = 3,\n","    num_fine_quantizers = 5,\n","    codebook_size = 1024,\n","    dim = 1024,\n","    depth = 6,\n","    audio_text_condition = True,\n",")\n","\n","fine_trainer = FineTransformerTrainer(\n","    transformer = fine_transformer,\n","    codec = soundstream,\n","    audio_conditioner = quantizer,\n","    folder = dataset_folder,\n","    batch_size = 1,\n","    data_max_length_seconds = 5,\n","    save_results_every = 50,\n","    save_model_every = 50,\n","    num_train_steps = 300+1,\n","    results_folder = RESULTS_ROOT_PATH+'/results_fine',\n","    force_clear_prev_results = False,\n",")\n","# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n","# adjusting save_*_every variables for the same reason\n","if load_ckpt:\n","  fine_trainer.load(fine_load_path)\n","\n","if train:\n","  fine_trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"QoHgkgA3XKXH"},"source":["## Inference"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":484,"status":"ok","timestamp":1682406324863,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"FiY1uTyic2vv"},"outputs":[],"source":["#!nvidia-smi --gpu-reset\n","#!sudo rmmod nvidia_uvm ; sudo modprobe nvidia_uvm\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2763857,"status":"ok","timestamp":1682409136678,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"},"user_tz":240},"id":"rzghrux5WinW","outputId":"4d4bddf7-8562-44b4-e361-126a64d8e3b7"},"outputs":[{"output_type":"stream","name":"stderr","text":["generating semantic: 100%|██████████| 2048/2048 [05:16<00:00,  6.46it/s]\n","generating coarse: 100%|██████████| 512/512 [14:33<00:00,  1.71s/it]\n","generating fine: 100%|██████████| 512/512 [26:12<00:00,  3.07s/it]\n"]}],"source":["# Everything together\n","audiolm = AudioLM(\n","    wav2vec = wav2vec,\n","    codec = soundstream,\n","    semantic_transformer = semantic_transformer,\n","    coarse_transformer = coarse_transformer,\n","    fine_transformer = fine_transformer\n",")\n","\n","text_embeddings = quantizer(texts=[\"Experimental high quality metal song with flute\", \"\"])[0:1]\n","\n","generated_wav = audiolm(text_embeds=text_embeddings, batch_size = 1, max_length=2048)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"4rQPHTSRngEr","executionInfo":{"status":"ok","timestamp":1682409210918,"user_tz":240,"elapsed":868,"user":{"displayName":"Omar Sanchez Granados","userId":"15895478376541239787"}}},"outputs":[],"source":["output_path = RESULTS_ROOT_PATH + \"/generated/out1.wav\"\n","sample_rate = 44100\n","torchaudio.save(output_path, generated_wav.cpu(), sample_rate)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["SglfcvOvmS_Z","eEFSss2wqCtL","oiEBo6goqFRE","PhK_3FSVqF2t","jBxNK5cKW--_","T7GiyBcBWiZV","aKAFNMrqpMBC","lqjN28L4Wc5Q","4eEvIzhEWwRz","QoHgkgA3XKXH"],"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPIdPOWGUy7zGKRUx7qJOEG"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}